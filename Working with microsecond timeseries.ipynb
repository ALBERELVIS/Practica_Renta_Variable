{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with microsecond time-series\n",
    "\n",
    "Here we are going to explore some important concepts you will need to do your home assignment when using high frequency time series of financial markets.\n",
    "\n",
    "For this, we are going to work with the following dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRD Data: Structure, Location, and Interpretation\n",
    "\n",
    "This section summarizes **only the trade (TRD)** files from the dataset, how they are organized on disk, how they should be interpreted, and the key rules for cleaning and processing them.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Where the TRD Files Live (Directory Layout)\n",
    "\n",
    "Your working directory contains a folder called `DATA/`.\n",
    "Inside it, there is **one subfolder per venue per trading date**, for example:\n",
    "\n",
    "- `BME_2025-11-07/`\n",
    "- `TURQUOISE_2025-11-07/`\n",
    "- `CBOE_2025-11-07/`\n",
    "- `AQUIS_2025-11-07/`\n",
    "\n",
    "Each of these venue–date folders contains gzip-compressed, semicolon-separated CSV files of three types:\n",
    "\n",
    "- `QTE_*.csv.gz` — order book snapshots\n",
    "- `TRD_*.csv.gz` — **trades**\n",
    "- `STS_*.csv.gz` — market trading status\n",
    "\n",
    "For the purposes of this document, only the `TRD_*.csv.gz` files matter.\n",
    "\n",
    "### File Naming Pattern\n",
    "\n",
    "All files follow:\n",
    "\n",
    "```\n",
    "<type>_<session>_<isin>_<ticker>_<mic>_<part>.csv.gz\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- **type**: `TRD`\n",
    "- **session**: trading date (`YYYY-MM-DD`)\n",
    "- **isin**: cross-venue ISIN\n",
    "- **ticker**: venue-specific symbol (multiple books for the same ISIN may exist)\n",
    "- **mic**: segment MIC (e.g., `XMAD`)\n",
    "- **part**: integer; if multiple `part` values exist for the same identity, **only the highest part number must be processed**, and you should emit a warning.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Identity & Join Keys (Relevant for Trades)\n",
    "\n",
    "A single order book identity is defined as:\n",
    "\n",
    "```\n",
    "(session, isin, mic, ticker)\n",
    "```\n",
    "\n",
    "All TRD files belonging to this identity correspond to trades occurring on the same order book.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Time Semantics for TRD\n",
    "\n",
    "All timestamps use:\n",
    "\n",
    "- **epoch** = microseconds since Unix epoch (UTC)\n",
    "- Feeds are **perfectly synchronized** across file types (no skew adjustments)\n",
    "\n",
    "When multiple trades share the same epoch:\n",
    "\n",
    "- TRD rows are ordered by **ascending sequence** (lowest sequence = earliest event at that timestamp)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. TRD (Trade) Files\n",
    "\n",
    "### Columns\n",
    "\n",
    "- session\n",
    "- inst_id\n",
    "- sequence\n",
    "- isin\n",
    "- ticker\n",
    "- mic\n",
    "- currency\n",
    "- epoch\n",
    "- auction_on_demand_mic\n",
    "- aggressor_side\n",
    "- trade_id\n",
    "- mmt_flags\n",
    "- mmt_flags_version\n",
    "- px\n",
    "- qty\n",
    "- securityTradingId\n",
    "- bloombergTicker\n",
    "- lastTradePriceCurrency\n",
    "\n",
    "### Column Semantics\n",
    "\n",
    "- **aggressor_side**\n",
    "  - `1` = buy-initiated trade\n",
    "  - `2` = sell-initiated\n",
    "  - empty = unknown\n",
    "\n",
    "- **px, qty**: Trade price and quantity.\n",
    "\n",
    "- **mmt_flags**: The first two characters encode the most important trade attributes:\n",
    "  - **First character** (Market Model) examples: `1=CLOB`, `3=Dark`, `4=Off Book`, `5=Periodic Auction`, etc.\n",
    "  - **Second character** (Trade Mode) examples: `2=Continuous`, `K=Opening Auction`, `I=Closing Auction`, `4=Out of Main Session`, etc.\n",
    "\n",
    "- **Off-book trades**: Valid TRD entries may include off-book prints. They **do not influence** addressability (that comes from QTE and STS only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load the Data (Using your existing files) ---\n",
    "# We assume the files are in the folder structure you provided\n",
    "path_xmad = 'DATA_SMALL/BME_2025-11-07/TRD_2025-11-07_ES0113900J37_SAN_XMAD_1.csv.gz'\n",
    "path_ceux = 'DATA_SMALL/CBOE_2025-11-07/TRD_2025-11-07_ES0113900J37_SANe_CEUX_1.csv.gz'\n",
    "\n",
    "# Load BME (Madrid)\n",
    "df_xmad = pd.read_csv(path_xmad, sep=';', compression='gzip')\n",
    "\n",
    "# Load CBOE (Europe)\n",
    "df_ceux = pd.read_csv(path_ceux, sep=';', compression='gzip')\n",
    "\n",
    "print(f\"Loaded {len(df_xmad)} rows from XMAD\")\n",
    "print(f\"Loaded {len(df_ceux)} rows from CEUX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Let's prepare the data step by step and then later we encapsulate it in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_xmad.copy()\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sort strictly\n",
    "df = df.sort_values(by=['epoch', 'sequence'], ascending=[True, True])\n",
    "\n",
    "# We can see how there are many trades happening at exactly the same epoch! \n",
    "# This is common in the stock exchange, when an aggressive order executes against many passive orders in the orderbook \n",
    "# at the same or differ price levels, the exchange reports one transaction at a time but all with the same timestamp\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a good idea to have an index without duplicates. Otherwise, many future transformations will trigger an error\n",
    "# because the require unique index values (e.g. merge_asof)\n",
    "duplicates = df.duplicated(subset='epoch', keep=False).sum()\n",
    "print(f\"Found {duplicates} trades sharing the same microsecond.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to treat a timeseries with events happening at exactly the same time\n",
    "\n",
    "We need to find a strategy to work with this. The simplest strategy is often the best, but it is not always acceptable depending on the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Good for orderbooksnapshots but not valid for trades because you lose them\n",
    "# Orderbook snapshots that happen on the same microsecond for us are virtually non existing. The only thing we see\n",
    "# is the last orderbook of the microsecond. Therefore for orderbooksnapshots it is valid to keep only the last.\n",
    "df_clean = df.drop_duplicates(subset='epoch', keep='last').copy()\n",
    "display(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2 (Intermediate): the nanoseconds trick\n",
    "temp_ts = pd.to_datetime(df['epoch'], unit='us')\n",
    "display(temp_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard groupby().cumcount() numbers the items in a group: 0, 1, 2, 3...\n",
    "# We group by 'epoch' to find trades happening at the same time.\n",
    "# We treat that count as Nanoseconds.\n",
    "\n",
    "# Basically, for every trade, we count how many trades had the same epoch before, this is like a counter that allow us to separate trades \n",
    "offset_ns = df.groupby('epoch').cumcount()\n",
    "display(offset_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check that there are not more than 1000 executions at the same microsecond\n",
    "# If there were more, this trick could cause issues, since the trade 1001 would be assigned to the next microsecond\n",
    "max(offset_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final High-Res Timestamp\n",
    "# Base Time (us) + Offset (ns)\n",
    "df['ts'] = temp_ts + pd.to_timedelta(offset_ns, unit='ns')\n",
    "df.set_index('ts', inplace=True)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3 (Advanced): Set MultiIndex\n",
    "# We keep 'epoch' as a column too, just in case we need the raw int later\n",
    "df['ts'] = pd.to_datetime(df['epoch'], unit='us')\n",
    "df.set_index(['ts', 'sequence'], inplace=True)\n",
    "display(df.head())\n",
    "\n",
    "# The problem of this method is that it prevent us from using the merge_asof which is very useful when timestamps do not repeat.\n",
    "# For the purpose of this lecture, we will use this approach. In real life, you would not pivot the data and you would rather process the full stream of data tick by tick\n",
    "# This makes it much more complex, so we can explore a sufficiently valid approach for our use case which does not require\n",
    "# the utmost precision. \n",
    "\n",
    "# Also this approach will be useful for your homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity and to be able to use merge_asof we will go for Option 2.\n",
    "# Here is the whole process encapsulated\n",
    "def clean_hft_data_preserve_all(df):\n",
    "    \"\"\"\n",
    "    1. Sorts by Epoch and Sequence.\n",
    "    2. Identifies duplicates.\n",
    "    3. Adds a 'nanosecond' offset to duplicates to make time unique.\n",
    "    4. Sets unique Index.\n",
    "    \"\"\"\n",
    "    # 1. Sort strictly\n",
    "    df = df.sort_values(by=['epoch', 'sequence'], ascending=[True, True])\n",
    "    \n",
    "    # 2. Convert to basic Timestamp (Microseconds)\n",
    "    # We do this temporarily to get the base time\n",
    "    temp_ts = pd.to_datetime(df['epoch'], unit='us')\n",
    "    \n",
    "    # 3. The \"Nanosecond Trick\"\n",
    "    # standard groupby().cumcount() numbers the items in a group: 0, 1, 2, 3...\n",
    "    # We group by 'epoch' to find trades happening at the same time.\n",
    "    # We treat that count as Nanoseconds.\n",
    "    \n",
    "    offset_ns = df.groupby('epoch').cumcount()\n",
    "    \n",
    "    if max(offset_ns) > 1000:\n",
    "        raise Exception(f\"There are more than 1000 executions happening at the same microsecond. Max number of execs: {offset_ns}\")\n",
    "    \n",
    "    # 4. Create the final High-Res Timestamp\n",
    "    # Base Time (us) + Offset (ns)\n",
    "    df['ts'] = temp_ts + pd.to_timedelta(offset_ns, unit='ns')\n",
    "    \n",
    "    # 5. Set Index\n",
    "    df.set_index('ts', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the new logic\n",
    "print(\"--- Processing XMAD (Preserving All) ---\")\n",
    "df_xmad_full = clean_hft_data_preserve_all(df_xmad)\n",
    "\n",
    "print(\"\\n--- Processing CEUX (Preserving All) ---\")\n",
    "df_ceux_full = clean_hft_data_preserve_all(df_ceux)\n",
    "\n",
    "# Verify Uniqueness\n",
    "print(f\"\\nIs XMAD index unique now? {df_xmad_full.index.is_unique}\")\n",
    "\n",
    "# Let's inspect a \"Burst\" (Trades that happened at the same microsecond)\n",
    "# We look for where the nanosecond part is not zero\n",
    "bursts = df_xmad_full[df_xmad_full.index.nanosecond > 0]\n",
    "\n",
    "if not bursts.empty:\n",
    "    print(\"\\nExample of a resolved collision (Look at the timestamps!):\")\n",
    "    # Show the base trade (ns=0) and the offset trade (ns=1)\n",
    "    t_base = bursts.index[0].replace(nanosecond=0)\n",
    "    print(df_xmad_full.loc[t_base : t_base + pd.Timedelta(nanoseconds=5), ['px', 'qty']])\n",
    "else:\n",
    "    print(\"No collisions found in this slice, but the code is ready for them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Consolidated Tape\n",
    "\n",
    "Now that we have cleaned the data for both venues (Madrid and CBOE Europe), we need to combine them into a single view of the market. This is often called a **Consolidated Tape**.\n",
    "\n",
    "**The Challenge: Asynchronous Data**\n",
    "\n",
    "Financial markets are *asynchronous*. A trade happens on BME at `t=100`, and a trade happens on CBOE at `t=105`. They never align perfectly.\n",
    "\n",
    "If we just join them, we will have gaps (NaNs). To solve this, we use the **Last Traded Price (LTP)** logic:\n",
    "\n",
    "> *\"If a trade didn't happen on this venue at this exact microsecond, the valid price is still the price of the **last** trade that did happen.\"*\n",
    "\n",
    "**The Solution:**\n",
    "\n",
    "1. **Concat & Sort:** Put all trades in one long list sorted by time.\n",
    "2. **Pivot:** Create columns for each venue (`XMAD`, `CEUX`).\n",
    "3. **Forward Fill (`ffill`):** This is the magic step. It propagates the last known valid observation forward to the next valid index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Combine\n",
    "tape_full = pd.concat([df_xmad_full, df_ceux_full])\n",
    "tape_full.sort_index(inplace=True)\n",
    "\n",
    "# 2. Pivot\n",
    "# Now strict uniqueness is guaranteed, so pivot works perfectly\n",
    "tape_pivot = tape_full.pivot(columns='mic', values='px')\n",
    "\n",
    "# 3. Fill Forward\n",
    "# This propagates the last valid price to the nanosecond level\n",
    "consolidated_tape = tape_pivot.ffill()\n",
    "\n",
    "print(f\"Total Updates in Tape: {len(consolidated_tape)}\")\n",
    "print(consolidated_tape.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instant Vectorized Calculation\n",
    "consolidated_tape['spread'] = consolidated_tape['XMAD'] - consolidated_tape['CEUX']\n",
    "display(consolidated_tape)\n",
    "\n",
    "# Visualize the discrepancy\n",
    "consolidated_tape['spread'].plot(title=\"Price Difference (XMAD - CEUX) in Euros\", figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Low-Frequency Data (News) with High-Frequency Data (Trades)\n",
    "\n",
    "We often need to enrich our trade data with external events (News, Macro announcements, Tweets). However, we face a major problem: **Timestamp Mismatch**.\n",
    "\n",
    "- **News arrives at:** `11:40:00.000000`\n",
    "- **Trade happens at:** `11:40:00.005231`\n",
    "\n",
    "A standard SQL-style `left_join` or pandas `merge` requires an **exact match**. Since the timestamps rarely match down to the microsecond, a standard merge would result in almost all data being lost.\n",
    "\n",
    "**The Solution: `merge_asof`**\n",
    "\n",
    "Pandas `merge_asof` allows for **inexact matches**. It tells the computer:\n",
    "\n",
    "> *\"For every trade, look **backwards** in time and find the most recent news headline that appeared before this trade.\"*\n",
    "\n",
    "- **`direction='backward'`**: Ensures we avoid \"look-ahead bias.\" We only tag a trade with news that was already public when the trade occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the News Dataframe\n",
    "news_data = pd.DataFrame([\n",
    "    {'ts': '2025-11-07 11:00:00', 'headline': 'Market Open', 'sentiment': 'NEUTRAL'},\n",
    "    {'ts': '2025-11-07 11:40:00', 'headline': 'RUMOR: Acquisition Talks', 'sentiment': 'POSITIVE'},\n",
    "    {'ts': '2025-11-07 12:10:00', 'headline': 'CORRECTION: Rumor Denied', 'sentiment': 'NEGATIVE'}\n",
    "])\n",
    "\n",
    "# Convert news time to datetime\n",
    "news_data['ts'] = pd.to_datetime(news_data['ts'])\n",
    "news_data.sort_values('ts', inplace=True)\n",
    "display(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare Trade Data\n",
    "# Use the cleaned XMAD data with unique timestamps\n",
    "trades_for_merge = df_xmad_full[['px', 'qty']].copy()\n",
    "trades_for_merge.reset_index(inplace=True)\n",
    "\n",
    "# 3. Use merge_asof\n",
    "# This is the key function for time-aware inexact joins\n",
    "enriched_trades = pd.merge_asof(\n",
    "    trades_for_merge,\n",
    "    news_data,\n",
    "    on='ts',\n",
    "    direction='backward'  # Look backwards in time to find the most recent news\n",
    ")\n",
    "\n",
    "print(\"Enriched Trades (Sample):\")\n",
    "display(enriched_trades.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis: What was the average trade price during each \"news regime\"?\n",
    "print(\"\\nAverage Price by News Sentiment:\")\n",
    "print(enriched_trades.groupby('sentiment')['px'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
