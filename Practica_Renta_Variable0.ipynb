{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab Exercise: High-Frequency Arbitrage in Fragmented Markets\n",
        "\n",
        "**Deadline:** 9th of December 23:59 CET\n",
        "\n",
        "**Submission:** Email to francisco.merlos@six-group.com with title: \"Arbitrage study in BME | Your name\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries for data processing and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set pandas display options for better readability\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Context: The Fragmented Market\n",
        "\n",
        "In modern European equity markets, liquidity is **fragmented**. The same stock (ISIN) trades simultaneously on the primary exchange (BME) and various Multilateral Trading Facilities (MTFs) like CBOE, Turquoise, and Aquis.\n",
        "\n",
        "Due to this fragmentation, temporary price discrepancies occur. A stock might be offered for sale at â‚¬10.00 on Turquoise while a buyer is bidding â‚¬10.01 on BME. A High-Frequency Trader (HFT) can profit from this by buying low and selling high instantaneously.\n",
        "\n",
        "However, these opportunities are fleeting. The \"theoretical\" profit you see in a snapshot might disappear by the time your order reaches the exchange due to **latency**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Mission\n",
        "\n",
        "You have been hired as a Quantitative Researcher at a proprietary trading firm. Your boss has given you a dataset of high-resolution market data and asked you to answer three critical questions:\n",
        "\n",
        "1. **Do arbitrage opportunities still exist in Spanish equities?**\n",
        "2. **What is the maximum theoretical profit** (assuming 0 latency)?\n",
        "3. **The \"Latency Decay\" Curve:** How quickly does this profit vanish as our trading system gets slower (from 0Âµs to 100ms)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Specifications\n",
        "\n",
        "You are provided with a `DATA_BIG/` folder containing subfolders for specific trading dates. Inside, you will find three types of compressed CSV files for various instruments.\n",
        "\n",
        "**Note:** You can also find a `DATA_SMALL` folder that you can use to test quickly without needing to run the simulation over all the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### File Naming Convention\n",
        "\n",
        "The naming pattern for all three file types (QTE, STS, TRD) is:\n",
        "\n",
        "```\n",
        "<type>_<session>_<isin>_<ticker>_<mic>_<part>.csv.gz\n",
        "```\n",
        "\n",
        "| Field | Description |\n",
        "|-------|-------------|\n",
        "| **type** | QTE, TRD, or STS |\n",
        "| **session** | Trading date (YYYY-MM-DD) |\n",
        "| **isin** | Cross-venue **ISIN** (International Securities Identification Number) |\n",
        "| **ticker** | Venue-specific trading symbol (distinguishes multiple books for the same ISIN on the same MIC) |\n",
        "| **mic** | Market Identifier Code (MIC, e.g., XMAD) |\n",
        "| **part** | Integer part number. Assume it is always 1 for simplicity. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Order Book Identity and Join Key\n",
        "\n",
        "A single **order book identity** is defined by the tuple:\n",
        "\n",
        "```\n",
        "(session, isin, mic, ticker)\n",
        "```\n",
        "\n",
        "This identity is the **key used to join** corresponding QTE, TRD, and STS data belonging to the same book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### File Types\n",
        "\n",
        "1. **QTE (Quotes/Snapshots):** Represents the state of the order book (up to 10 levels deep).\n",
        "   - `epoch`: Timestamp in microseconds (UTC).\n",
        "   - `px_bid_0`, `px_ask_0`: Best Bid and Best Ask prices.\n",
        "   - `qty_bid_0`, `qty_ask_0`: Available volume at the best price.\n",
        "   - *Note: Columns exist for levels 0-9.*\n",
        "\n",
        "2. **STS (Trading Status):** Updates on the market phase (e.g., Open, Auction, Closed).\n",
        "   - `epoch`: Timestamp.\n",
        "   - `market_trading_status`: An integer code representing the state.\n",
        "\n",
        "3. **TRD (Trades):** Represents the transactions. Not needed for this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CRITICAL: Vendor Data Definitions\n",
        "\n",
        "Real-world financial data is rarely clean. The data vendor has provided the following specifications. **Ignoring these will result in massive errors in your P&L calculation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### A. \"Magic Numbers\" (Special Price Codes)\n",
        "\n",
        "The vendor uses specific high-value constants to indicate non-tradable states (e.g., Market Orders during auctions). **These are NOT real prices.** If you treat 999,999 as a valid bid, your algorithm will assume you can sell for a million euros.\n",
        "\n",
        "| Value | Meaning | Action Required |\n",
        "|-------|---------|----------------|\n",
        "| 666666.666 | Unquoted/Unknown | **Discard** |\n",
        "| 999999.999 | Market Order (At Best) | **Discard** |\n",
        "| 999999.989 | At Open Order | **Discard** |\n",
        "| 999999.988 | At Close Order | **Discard** |\n",
        "| 999999.979 | Pegged Order | **Discard** |\n",
        "| 999999.123 | Unquoted/Unknown | **Discard** |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### B. Market Status Codes\n",
        "\n",
        "You can only trade when the market is in **Continuous Trading**. If you trade during an Auction, a Halt, or Pre-Open, your order will not execute immediately. A snapshot is only valid/addressable if the STS for that venue is one of these codes:\n",
        "\n",
        "| Venue | Continuous Trading Code |\n",
        "|-------|------------------------|\n",
        "| AQUIS | 5308427 |\n",
        "| BME | 5832713, 5832756 |\n",
        "| CBOE | 12255233 |\n",
        "| TURQUOISE | 7608181 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Implementation Guide\n",
        "\n",
        "You are encouraged to use AI tools (ChatGPT, Claude, etc.) to generate the Python/Pandas code. However, **you** are responsible for the logic and the financial validity of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Data Ingestion & Cleaning\n",
        "\n",
        "- Write a function to load the QTE and STS files for a given ISIN.\n",
        "- **Task:** Ensure you are using only valid prices\n",
        "- **Task:** Ensure you are only looking at addressable orderbooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 1: DATA INGESTION & CLEANING\n",
        "# ============================================================================\n",
        "# This step discovers all ISINs in DATA_BIG and loads QTE (quotes) and STS \n",
        "# (trading status) files for each ISIN across all venues.\n",
        "# ============================================================================\n",
        "\n",
        "# Define the base data directory and trading session\n",
        "DATA_DIR = 'DATA_BIG'\n",
        "SESSION = '2025-11-07'\n",
        "\n",
        "# Define venues (Market Identifier Codes) we need to process\n",
        "VENUES = ['BME', 'CBOE', 'TURQUOISE', 'AQUIS']\n",
        "\n",
        "# Define \"magic numbers\" that represent invalid/non-tradable prices\n",
        "# These are special codes used by the vendor to indicate non-executable states\n",
        "INVALID_PRICES = [\n",
        "    666666.666,  # Unquoted/Unknown\n",
        "    999999.999,  # Market Order (At Best)\n",
        "    999999.989,  # At Open Order\n",
        "    999999.988,  # At Close Order\n",
        "    999999.979,  # Pegged Order\n",
        "    999999.123   # Unquoted/Unknown\n",
        "]\n",
        "\n",
        "# Define valid market trading status codes for continuous trading per venue\n",
        "# Only orderbooks in continuous trading are \"addressable\" (can be traded)\n",
        "VALID_STATUS_CODES = {\n",
        "    'AQUIS': [5308427],\n",
        "    'BME': [5832713, 5832756],\n",
        "    'CBOE': [12255233],\n",
        "    'TURQUOISE': [7608181]\n",
        "}\n",
        "\n",
        "def discover_all_isins(data_dir=DATA_DIR, session=SESSION, venues=VENUES):\n",
        "    \"\"\"\n",
        "    Discovers all unique ISINs across all venues by scanning QTE files.\n",
        "    \n",
        "    In fragmented markets, the same ISIN trades on multiple venues. This function\n",
        "    scans all venue folders to build a comprehensive list of all available instruments.\n",
        "    \n",
        "    Returns:\n",
        "        set: Unique ISINs found across all venues\n",
        "    \"\"\"\n",
        "    isins = set()\n",
        "    \n",
        "    # Scan each venue folder for QTE files\n",
        "    for venue in venues:\n",
        "        venue_folder = f\"{data_dir}/{venue}_{session}\"\n",
        "        \n",
        "        if not os.path.exists(venue_folder):\n",
        "            print(f\"Warning: Venue folder {venue_folder} not found. Skipping.\")\n",
        "            continue\n",
        "        \n",
        "        # Find all QTE files in this venue\n",
        "        # File pattern: QTE_<session>_<isin>_<ticker>_<mic>_<part>.csv.gz\n",
        "        qte_files = glob(f\"{venue_folder}/QTE_*.csv.gz\")\n",
        "        \n",
        "        # Extract ISIN from each filename (3rd component after splitting by '_')\n",
        "        for file_path in qte_files:\n",
        "            filename = os.path.basename(file_path)\n",
        "            parts = filename.split('_')\n",
        "            if len(parts) >= 3:\n",
        "                isin = parts[2]  # ISIN is the 3rd component\n",
        "                isins.add(isin)\n",
        "    \n",
        "    print(f\"Discovered {len(isins)} unique ISINs across all venues\")\n",
        "    return sorted(isins)\n",
        "\n",
        "def load_qte_sts_for_isin(isin, data_dir=DATA_DIR, session=SESSION, venues=VENUES):\n",
        "    \"\"\"\n",
        "    Loads all QTE (quotes) and STS (trading status) files for a given ISIN across all venues.\n",
        "    \n",
        "    Market Microstructure Context:\n",
        "    - QTE files contain orderbook snapshots showing bid/ask prices and quantities\n",
        "    - STS files contain market phase information (Open, Continuous Trading, Auction, etc.)\n",
        "    - We need both to determine: 1) What prices are available, 2) Whether we can actually trade\n",
        "    \n",
        "    Args:\n",
        "        isin: The ISIN identifier (e.g., 'ES0113900J37')\n",
        "        data_dir: Base directory containing venue folders\n",
        "        session: Trading date (YYYY-MM-DD)\n",
        "        venues: List of venue codes to process\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (qte_dataframes_dict, sts_dataframes_dict)\n",
        "            - qte_dataframes_dict: {venue: DataFrame} mapping\n",
        "            - sts_dataframes_dict: {venue: DataFrame} mapping\n",
        "    \"\"\"\n",
        "    qte_dfs = {}\n",
        "    sts_dfs = {}\n",
        "    \n",
        "    # Process each venue\n",
        "    for venue in venues:\n",
        "        venue_folder = f\"{data_dir}/{venue}_{session}\"\n",
        "        \n",
        "        if not os.path.exists(venue_folder):\n",
        "            continue\n",
        "        \n",
        "        # Find QTE and STS files for this ISIN in this venue\n",
        "        # Note: There may be multiple tickers for the same ISIN on the same venue\n",
        "        qte_pattern = f\"{venue_folder}/QTE_{session}_{isin}_*_*.csv.gz\"\n",
        "        sts_pattern = f\"{venue_folder}/STS_{session}_{isin}_*_*.csv.gz\"\n",
        "        \n",
        "        qte_files = glob(qte_pattern)\n",
        "        sts_files = glob(sts_pattern)\n",
        "        \n",
        "        # Load all QTE files for this venue (may have multiple orderbooks)\n",
        "        if qte_files:\n",
        "            # Load and concatenate all QTE files for this ISIN+venue combination\n",
        "            qte_list = []\n",
        "            for qte_file in qte_files:\n",
        "                try:\n",
        "                    df_qte = pd.read_csv(qte_file, sep=';', compression='gzip')\n",
        "                    # Add metadata columns to identify the orderbook\n",
        "                    filename = os.path.basename(qte_file)\n",
        "                    parts = filename.split('_')\n",
        "                    if len(parts) >= 5:\n",
        "                        df_qte['ticker'] = parts[3]  # Extract ticker from filename\n",
        "                        df_qte['mic'] = parts[4]     # Extract MIC from filename\n",
        "                        df_qte['venue'] = venue      # Add venue identifier\n",
        "                    qte_list.append(df_qte)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {qte_file}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            if qte_list:\n",
        "                # Combine all QTE files for this venue\n",
        "                qte_combined = pd.concat(qte_list, ignore_index=True)\n",
        "                qte_dfs[venue] = qte_combined\n",
        "                print(f\"  {venue}: Loaded {len(qte_combined)} QTE records\")\n",
        "        \n",
        "        # Load all STS files for this venue\n",
        "        if sts_files:\n",
        "            sts_list = []\n",
        "            for sts_file in sts_files:\n",
        "                try:\n",
        "                    df_sts = pd.read_csv(sts_file, sep=';', compression='gzip')\n",
        "                    filename = os.path.basename(sts_file)\n",
        "                    parts = filename.split('_')\n",
        "                    if len(parts) >= 5:\n",
        "                        df_sts['ticker'] = parts[3]\n",
        "                        df_sts['mic'] = parts[4]\n",
        "                        df_sts['venue'] = venue\n",
        "                    sts_list.append(df_sts)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {sts_file}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            if sts_list:\n",
        "                sts_combined = pd.concat(sts_list, ignore_index=True)\n",
        "                sts_dfs[venue] = sts_combined\n",
        "                print(f\"  {venue}: Loaded {len(sts_combined)} STS records\")\n",
        "    \n",
        "    return qte_dfs, sts_dfs\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTE: Discover all ISINs and load QTE/STS files\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 1: DISCOVERING ISINs AND LOADING QTE/STS FILES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Discover all unique ISINs in the dataset\n",
        "all_isins = discover_all_isins()\n",
        "print(f\"\\nTotal unique ISINs found: {len(all_isins)}\")\n",
        "print(f\"First 10 ISINs: {all_isins[:10]}\\n\")\n",
        "\n",
        "# Load QTE and STS files for each ISIN\n",
        "# Note: We'll process all ISINs, but for large datasets, you might want to \n",
        "# process in batches or use lazy loading strategies\n",
        "print(\"\\nLoading QTE and STS files for all ISINs...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Store all loaded data in a dictionary structure\n",
        "# Structure: {isin: {'qte': {venue: df}, 'sts': {venue: df}}}\n",
        "all_data = {}\n",
        "\n",
        "for idx, isin in enumerate(all_isins, 1):\n",
        "    print(f\"\\n[{idx}/{len(all_isins)}] Processing ISIN: {isin}\")\n",
        "    qte_dfs, sts_dfs = load_qte_sts_for_isin(isin)\n",
        "    \n",
        "    if qte_dfs or sts_dfs:\n",
        "        all_data[isin] = {\n",
        "            'qte': qte_dfs,\n",
        "            'sts': sts_dfs\n",
        "        }\n",
        "    else:\n",
        "        print(f\"  Warning: No QTE/STS files found for {isin}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"STEP 1 COMPLETE: Loaded data for {len(all_data)} ISINs\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATA CLEANING & PREPARATION: Unified Function Following Reference Pattern\n",
        "# ============================================================================\n",
        "# This unified function combines cleaning and timestamp preparation following\n",
        "# the structure from 'Working with microsecond timeseries.py' (clean_hft_data_preserve_all).\n",
        "# \n",
        "# The function:\n",
        "# 1. Sorts by ['epoch', 'sequence'] FIRST (before any operations)\n",
        "# 2. Applies nanosecond trick to create unique timestamps\n",
        "# 3. Sets timestamp index\n",
        "# 4. Merges STS status and filters invalid prices/status codes\n",
        "# ============================================================================\n",
        "\n",
        "def clean_and_prepare_qte_data(qte_df, sts_df=None, venue=None, \n",
        "                                invalid_prices=INVALID_PRICES, \n",
        "                                valid_status_codes=VALID_STATUS_CODES):\n",
        "    \"\"\"\n",
        "    Unified function that cleans and prepares QTE data following the reference pattern.\n",
        "    \n",
        "    This function follows the structure from 'clean_hft_data_preserve_all' in the reference:\n",
        "    1. Sorts strictly by epoch and sequence (if available)\n",
        "    2. Applies nanosecond trick to handle duplicate epochs\n",
        "    3. Sets unique timestamp index\n",
        "    4. Merges STS trading status (if available)\n",
        "    5. Filters invalid prices and non-continuous trading periods\n",
        "    \n",
        "    Market Microstructure Context:\n",
        "    - Multiple orderbook snapshots can occur at the same microsecond\n",
        "    - Sequence column preserves the order of events within the same microsecond\n",
        "    - We can only trade during Continuous Trading (not during auctions/halts)\n",
        "    - Magic numbers represent non-executable orders (market orders, pegged orders, etc.)\n",
        "    - These must be filtered out to avoid false arbitrage signals\n",
        "    \n",
        "    Args:\n",
        "        qte_df: DataFrame with QTE (quote) data (must have 'epoch' column)\n",
        "        sts_df: Optional DataFrame with STS (trading status) data\n",
        "        venue: Optional venue identifier (BME, CBOE, etc.) for status code filtering\n",
        "        invalid_prices: List of magic numbers to filter out\n",
        "        valid_status_codes: Dict mapping venue to valid status codes\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with unique timestamp index, cleaned and filtered\n",
        "    \"\"\"\n",
        "    if qte_df.empty:\n",
        "        return qte_df\n",
        "    \n",
        "    # Make a copy to avoid modifying original\n",
        "    df = qte_df.copy()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 1: Sort strictly by epoch and sequence (if available)\n",
        "    # ========================================================================\n",
        "    # This is CRITICAL: We must sort by ['epoch', 'sequence'] BEFORE any\n",
        "    # time-based operations. The sequence column preserves the order of events\n",
        "    # within the same microsecond, which is essential for accurate time ordering.\n",
        "    # \n",
        "    # Market Microstructure: When multiple orderbook updates happen at the\n",
        "    # same microsecond, the sequence number indicates the order in which they\n",
        "    # occurred. This is crucial for maintaining chronological accuracy.\n",
        "    sort_cols = ['epoch']\n",
        "    if 'sequence' in df.columns:\n",
        "        sort_cols.append('sequence')\n",
        "        # Note: QTE files include sequence column to preserve event order\n",
        "    else:\n",
        "        # If sequence is missing, document why (shouldn't happen for QTE files)\n",
        "        print(\"    Warning: 'sequence' column not found in QTE data. Using epoch only for sorting.\")\n",
        "    \n",
        "    df = df.sort_values(by=sort_cols, ascending=[True] * len(sort_cols))\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 2: Apply the \"Nanosecond Trick\" to create unique timestamps\n",
        "    # ========================================================================\n",
        "    # Convert epoch to datetime (microseconds)\n",
        "    temp_ts = pd.to_datetime(df['epoch'], unit='us')\n",
        "    \n",
        "    # The \"Nanosecond Trick\" (from reference: clean_hft_data_preserve_all)\n",
        "    # groupby().cumcount() numbers items in a group: 0, 1, 2, 3...\n",
        "    # We group by 'epoch' to find snapshots happening at the same time.\n",
        "    # We treat that count as nanoseconds to create unique timestamps.\n",
        "    offset_ns = df.groupby('epoch').cumcount()\n",
        "    \n",
        "    # Safety check: ensure we don't exceed 1000 snapshots per microsecond\n",
        "    # (nanoseconds go from 0-999, so 1000 would overflow to next microsecond)\n",
        "    max_offset = offset_ns.max() if len(offset_ns) > 0 else 0\n",
        "    if max_offset >= 1000:\n",
        "        raise Exception(f\"Too many snapshots at the same microsecond. Max offset: {max_offset}\")\n",
        "    \n",
        "    # Create the final High-Resolution Timestamp\n",
        "    # Base Time (microseconds) + Offset (nanoseconds)\n",
        "    df['ts'] = temp_ts + pd.to_timedelta(offset_ns, unit='ns')\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 3: Set timestamp as index\n",
        "    # ========================================================================\n",
        "    # This creates a unique timestamp index that can be used for merge_asof,\n",
        "    # pivot operations, and time-based lookups\n",
        "    df.set_index('ts', inplace=True)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 4: Merge STS trading status (if available)\n",
        "    # ========================================================================\n",
        "    # Process STS data if available to filter by market trading status\n",
        "    if sts_df is not None and not sts_df.empty:\n",
        "        # Prepare STS data for merging\n",
        "        sts_clean = sts_df.copy()\n",
        "        sts_clean['ts'] = pd.to_datetime(sts_clean['epoch'], unit='us')\n",
        "        \n",
        "        # Sort STS by timestamp for merge_asof\n",
        "        sts_clean = sts_clean.sort_values('ts').reset_index(drop=True)\n",
        "        \n",
        "        # Get valid status codes for this venue\n",
        "        venue_valid_codes = valid_status_codes.get(venue, []) if venue else []\n",
        "        \n",
        "        # Merge STS status with QTE using merge_asof (backward direction)\n",
        "        # This assigns the most recent market status to each quote\n",
        "        # Direction='backward' ensures we only use status that was known at quote time\n",
        "        # (avoids look-ahead bias)\n",
        "        df = pd.merge_asof(\n",
        "            df.reset_index(),  # Reset index temporarily for merge_asof\n",
        "            sts_clean[['ts', 'market_trading_status']],\n",
        "            on='ts',\n",
        "            direction='backward'\n",
        "        )\n",
        "        \n",
        "        # Filter: Keep only rows where market status is in continuous trading\n",
        "        if venue_valid_codes:\n",
        "            df = df[df['market_trading_status'].isin(venue_valid_codes)].copy()\n",
        "        \n",
        "        # Drop the market_trading_status column (no longer needed)\n",
        "        df = df.drop(columns=['market_trading_status'], errors='ignore')\n",
        "        \n",
        "        # Set timestamp back as index\n",
        "        df.set_index('ts', inplace=True)\n",
        "    elif venue:\n",
        "        # If no STS data but venue is specified, warn but continue\n",
        "        print(f\"    Warning: No STS data for {venue}, cannot filter by trading status\")\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 5: Filter out invalid prices (magic numbers)\n",
        "    # ========================================================================\n",
        "    # Check both bid and ask prices at level 0 (best bid/ask)\n",
        "    price_columns = ['px_bid_0', 'px_ask_0']\n",
        "    \n",
        "    # Create a mask: True for rows with valid prices\n",
        "    valid_price_mask = pd.Series(True, index=df.index)\n",
        "    \n",
        "    for col in price_columns:\n",
        "        if col in df.columns:\n",
        "            # Check if price is in the invalid prices list\n",
        "            col_mask = ~df[col].isin(invalid_prices)\n",
        "            valid_price_mask = valid_price_mask & col_mask\n",
        "    \n",
        "    # Apply the filter\n",
        "    df = df[valid_price_mask].copy()\n",
        "    \n",
        "    return df\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTE: Clean all loaded QTE data\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CLEANING & PREPARING QTE DATA: Filtering Invalid Prices, Non-Continuous Trading, and Creating Unique Timestamps\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Clean QTE data for each ISIN\n",
        "cleaned_data = {}\n",
        "\n",
        "for isin, data in all_data.items():\n",
        "    print(f\"\\nCleaning and preparing data for ISIN: {isin}\")\n",
        "    qte_dfs = data.get('qte', {})\n",
        "    sts_dfs = data.get('sts', {})\n",
        "    \n",
        "    cleaned_qte = {}\n",
        "    \n",
        "    for venue in qte_dfs.keys():\n",
        "        qte_df = qte_dfs[venue]\n",
        "        sts_df = sts_dfs.get(venue, pd.DataFrame())\n",
        "        \n",
        "        # Get original count\n",
        "        original_count = len(qte_df)\n",
        "        \n",
        "        # Clean and prepare the data (unified function)\n",
        "        qte_cleaned = clean_and_prepare_qte_data(qte_df, sts_df, venue)\n",
        "        \n",
        "        # Get cleaned count\n",
        "        cleaned_count = len(qte_cleaned)\n",
        "        removed_count = original_count - cleaned_count\n",
        "        \n",
        "        if original_count > 0:\n",
        "            removal_pct = (removed_count / original_count) * 100\n",
        "            print(f\"  {venue}: {original_count:,} -> {cleaned_count:,} records \"\n",
        "                  f\"({removed_count:,} removed, {removal_pct:.2f}%)\")\n",
        "        \n",
        "        if cleaned_count > 0:\n",
        "            cleaned_qte[venue] = qte_cleaned\n",
        "    \n",
        "    if cleaned_qte:\n",
        "        cleaned_data[isin] = {\n",
        "            'qte': cleaned_qte,\n",
        "            'sts': sts_dfs  # STS data unchanged\n",
        "        }\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"CLEANING & PREPARATION COMPLETE: Processed data for {len(cleaned_data)} ISINs\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Update all_data with cleaned and prepared version\n",
        "all_data = cleaned_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# NOTE: Timestamp Preparation is Now Integrated\n",
        "# ============================================================================\n",
        "# The timestamp preparation (nanosecond trick) is now integrated into the\n",
        "# unified clean_and_prepare_qte_data() function in the previous cell.\n",
        "# This ensures proper sorting by ['epoch', 'sequence'] before any operations.\n",
        "# \n",
        "# The data in all_data is already cleaned and prepared with unique timestamps.\n",
        "# ============================================================================\n",
        "\n",
        "# Verification: Check that timestamps are unique\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"VERIFICATION: Checking Timestamp Uniqueness\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "unique_check_passed = True\n",
        "for isin, data in all_data.items():\n",
        "    qte_dfs = data.get('qte', {})\n",
        "    for venue, qte_df in qte_dfs.items():\n",
        "        if not qte_df.empty:\n",
        "            is_unique = qte_df.index.is_unique\n",
        "            if not is_unique:\n",
        "                print(f\"  WARNING: {isin} on {venue} has non-unique timestamps!\")\n",
        "                unique_check_passed = False\n",
        "\n",
        "if unique_check_passed:\n",
        "    print(\"  âœ“ All timestamps are unique across all ISINs and venues\")\n",
        "else:\n",
        "    print(\"  âœ— Some timestamps are not unique - please check the data\")\n",
        "\n",
        "# Note: The old prepare_qte_timestamps function has been removed as it's now\n",
        "# integrated into clean_and_prepare_qte_data() following the reference pattern.\n",
        "\n",
        "# ============================================================================\n",
        "# DEMONSTRATION: Nanosecond Trick for Duplicate Epochs\n",
        "# ============================================================================\n",
        "# Similar to the reference notebook, we demonstrate how the nanosecond trick\n",
        "# handles multiple snapshots at the same microsecond\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DEMONSTRATION: Handling Duplicate Epochs with Nanosecond Trick\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Find an example with duplicate epochs (one that needed the nanosecond trick)\n",
        "example_found = False\n",
        "example_isin = None\n",
        "example_venue = None\n",
        "example_df = None\n",
        "\n",
        "for isin, data in all_data.items():\n",
        "    qte_dfs = data.get('qte', {})\n",
        "    for venue, qte_df in qte_dfs.items():\n",
        "        if not qte_df.empty:\n",
        "            # Check for duplicate epochs (before preparation, but we can check the index)\n",
        "            # Look for timestamps with nanosecond offsets (indicating duplicates were resolved)\n",
        "            bursts = qte_df[qte_df.index.nanosecond > 0]\n",
        "            if not bursts.empty:\n",
        "                example_isin = isin\n",
        "                example_venue = venue\n",
        "                example_df = qte_df\n",
        "                example_found = True\n",
        "                break\n",
        "    if example_found:\n",
        "        break\n",
        "\n",
        "if example_found and example_df is not None:\n",
        "    print(f\"\\nðŸ“Š Example: {example_isin} on {example_venue}\")\n",
        "    print(f\"   Total snapshots: {len(example_df):,}\")\n",
        "    \n",
        "    # Check for duplicate epochs (like in reference notebook lines 1-3)\n",
        "    # We need to check the original epoch column to see duplicates\n",
        "    duplicates = example_df.duplicated(subset='epoch', keep=False).sum()\n",
        "    print(f\"   Found {duplicates} snapshots sharing the same microsecond (duplicate epochs)\")\n",
        "    \n",
        "    # Find a specific burst example\n",
        "    bursts = example_df[example_df.index.nanosecond > 0]\n",
        "    if not bursts.empty:\n",
        "        print(f\"   Found {len(bursts)} snapshots with nanosecond offsets (collisions resolved)\")\n",
        "        \n",
        "        # Get the first burst timestamp\n",
        "        burst_ts = bursts.index[0]\n",
        "        base_ts = burst_ts.replace(nanosecond=0)\n",
        "        \n",
        "        # Get all snapshots at this base timestamp (the \"burst\")\n",
        "        burst_snapshots = example_df.loc[base_ts : base_ts + pd.Timedelta(nanoseconds=999)]\n",
        "        \n",
        "        print(f\"\\nðŸ’¥ Example of a resolved collision (Look at the timestamps!):\")\n",
        "        print(f\"   Base timestamp: {base_ts}\")\n",
        "        print(f\"   Number of snapshots at this microsecond: {len(burst_snapshots)}\")\n",
        "        print(f\"\\n   All snapshots at this microsecond:\")\n",
        "        \n",
        "        # Show relevant columns including sequence if available\n",
        "        display_cols = ['epoch', 'px_bid_0', 'px_ask_0', 'qty_bid_0', 'qty_ask_0']\n",
        "        if 'sequence' in burst_snapshots.columns:\n",
        "            display_cols.insert(1, 'sequence')\n",
        "        \n",
        "        print(burst_snapshots[display_cols].to_string())\n",
        "        \n",
        "        print(f\"\\n   Note: Each snapshot has a unique timestamp index with nanosecond offsets\")\n",
        "        print(f\"   This preserves the order (by sequence if available) while ensuring uniqueness\")\n",
        "    else:\n",
        "        print(\"   No duplicate epochs found in this example\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  No examples with duplicate epochs found to demonstrate\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DESCRIPTIVE SUMMARY: Overview of Loaded Data\n",
        "# ============================================================================\n",
        "# This section creates a summary DataFrame showing key statistics about the\n",
        "# downloaded data: venues, ISINs, record counts, etc.\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DESCRIPTIVE SUMMARY OF LOADED DATA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Initialize summary statistics\n",
        "summary_data = []\n",
        "\n",
        "# Analyze each ISIN\n",
        "for isin, data in all_data.items():\n",
        "    qte_dfs = data.get('qte', {})\n",
        "    sts_dfs = data.get('sts', {})\n",
        "    \n",
        "    # Count records per venue for this ISIN\n",
        "    for venue in VENUES:\n",
        "        qte_count = len(qte_dfs.get(venue, pd.DataFrame()))\n",
        "        sts_count = len(sts_dfs.get(venue, pd.DataFrame()))\n",
        "        \n",
        "        if qte_count > 0 or sts_count > 0:\n",
        "            summary_data.append({\n",
        "                'ISIN': isin,\n",
        "                'Venue': venue,\n",
        "                'QTE_Records': qte_count,\n",
        "                'STS_Records': sts_count,\n",
        "                'Total_Records': qte_count + sts_count\n",
        "            })\n",
        "\n",
        "# Create summary DataFrame\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "\n",
        "if len(df_summary) > 0:\n",
        "    # Overall statistics\n",
        "    print(f\"\\nðŸ“Š OVERALL STATISTICS:\")\n",
        "    print(f\"   â€¢ Total unique ISINs: {df_summary['ISIN'].nunique()}\")\n",
        "    print(f\"   â€¢ Total venue-ISIN combinations: {len(df_summary)}\")\n",
        "    print(f\"   â€¢ Total QTE records: {df_summary['QTE_Records'].sum():,}\")\n",
        "    print(f\"   â€¢ Total STS records: {df_summary['STS_Records'].sum():,}\")\n",
        "    print(f\"   â€¢ Total records: {df_summary['Total_Records'].sum():,}\")\n",
        "    \n",
        "    # Statistics by venue\n",
        "    print(f\"\\nðŸ¢ STATISTICS BY VENUE:\")\n",
        "    venue_stats = df_summary.groupby('Venue').agg({\n",
        "        'ISIN': 'nunique',\n",
        "        'QTE_Records': 'sum',\n",
        "        'STS_Records': 'sum',\n",
        "        'Total_Records': 'sum'\n",
        "    }).rename(columns={'ISIN': 'Unique_ISINs'})\n",
        "    venue_stats = venue_stats.sort_values('Total_Records', ascending=False)\n",
        "    print(venue_stats.to_string())\n",
        "    \n",
        "    # Statistics by ISIN (top 10)\n",
        "    print(f\"\\nðŸ“ˆ TOP 10 ISINs BY TOTAL RECORDS:\")\n",
        "    isin_stats = df_summary.groupby('ISIN').agg({\n",
        "        'Venue': 'count',\n",
        "        'QTE_Records': 'sum',\n",
        "        'STS_Records': 'sum',\n",
        "        'Total_Records': 'sum'\n",
        "    }).rename(columns={'Venue': 'Num_Venues'})\n",
        "    isin_stats = isin_stats.sort_values('Total_Records', ascending=False)\n",
        "    print(isin_stats.head(10).to_string())\n",
        "    \n",
        "    # Venue coverage per ISIN\n",
        "    print(f\"\\nðŸ”— VENUE COVERAGE:\")\n",
        "    venue_coverage = df_summary.groupby('ISIN')['Venue'].count()\n",
        "    coverage_stats = venue_coverage.value_counts().sort_index()\n",
        "    print(\"Number of venues per ISIN:\")\n",
        "    for num_venues, count in coverage_stats.items():\n",
        "        print(f\"   â€¢ {num_venues} venue(s): {count} ISIN(s)\")\n",
        "    \n",
        "    # Statistics about duplicate epochs (nanosecond trick usage)\n",
        "    print(f\"\\nâ±ï¸  DUPLICATE EPOCHS STATISTICS (Nanosecond Trick Required):\")\n",
        "    duplicate_epoch_stats = []\n",
        "    \n",
        "    for isin, data in all_data.items():\n",
        "        qte_dfs = data.get('qte', {})\n",
        "        for venue, qte_df in qte_dfs.items():\n",
        "            if not qte_df.empty:\n",
        "                # Count snapshots with nanosecond offsets (indicating duplicate epochs)\n",
        "                bursts = qte_df[qte_df.index.nanosecond > 0]\n",
        "                num_bursts = len(bursts)\n",
        "                \n",
        "                if num_bursts > 0:\n",
        "                    # Count unique epochs that had duplicates\n",
        "                    burst_epochs = qte_df.loc[bursts.index, 'epoch'].unique()\n",
        "                    num_duplicate_epochs = len(burst_epochs)\n",
        "                    \n",
        "                    # Find max number of snapshots at a single epoch\n",
        "                    epoch_counts = qte_df.groupby('epoch').size()\n",
        "                    max_snapshots_per_epoch = epoch_counts.max()\n",
        "                    \n",
        "                    duplicate_epoch_stats.append({\n",
        "                        'ISIN': isin,\n",
        "                        'Venue': venue,\n",
        "                        'Snapshots_With_NS_Offset': num_bursts,\n",
        "                        'Unique_Epochs_With_Duplicates': num_duplicate_epochs,\n",
        "                        'Max_Snapshots_Per_Epoch': max_snapshots_per_epoch\n",
        "                    })\n",
        "    \n",
        "    if duplicate_epoch_stats:\n",
        "        df_dup_epochs = pd.DataFrame(duplicate_epoch_stats)\n",
        "        total_dup_combinations = len(df_dup_epochs)\n",
        "        total_snapshots_with_offset = df_dup_epochs['Snapshots_With_NS_Offset'].sum()\n",
        "        total_dup_epochs = df_dup_epochs['Unique_Epochs_With_Duplicates'].sum()\n",
        "        max_snapshots = df_dup_epochs['Max_Snapshots_Per_Epoch'].max()\n",
        "        \n",
        "        print(f\"   â€¢ Venue-ISIN combinations with duplicate epochs: {total_dup_combinations}\")\n",
        "        print(f\"   â€¢ Total snapshots requiring nanosecond offsets: {total_snapshots_with_offset:,}\")\n",
        "        print(f\"   â€¢ Total unique epochs with duplicates: {total_dup_epochs:,}\")\n",
        "        print(f\"   â€¢ Maximum snapshots at a single epoch: {max_snapshots}\")\n",
        "        print(f\"\\n   Top 5 by number of duplicate epochs:\")\n",
        "        top_5_dup = df_dup_epochs.nlargest(5, 'Unique_Epochs_With_Duplicates')\n",
        "        print(top_5_dup[['ISIN', 'Venue', 'Unique_Epochs_With_Duplicates', 'Max_Snapshots_Per_Epoch']].to_string(index=False))\n",
        "    else:\n",
        "        print(f\"   â€¢ No duplicate epochs found - all snapshots have unique microsecond timestamps\")\n",
        "    \n",
        "    # Create a detailed summary DataFrame for display\n",
        "    print(f\"\\nðŸ“‹ DETAILED SUMMARY DATAFRAME (First 20 rows):\")\n",
        "    print(df_summary.head(20).to_string(index=False))\n",
        "    \n",
        "    # Store the summary DataFrame for later use\n",
        "    print(f\"\\nâœ… Summary DataFrame stored as 'df_summary' ({len(df_summary)} rows)\")\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸  No data loaded. Please check Step 1 execution.\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STATISTICS: Epochs Requiring Nanosecond Trick\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STATISTICS: Epochs Requiring Nanosecond Trick\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Collect statistics about duplicate epochs\n",
        "duplicate_stats = []\n",
        "\n",
        "for isin, data in all_data.items():\n",
        "    qte_dfs = data.get('qte', {})\n",
        "    for venue, qte_df in qte_dfs.items():\n",
        "        if not qte_df.empty:\n",
        "            # Count snapshots with nanosecond offsets (indicating duplicate epochs were resolved)\n",
        "            bursts = qte_df[qte_df.index.nanosecond > 0]\n",
        "            num_bursts = len(bursts)\n",
        "            \n",
        "            if num_bursts > 0:\n",
        "                # Count unique epochs that had duplicates\n",
        "                burst_epochs = qte_df.loc[bursts.index, 'epoch'].unique()\n",
        "                num_duplicate_epochs = len(burst_epochs)\n",
        "                \n",
        "                # Find max number of snapshots at a single epoch\n",
        "                epoch_counts = qte_df.groupby('epoch').size()\n",
        "                max_snapshots_per_epoch = epoch_counts.max()\n",
        "                \n",
        "                duplicate_stats.append({\n",
        "                    'ISIN': isin,\n",
        "                    'Venue': venue,\n",
        "                    'Total_Snapshots': len(qte_df),\n",
        "                    'Snapshots_With_NS_Offset': num_bursts,\n",
        "                    'Unique_Epochs_With_Duplicates': num_duplicate_epochs,\n",
        "                    'Max_Snapshots_Per_Epoch': max_snapshots_per_epoch\n",
        "                })\n",
        "\n",
        "if duplicate_stats:\n",
        "    df_duplicate_stats = pd.DataFrame(duplicate_stats)\n",
        "    \n",
        "    print(f\"\\nðŸ“ˆ Summary of Epochs Requiring Nanosecond Trick:\")\n",
        "    print(f\"   â€¢ Total venue-ISIN combinations with duplicate epochs: {len(df_duplicate_stats)}\")\n",
        "    print(f\"   â€¢ Total snapshots requiring nanosecond offsets: {df_duplicate_stats['Snapshots_With_NS_Offset'].sum():,}\")\n",
        "    print(f\"   â€¢ Total unique epochs with duplicates: {df_duplicate_stats['Unique_Epochs_With_Duplicates'].sum():,}\")\n",
        "    print(f\"   â€¢ Maximum snapshots at a single epoch: {df_duplicate_stats['Max_Snapshots_Per_Epoch'].max()}\")\n",
        "    \n",
        "    print(f\"\\nðŸ“‹ Top 10 ISIN-Venue combinations by number of duplicate epochs:\")\n",
        "    top_duplicates = df_duplicate_stats.nlargest(10, 'Unique_Epochs_With_Duplicates')\n",
        "    print(top_duplicates[['ISIN', 'Venue', 'Total_Snapshots', 'Unique_Epochs_With_Duplicates', \n",
        "                          'Max_Snapshots_Per_Epoch']].to_string(index=False))\n",
        "    \n",
        "    print(f\"\\nðŸ’¡ Market Microstructure Context:\")\n",
        "    print(f\"   Multiple orderbook snapshots at the same microsecond occur when:\")\n",
        "    print(f\"   - Rapid orderbook updates happen faster than microsecond resolution\")\n",
        "    print(f\"   - Market data feeds batch multiple updates together\")\n",
        "    print(f\"   - High-frequency trading activity creates bursts of updates\")\n",
        "    print(f\"   The nanosecond trick preserves all snapshots while maintaining chronological order\")\n",
        "else:\n",
        "    print(\"\\nâœ… No duplicate epochs found - all snapshots have unique microsecond timestamps\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create the \"Consolidated Tape\"\n",
        "\n",
        "- To detect arbitrage, you need to compare prices across venues *at the exact same time*.\n",
        "- **Task:** Create a single DataFrame where the index is the timestamp, and the columns represent the Best Bid and Best Ask for **every** venue (BME, XMAD, CBOE, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 2: CREATE THE CONSOLIDATED TAPE\n",
        "# ============================================================================\n",
        "# The consolidated tape combines orderbook snapshots from all venues into\n",
        "# a single time-indexed DataFrame, allowing us to compare prices across\n",
        "# venues at the exact same timestamp.\n",
        "#\n",
        "# Market Microstructure Context:\n",
        "# - Financial markets are asynchronous: updates happen at different times\n",
        "#   on different venues\n",
        "# - To detect arbitrage, we need to know what prices are available on ALL\n",
        "#   venues at the SAME moment in time\n",
        "# - We use \"Last Traded Price\" (LTP) logic: if a venue hasn't updated at\n",
        "#   time T, we use the last known price from that venue\n",
        "# - Forward fill (ffill) propagates the last known bid/ask forward until\n",
        "#   a new update arrives\n",
        "# ============================================================================\n",
        "\n",
        "def create_consolidated_tape_for_isin(isin, cleaned_data_dict):\n",
        "    \"\"\"\n",
        "    Creates a consolidated tape for a single ISIN across all venues.\n",
        "    \n",
        "    The consolidated tape is a DataFrame where:\n",
        "    - Index: Timestamp (nanosecond precision)\n",
        "    - Columns: Best Bid and Best Ask for each venue\n",
        "      (e.g., 'BME_bid', 'BME_ask', 'CBOE_bid', 'CBOE_ask', etc.)\n",
        "    \n",
        "    Market Microstructure Context:\n",
        "    - Each venue updates its orderbook independently\n",
        "    - We need to see all venues' prices at the same timestamp to detect\n",
        "      arbitrage opportunities\n",
        "    - Forward fill ensures that if BME updates at t=100Âµs and CBOE updates\n",
        "      at t=105Âµs, we can still compare prices at t=102Âµs using BME's last\n",
        "      known price\n",
        "    \n",
        "    Args:\n",
        "        isin: The ISIN identifier\n",
        "        cleaned_data_dict: Dictionary with structure {isin: {'qte': {venue: df}}}\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with timestamp index and columns for each venue's bid/ask\n",
        "    \"\"\"\n",
        "    if isin not in cleaned_data_dict:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    qte_dfs = cleaned_data_dict[isin].get('qte', {})\n",
        "    \n",
        "    if not qte_dfs:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 1: Extract Best Bid and Ask from Each Venue\n",
        "    # ========================================================================\n",
        "    # For each venue, we extract px_bid_0 (best bid) and px_ask_0 (best ask)\n",
        "    # along with their quantities. We also preserve the venue identifier\n",
        "    # so we can distinguish between venues after concatenation.\n",
        "    \n",
        "    venue_data_list = []\n",
        "    \n",
        "    for venue, qte_df in qte_dfs.items():\n",
        "        if qte_df.empty:\n",
        "            continue\n",
        "        \n",
        "        # Extract only the columns we need: best bid/ask prices and quantities\n",
        "        # Market Microstructure: We use level 0 (best bid/ask) because these\n",
        "        # are the prices at which we can immediately execute trades\n",
        "        venue_subset = qte_df[['px_bid_0', 'px_ask_0', 'qty_bid_0', 'qty_ask_0']].copy()\n",
        "        \n",
        "        # Add venue identifier as a column (needed for pivot operation)\n",
        "        venue_subset['venue'] = venue\n",
        "        \n",
        "        # Rename columns to include venue prefix for clarity\n",
        "        # This will help us identify which venue each price comes from\n",
        "        venue_subset = venue_subset.rename(columns={\n",
        "            'px_bid_0': 'bid',\n",
        "            'px_ask_0': 'ask',\n",
        "            'qty_bid_0': 'bid_qty',\n",
        "            'qty_ask_0': 'ask_qty'\n",
        "        })\n",
        "        \n",
        "        venue_data_list.append(venue_subset)\n",
        "    \n",
        "    if not venue_data_list:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 2: Combine All Venues into Single DataFrame\n",
        "    # ========================================================================\n",
        "    # Concatenate all venue data into one long DataFrame, preserving the\n",
        "    # timestamp index from each venue's cleaned QTE data.\n",
        "    # \n",
        "    # Market Microstructure: This creates a unified timeline where we can\n",
        "    # see all orderbook updates from all venues in chronological order.\n",
        "    \n",
        "    tape_full = pd.concat(venue_data_list)\n",
        "    \n",
        "    # Sort by timestamp to ensure chronological order\n",
        "    # This is critical: we need events in time order for forward fill to work\n",
        "    tape_full.sort_index(inplace=True)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 3: Create Separate DataFrames for Bid and Ask\n",
        "    # ========================================================================\n",
        "    # We need to pivot bid and ask separately because pivot can only handle\n",
        "    # one value column at a time. We'll create two pivoted DataFrames and\n",
        "    # then combine them.\n",
        "    \n",
        "    # Pivot bids: creates columns like 'BME', 'CBOE', etc. with bid prices\n",
        "    # Market Microstructure: Each column represents the best bid price\n",
        "    # available on that venue at each timestamp\n",
        "    tape_bid_pivot = tape_full.pivot(columns='venue', values='bid')\n",
        "    \n",
        "    # Pivot asks: creates columns like 'BME', 'CBOE', etc. with ask prices\n",
        "    # Market Microstructure: Each column represents the best ask price\n",
        "    # available on that venue at each timestamp\n",
        "    tape_ask_pivot = tape_full.pivot(columns='venue', values='ask')\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 4: Forward Fill to Propagate Last Known Prices\n",
        "    # ========================================================================\n",
        "    # Forward fill (ffill) is the key operation that makes the consolidated\n",
        "    # tape work. It propagates the last known price forward until a new\n",
        "    # update arrives.\n",
        "    #\n",
        "    # Market Microstructure: This implements the \"Last Traded Price\" (LTP)\n",
        "    # logic. If BME updates at t=100Âµs and CBOE updates at t=105Âµs, then:\n",
        "    # - At t=100Âµs: BME has new price, CBOE still has old price\n",
        "    # - At t=101-104Âµs: Both venues have their last known prices\n",
        "    # - At t=105Âµs: CBOE updates, BME still has price from t=100Âµs\n",
        "    #\n",
        "    # This allows us to compare prices across venues at ANY timestamp,\n",
        "    # not just when all venues update simultaneously.\n",
        "    \n",
        "    tape_bid_ffill = tape_bid_pivot.ffill()\n",
        "    tape_ask_ffill = tape_ask_pivot.ffill()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 5: Rename Columns to Include Bid/Ask Suffix\n",
        "    # ========================================================================\n",
        "    # Rename columns to make it clear which is bid and which is ask\n",
        "    # This creates columns like 'BME_bid', 'BME_ask', 'CBOE_bid', etc.\n",
        "    \n",
        "    tape_bid_ffill.columns = [f'{venue}_bid' for venue in tape_bid_ffill.columns]\n",
        "    tape_ask_ffill.columns = [f'{venue}_ask' for venue in tape_ask_ffill.columns]\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 6: Combine Bid and Ask into Final Consolidated Tape\n",
        "    # ========================================================================\n",
        "    # Join the bid and ask DataFrames together to create the final consolidated\n",
        "    # tape with all venue prices in one place.\n",
        "    \n",
        "    consolidated_tape = pd.concat([tape_bid_ffill, tape_ask_ffill], axis=1)\n",
        "    \n",
        "    # Sort columns alphabetically for consistency (optional, but makes output cleaner)\n",
        "    consolidated_tape = consolidated_tape.sort_index(axis=1)\n",
        "    \n",
        "    return consolidated_tape\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTE: Create Consolidated Tape for All ISINs\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 2: CREATING CONSOLIDATED TAPE FOR ALL ISINs\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Store consolidated tapes for all ISINs\n",
        "consolidated_tapes = {}\n",
        "\n",
        "# Process each ISIN\n",
        "for idx, isin in enumerate(all_data.keys(), 1):\n",
        "    print(f\"\\n[{idx}/{len(all_data)}] Creating consolidated tape for ISIN: {isin}\")\n",
        "    \n",
        "    # Create consolidated tape for this ISIN\n",
        "    tape = create_consolidated_tape_for_isin(isin, all_data)\n",
        "    \n",
        "    if not tape.empty:\n",
        "        consolidated_tapes[isin] = tape\n",
        "        \n",
        "        # Display summary statistics\n",
        "        num_venues = len([col for col in tape.columns if '_bid' in col])\n",
        "        num_updates = len(tape)\n",
        "        \n",
        "        # Count non-null values to see data coverage\n",
        "        bid_cols = [col for col in tape.columns if '_bid' in col]\n",
        "        ask_cols = [col for col in tape.columns if '_ask' in col]\n",
        "        \n",
        "        total_bid_updates = tape[bid_cols].notna().sum().sum() if bid_cols else 0\n",
        "        total_ask_updates = tape[ask_cols].notna().sum().sum() if ask_cols else 0\n",
        "        \n",
        "        print(f\"  âœ“ Created tape with {num_venues} venue(s)\")\n",
        "        print(f\"  âœ“ Total timestamps: {num_updates:,}\")\n",
        "        print(f\"  âœ“ Total bid updates: {total_bid_updates:,}\")\n",
        "        print(f\"  âœ“ Total ask updates: {total_ask_updates:,}\")\n",
        "        \n",
        "        # Show first few rows as example\n",
        "        if idx <= 3:  # Only show for first 3 ISINs to avoid clutter\n",
        "            print(f\"\\n  Sample of consolidated tape (first 5 rows):\")\n",
        "            print(tape.head().to_string())\n",
        "    else:\n",
        "        print(f\"  âš ï¸  No data available for consolidated tape\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"STEP 2 COMPLETE: Created consolidated tapes for {len(consolidated_tapes)} ISINs\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VERIFICATION: Check Consolidated Tape Structure\n",
        "# ============================================================================\n",
        "# This verification demonstrates the consolidated tape structure using an\n",
        "# ISIN that has multiple venues and uses the nanosecond trick (has duplicate\n",
        "# epochs that were resolved).\n",
        "# ============================================================================\n",
        "\n",
        "if consolidated_tapes:\n",
        "    # Find an ISIN with multiple venues (3-4) that also uses nanosecond trick\n",
        "    # Market Microstructure: We want to demonstrate the consolidated tape\n",
        "    # with a realistic example that shows:\n",
        "    # 1. Multiple venues trading the same ISIN (fragmented market)\n",
        "    # 2. Nanosecond trick usage (multiple snapshots at same microsecond)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"VERIFICATION: Consolidated Tape Structure\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Search for an ISIN with multiple venues and nanosecond trick usage\n",
        "    example_isin = None\n",
        "    example_tape = None\n",
        "    max_venues = 0\n",
        "    has_nanosecond_trick = False\n",
        "    \n",
        "    # Priority: Find ISIN with 4 venues that uses nanosecond trick\n",
        "    # Known examples from Step 1: ES0113900J37, ES0177542018 have 4 venues\n",
        "    candidate_isins = ['ES0113900J37', 'ES0177542018', 'ES0113211835', 'ES0140609019']\n",
        "    \n",
        "    for candidate_isin in candidate_isins:\n",
        "        if candidate_isin in consolidated_tapes:\n",
        "            tape = consolidated_tapes[candidate_isin]\n",
        "            num_venues = len([col for col in tape.columns if '_bid' in col])\n",
        "            \n",
        "            # Check if this ISIN uses nanosecond trick (has timestamps with nanosecond offsets)\n",
        "            if candidate_isin in all_data:\n",
        "                qte_dfs = all_data[candidate_isin].get('qte', {})\n",
        "                for venue, qte_df in qte_dfs.items():\n",
        "                    if not qte_df.empty:\n",
        "                        # Check for nanosecond offsets (indicating nanosecond trick was used)\n",
        "                        bursts = qte_df[qte_df.index.nanosecond > 0]\n",
        "                        if not bursts.empty and num_venues >= 3:\n",
        "                            example_isin = candidate_isin\n",
        "                            example_tape = tape\n",
        "                            max_venues = num_venues\n",
        "                            has_nanosecond_trick = True\n",
        "                            break\n",
        "                if has_nanosecond_trick:\n",
        "                    break\n",
        "    \n",
        "    # If no candidate found, find any ISIN with 3+ venues\n",
        "    if example_isin is None:\n",
        "        for isin, tape in consolidated_tapes.items():\n",
        "            num_venues = len([col for col in tape.columns if '_bid' in col])\n",
        "            if num_venues >= 3 and num_venues > max_venues:\n",
        "                example_isin = isin\n",
        "                example_tape = tape\n",
        "                max_venues = num_venues\n",
        "    \n",
        "    # If still no candidate, use first available\n",
        "    if example_isin is None and consolidated_tapes:\n",
        "        example_isin = list(consolidated_tapes.keys())[0]\n",
        "        example_tape = consolidated_tapes[example_isin]\n",
        "        max_venues = len([col for col in example_tape.columns if '_bid' in col])\n",
        "    \n",
        "    if example_isin and example_tape is not None:\n",
        "        print(f\"\\nðŸ“Š Example ISIN: {example_isin}\")\n",
        "        print(f\"   Number of venues: {max_venues}\")\n",
        "        print(f\"   Uses nanosecond trick: {has_nanosecond_trick}\")\n",
        "        \n",
        "        # Get venue names\n",
        "        bid_cols = [col for col in example_tape.columns if '_bid' in col]\n",
        "        venues = [col.replace('_bid', '') for col in bid_cols]\n",
        "        print(f\"   Venues: {', '.join(venues)}\")\n",
        "        \n",
        "        print(f\"\\nðŸ“ Consolidated Tape Structure:\")\n",
        "        print(f\"   Shape: {example_tape.shape} (rows, columns)\")\n",
        "        print(f\"   Index Type: {type(example_tape.index).__name__}\")\n",
        "        print(f\"   Index Unique: {example_tape.index.is_unique} âœ“\")\n",
        "        print(f\"   Total Timestamps: {len(example_tape):,}\")\n",
        "        \n",
        "        print(f\"\\nðŸ“‹ Column Names ({len(example_tape.columns)} total):\")\n",
        "        for col in sorted(example_tape.columns):\n",
        "            non_null_count = example_tape[col].notna().sum()\n",
        "            pct_coverage = (non_null_count / len(example_tape)) * 100\n",
        "            print(f\"   â€¢ {col:15s} ({non_null_count:>8,} values, {pct_coverage:>5.1f}% coverage)\")\n",
        "        \n",
        "        print(f\"\\nðŸ“Š Sample Data (first 10 rows):\")\n",
        "        print(example_tape.head(10).to_string())\n",
        "        \n",
        "        # Demonstrate nanosecond trick if applicable\n",
        "        if has_nanosecond_trick and example_isin in all_data:\n",
        "            print(f\"\\nâ±ï¸  Nanosecond Trick Demonstration:\")\n",
        "            qte_dfs = all_data[example_isin].get('qte', {})\n",
        "            \n",
        "            # Find a venue with nanosecond offsets\n",
        "            for venue, qte_df in qte_dfs.items():\n",
        "                if not qte_df.empty:\n",
        "                    bursts = qte_df[qte_df.index.nanosecond > 0]\n",
        "                    if not bursts.empty:\n",
        "                        # Get first burst example\n",
        "                        burst_ts = bursts.index[0]\n",
        "                        base_ts = burst_ts.replace(nanosecond=0)\n",
        "                        \n",
        "                        # Get all snapshots at this base timestamp\n",
        "                        burst_snapshots = qte_df.loc[base_ts : base_ts + pd.Timedelta(nanoseconds=999)]\n",
        "                        \n",
        "                        print(f\"\\n   Example from {venue} venue:\")\n",
        "                        print(f\"   Base timestamp: {base_ts}\")\n",
        "                        print(f\"   Number of snapshots at this microsecond: {len(burst_snapshots)}\")\n",
        "                        print(f\"   These snapshots were resolved using nanosecond offsets:\")\n",
        "                        \n",
        "                        # Show relevant columns\n",
        "                        display_cols = ['epoch', 'px_bid_0', 'px_ask_0']\n",
        "                        if 'sequence' in burst_snapshots.columns:\n",
        "                            display_cols.insert(1, 'sequence')\n",
        "                        \n",
        "                        print(burst_snapshots[display_cols].to_string())\n",
        "                        break\n",
        "        \n",
        "        print(f\"\\nðŸ’¡ Market Microstructure Context:\")\n",
        "        print(f\"   â€¢ Each row represents a point in time where at least one venue\")\n",
        "        print(f\"     updated its orderbook\")\n",
        "        print(f\"   â€¢ Forward fill (ffill) ensures that all venues' last known prices\")\n",
        "        print(f\"     are available at every timestamp\")\n",
        "        print(f\"   â€¢ This enables cross-venue price comparison for arbitrage detection\")\n",
        "        print(f\"   â€¢ The nanosecond trick preserves chronological order when multiple\")\n",
        "        print(f\"     snapshots occur at the same microsecond\")\n",
        "        \n",
        "        # Show example of forward fill in action\n",
        "        print(f\"\\nðŸ”„ Forward Fill Example:\")\n",
        "        # Find a row where one venue has a value and another doesn't initially\n",
        "        bid_cols = [col for col in example_tape.columns if '_bid' in col]\n",
        "        if len(bid_cols) >= 2:\n",
        "            # Find first non-null value for first venue\n",
        "            first_venue_col = bid_cols[0]\n",
        "            first_non_null_idx = example_tape[first_venue_col].first_valid_index()\n",
        "            \n",
        "            if first_non_null_idx is not None:\n",
        "                # Show a few rows around this point\n",
        "                start_idx = example_tape.index.get_loc(first_non_null_idx)\n",
        "                end_idx = min(start_idx + 5, len(example_tape))\n",
        "                sample_range = example_tape.iloc[start_idx:end_idx]\n",
        "                \n",
        "                print(f\"   Showing how forward fill propagates prices:\")\n",
        "                print(f\"   (First non-null value for {first_venue_col} at {first_non_null_idx})\")\n",
        "                print(sample_range[bid_cols].to_string())\n",
        "    else:\n",
        "        print(\"\\nâš ï¸  No consolidated tapes available for verification.\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  No consolidated tapes created. Please check Step 1 execution.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **EXTRA** Visualization: Exploring Consolidated Tape Data\n",
        "\n",
        "Before proceeding to Step 3 (Signal Generation), let's visualize the consolidated tape data to understand price dynamics, spreads, and potential arbitrage opportunities across venues. These visualizations will help us understand the market microstructure and identify patterns in the data.\n",
        "\n",
        "### **(Estimado Francisco, en estos graficos a proposito he dejado la duplicidad de oportunidades de arbitraje puesto que todavia no hemos llegado al Step 3 en el cual nos instruias eliminarlos, los muestro a propÃ³sito porque creo que son muy ilustrativos a la hora de remarcar la importancia de lidiar con estas duplicidades ya que falsamente aumenta las oportunidades de arbitraje a cifras disparatadas y no reflejan lo que sucederia en la vida real, en el Step 3 lo corrijo, lo mismo sucede en el caso de los masivos outliers al comienzo y final de la sesiÃ³n, a estos Ãºltimos les he dedicado un segmento muy largo antes de empezar el Step 3, ESPERO QUE TE GUSTE!)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION 1: Price Evolution Across Venues (Multi-Venue Comparison)\n",
        "# ============================================================================\n",
        "# This visualization shows how prices evolve across different venues for\n",
        "# selected ISINs, similar to the XMAD vs CEUX comparison in the reference notebook.\n",
        "# ============================================================================\n",
        "\n",
        "# Select representative ISINs with multiple venues (3-4 venues preferred)\n",
        "# Priority: ISINs with good data coverage and multiple venues\n",
        "selected_isins = []\n",
        "\n",
        "# Find ISINs with 3-4 venues\n",
        "for isin, tape in consolidated_tapes.items():\n",
        "    bid_cols = [col for col in tape.columns if '_bid' in col]\n",
        "    num_venues = len(bid_cols)\n",
        "    if num_venues >= 3:\n",
        "        selected_isins.append((isin, num_venues))\n",
        "        if len(selected_isins) >= 3:  # Select top 3 ISINs\n",
        "            break\n",
        "\n",
        "# Sort by number of venues (descending)\n",
        "selected_isins.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"VISUALIZATION 1: Price Evolution Across Venues\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nSelected ISINs for visualization: {[isin for isin, _ in selected_isins]}\")\n",
        "\n",
        "# Create visualizations for selected ISINs\n",
        "for isin, num_venues in selected_isins[:2]:  # Show first 2 ISINs\n",
        "    if isin not in consolidated_tapes:\n",
        "        continue\n",
        "    \n",
        "    tape = consolidated_tapes[isin]\n",
        "    \n",
        "    # Get venue names\n",
        "    bid_cols = [col for col in tape.columns if '_bid' in col]\n",
        "    ask_cols = [col for col in tape.columns if '_ask' in col]\n",
        "    venues = [col.replace('_bid', '') for col in bid_cols]\n",
        "    \n",
        "    # Use full trading day data (from market open to market close)\n",
        "    if len(tape) > 0:\n",
        "        # Use complete data range - from first to last timestamp\n",
        "        time_window = tape\n",
        "        \n",
        "        if len(time_window) > 0:\n",
        "            # Create figure\n",
        "            fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
        "            \n",
        "            # Plot 1: Bid Prices\n",
        "            ax1 = axes[0]\n",
        "            for venue in venues:\n",
        "                bid_col = f'{venue}_bid'\n",
        "                if bid_col in time_window.columns:\n",
        "                    ax1.plot(time_window.index, time_window[bid_col], \n",
        "                            label=f'{venue} Bid', linewidth=1.5, alpha=0.8)\n",
        "            \n",
        "            ax1.set_title(f'Bid Prices Across Venues - ISIN: {isin}', fontsize=14, fontweight='bold')\n",
        "            ax1.set_ylabel('Bid Price (â‚¬)', fontsize=12)\n",
        "            ax1.set_xlabel('Timestamp', fontsize=12)\n",
        "            ax1.legend(loc='best', fontsize=10)\n",
        "            ax1.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "            \n",
        "            # Plot 2: Ask Prices\n",
        "            ax2 = axes[1]\n",
        "            for venue in venues:\n",
        "                ask_col = f'{venue}_ask'\n",
        "                if ask_col in time_window.columns:\n",
        "                    ax2.plot(time_window.index, time_window[ask_col], \n",
        "                            label=f'{venue} Ask', linewidth=1.5, alpha=0.8, linestyle='--')\n",
        "            \n",
        "            ax2.set_title(f'Ask Prices Across Venues - ISIN: {isin}', fontsize=14, fontweight='bold')\n",
        "            ax2.set_ylabel('Ask Price (â‚¬)', fontsize=12)\n",
        "            ax2.set_xlabel('Timestamp', fontsize=12)\n",
        "            ax2.legend(loc='best', fontsize=10)\n",
        "            ax2.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            print(f\"\\nâœ“ Created price evolution plots for {isin} ({num_venues} venues)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION 2: Spread Analysis (Price Differences Between Venues)\n",
        "# ============================================================================\n",
        "# This visualization shows price discrepancies between venues, similar to\n",
        "# the spread plot in the reference notebook (XMAD - CEUX).\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"VISUALIZATION 2: Spread Analysis Between Venues\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Use the same selected ISINs from Visualization 1\n",
        "for isin, num_venues in selected_isins[:2]:  # Show first 2 ISINs\n",
        "    if isin not in consolidated_tapes:\n",
        "        continue\n",
        "    \n",
        "    tape = consolidated_tapes[isin]\n",
        "    \n",
        "    # Get venue names\n",
        "    bid_cols = [col for col in tape.columns if '_bid' in col]\n",
        "    venues = [col.replace('_bid', '') for col in bid_cols]\n",
        "    \n",
        "    if len(venues) < 2:\n",
        "        continue\n",
        "    \n",
        "    # Use full trading day data (from market open to market close)\n",
        "    time_window = tape\n",
        "    \n",
        "    if len(time_window) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Calculate spreads between venue pairs\n",
        "    # Use BME as reference if available, otherwise use first venue\n",
        "    reference_venue = 'BME' if 'BME' in venues else venues[0]\n",
        "    \n",
        "    # Create figure with subplots\n",
        "    num_pairs = min(3, len(venues) - 1)  # Show up to 3 spread pairs\n",
        "    fig, axes = plt.subplots(num_pairs, 1, figsize=(16, 4 * num_pairs))\n",
        "    \n",
        "    if num_pairs == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    plot_idx = 0\n",
        "    for other_venue in venues:\n",
        "        if other_venue == reference_venue or plot_idx >= num_pairs:\n",
        "            continue\n",
        "        \n",
        "        # Calculate spread: Reference Bid - Other Ask (potential arbitrage direction)\n",
        "        ref_bid_col = f'{reference_venue}_bid'\n",
        "        other_ask_col = f'{other_venue}_ask'\n",
        "        \n",
        "        if ref_bid_col in time_window.columns and other_ask_col in time_window.columns:\n",
        "            spread = time_window[ref_bid_col] - time_window[other_ask_col]\n",
        "            \n",
        "            ax = axes[plot_idx]\n",
        "            ax.plot(time_window.index, spread, linewidth=1.5, alpha=0.8, color='steelblue')\n",
        "            ax.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Zero Spread')\n",
        "            \n",
        "            # Highlight positive spreads (potential arbitrage)\n",
        "            positive_spread = spread[spread > 0]\n",
        "            if len(positive_spread) > 0:\n",
        "                ax.scatter(positive_spread.index, positive_spread.values, \n",
        "                          color='green', s=10, alpha=0.6, label='Positive Spread (Arbitrage Opportunity)')\n",
        "            \n",
        "            ax.set_title(f'Spread: {reference_venue} Bid - {other_venue} Ask | ISIN: {isin}', \n",
        "                        fontsize=12, fontweight='bold')\n",
        "            ax.set_ylabel('Spread (â‚¬)', fontsize=11)\n",
        "            ax.set_xlabel('Timestamp', fontsize=11)\n",
        "            ax.legend(loc='best', fontsize=9)\n",
        "            ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "            \n",
        "            plot_idx += 1\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Also create a combined spread visualization\n",
        "    if len(venues) >= 2:\n",
        "        fig, ax = plt.subplots(figsize=(16, 6))\n",
        "        \n",
        "        # Calculate all pairwise spreads and plot the maximum spread\n",
        "        max_spreads = []\n",
        "        min_spreads = []\n",
        "        \n",
        "        for i, venue1 in enumerate(venues):\n",
        "            for venue2 in venues[i+1:]:\n",
        "                bid_col1 = f'{venue1}_bid'\n",
        "                ask_col2 = f'{venue2}_ask'\n",
        "                bid_col2 = f'{venue2}_bid'\n",
        "                ask_col1 = f'{venue1}_ask'\n",
        "                \n",
        "                if all(col in time_window.columns for col in [bid_col1, ask_col2, bid_col2, ask_col1]):\n",
        "                    # Spread in both directions\n",
        "                    spread1 = time_window[bid_col1] - time_window[ask_col2]\n",
        "                    spread2 = time_window[bid_col2] - time_window[ask_col1]\n",
        "                    \n",
        "                    # Take the maximum spread at each timestamp\n",
        "                    combined_spread = pd.concat([spread1, spread2], axis=1).max(axis=1)\n",
        "                    max_spreads.append(combined_spread)\n",
        "        \n",
        "        if max_spreads:\n",
        "            overall_max_spread = pd.concat(max_spreads, axis=1).max(axis=1)\n",
        "            ax.plot(time_window.index, overall_max_spread, linewidth=1.5, alpha=0.8, \n",
        "                   color='purple', label='Maximum Cross-Venue Spread')\n",
        "            ax.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
        "            \n",
        "            # Highlight positive regions\n",
        "            positive_regions = overall_max_spread[overall_max_spread > 0]\n",
        "            if len(positive_regions) > 0:\n",
        "                ax.fill_between(positive_regions.index, 0, positive_regions.values, \n",
        "                              alpha=0.3, color='green', label='Arbitrage Opportunity Regions')\n",
        "            \n",
        "            ax.set_title(f'Maximum Cross-Venue Spread (Arbitrage Indicator) | ISIN: {isin}', \n",
        "                        fontsize=14, fontweight='bold')\n",
        "            ax.set_ylabel('Spread (â‚¬)', fontsize=12)\n",
        "            ax.set_xlabel('Timestamp', fontsize=12)\n",
        "            ax.legend(loc='best', fontsize=10)\n",
        "            ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    \n",
        "    print(f\"\\nâœ“ Created spread analysis plots for {isin}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION 3: Bid-Ask Spread Per Venue\n",
        "# ============================================================================\n",
        "# This visualization shows the bid-ask spread (liquidity cost) for each venue.\n",
        "# Tighter spreads indicate better liquidity.\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"VISUALIZATION 3: Bid-Ask Spread Per Venue\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for isin, num_venues in selected_isins[:2]:  # Show first 2 ISINs\n",
        "    if isin not in consolidated_tapes:\n",
        "        continue\n",
        "    \n",
        "    tape = consolidated_tapes[isin]\n",
        "    \n",
        "    # Get venue names\n",
        "    bid_cols = [col for col in tape.columns if '_bid' in col]\n",
        "    venues = [col.replace('_bid', '') for col in bid_cols]\n",
        "    \n",
        "    # Use full trading day data (from market open to market close)\n",
        "    time_window = tape\n",
        "    \n",
        "    if len(time_window) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Calculate bid-ask spread for each venue\n",
        "    fig, axes = plt.subplots(len(venues), 1, figsize=(16, 4 * len(venues)))\n",
        "    \n",
        "    if len(venues) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, venue in enumerate(venues):\n",
        "        bid_col = f'{venue}_bid'\n",
        "        ask_col = f'{venue}_ask'\n",
        "        \n",
        "        if bid_col in time_window.columns and ask_col in time_window.columns:\n",
        "            # Calculate spread: Ask - Bid\n",
        "            spread = time_window[ask_col] - time_window[bid_col]\n",
        "            \n",
        "            # Remove NaN values\n",
        "            spread_clean = spread.dropna()\n",
        "            \n",
        "            if len(spread_clean) > 0:\n",
        "                ax = axes[idx]\n",
        "                ax.plot(spread_clean.index, spread_clean.values, \n",
        "                       linewidth=1.5, alpha=0.8, color='darkblue', label='Bid-Ask Spread')\n",
        "                \n",
        "                # Add mean line\n",
        "                mean_spread = spread_clean.mean()\n",
        "                ax.axhline(y=mean_spread, color='red', linestyle='--', \n",
        "                          linewidth=1.5, alpha=0.7, label=f'Mean Spread: {mean_spread:.4f} â‚¬')\n",
        "                \n",
        "                ax.set_title(f'{venue} Bid-Ask Spread | ISIN: {isin} | Mean: {mean_spread:.4f} â‚¬', \n",
        "                            fontsize=12, fontweight='bold')\n",
        "                ax.set_ylabel('Spread (â‚¬)', fontsize=11)\n",
        "                ax.set_xlabel('Timestamp', fontsize=11)\n",
        "                ax.legend(loc='best', fontsize=9)\n",
        "                ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Create a comparison plot showing all venues together\n",
        "    fig, ax = plt.subplots(figsize=(16, 6))\n",
        "    \n",
        "    for venue in venues:\n",
        "        bid_col = f'{venue}_bid'\n",
        "        ask_col = f'{venue}_ask'\n",
        "        \n",
        "        if bid_col in time_window.columns and ask_col in time_window.columns:\n",
        "            spread = time_window[ask_col] - time_window[bid_col]\n",
        "            spread_clean = spread.dropna()\n",
        "            \n",
        "            if len(spread_clean) > 0:\n",
        "                ax.plot(spread_clean.index, spread_clean.values, \n",
        "                       linewidth=1.5, alpha=0.7, label=f'{venue} (Mean: {spread_clean.mean():.4f} â‚¬)')\n",
        "    \n",
        "    ax.set_title(f'Bid-Ask Spread Comparison Across Venues | ISIN: {isin}', \n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Bid-Ask Spread (â‚¬)', fontsize=12)\n",
        "    ax.set_xlabel('Timestamp', fontsize=12)\n",
        "    ax.legend(loc='best', fontsize=10)\n",
        "    ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nâœ“ Created bid-ask spread plots for {isin}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION 4: Arbitrage Opportunity Visualization\n",
        "# ============================================================================\n",
        "# This visualization identifies where arbitrage opportunities exist by showing\n",
        "# max_bid (across all venues) vs min_ask (across all venues).\n",
        "# When max_bid > min_ask, an arbitrage opportunity exists.\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"VISUALIZATION 4: Arbitrage Opportunity Detection\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Find ISINs that actually have arbitrage opportunities\n",
        "arbitrage_isins = []\n",
        "\n",
        "for isin, tape in consolidated_tapes.items():\n",
        "    bid_cols = [col for col in tape.columns if '_bid' in col]\n",
        "    ask_cols = [col for col in tape.columns if '_ask' in col]\n",
        "    \n",
        "    if len(bid_cols) < 2:  # Need at least 2 venues for arbitrage\n",
        "        continue\n",
        "    \n",
        "    # Calculate max bid and min ask across all venues\n",
        "    max_bid = tape[bid_cols].max(axis=1)\n",
        "    min_ask = tape[ask_cols].min(axis=1)\n",
        "    \n",
        "    # Check if there are any arbitrage opportunities\n",
        "    theoretical_profit = max_bid - min_ask\n",
        "    arbitrage_opportunity = theoretical_profit > 0\n",
        "    \n",
        "    if arbitrage_opportunity.sum() > 0:  # Has at least some arbitrage opportunities\n",
        "        num_opportunities = arbitrage_opportunity.sum()\n",
        "        max_profit = theoretical_profit.max()\n",
        "        arbitrage_isins.append((isin, len(bid_cols), num_opportunities, max_profit))\n",
        "\n",
        "# Sort by number of opportunities (descending), then by max profit\n",
        "arbitrage_isins.sort(key=lambda x: (x[2], x[3]), reverse=True)\n",
        "\n",
        "print(f\"\\nFound {len(arbitrage_isins)} ISINs with arbitrage opportunities\")\n",
        "if len(arbitrage_isins) > 0:\n",
        "    print(f\"Top ISINs with arbitrage opportunities:\")\n",
        "    for isin, num_venues, num_opps, max_prof in arbitrage_isins[:5]:\n",
        "        print(f\"  {isin}: {num_opps:,} opportunities, max profit: {max_prof:.4f} â‚¬\")\n",
        "\n",
        "# Use ISINs with arbitrage opportunities, or fall back to selected_isins if none found\n",
        "if len(arbitrage_isins) > 0:\n",
        "    display_isins = arbitrage_isins[:2]  # Show top 2 ISINs with arbitrage\n",
        "else:\n",
        "    print(\"\\nâš ï¸  No ISINs with arbitrage opportunities found. Using selected ISINs instead.\")\n",
        "    display_isins = [(isin, num_venues, 0, 0) for isin, num_venues in selected_isins[:2]]\n",
        "\n",
        "for isin_info in display_isins:\n",
        "    isin = isin_info[0]\n",
        "    num_venues = isin_info[1]\n",
        "    \n",
        "    if isin not in consolidated_tapes:\n",
        "        continue\n",
        "    \n",
        "    tape = consolidated_tapes[isin]\n",
        "    \n",
        "    # Get all bid and ask columns\n",
        "    bid_cols = [col for col in tape.columns if '_bid' in col]\n",
        "    ask_cols = [col for col in tape.columns if '_ask' in col]\n",
        "    \n",
        "    if len(bid_cols) < 2:  # Need at least 2 venues for arbitrage\n",
        "        continue\n",
        "    \n",
        "    # Use full trading day data (from market open to market close)\n",
        "    time_window = tape\n",
        "    \n",
        "    if len(time_window) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Calculate max bid and min ask across all venues at each timestamp\n",
        "    max_bid = time_window[bid_cols].max(axis=1)\n",
        "    min_ask = time_window[ask_cols].min(axis=1)\n",
        "    \n",
        "    # Calculate theoretical profit (arbitrage opportunity)\n",
        "    # Profit exists when max_bid > min_ask\n",
        "    theoretical_profit = max_bid - min_ask\n",
        "    arbitrage_opportunity = theoretical_profit > 0\n",
        "    \n",
        "    # Create main visualization\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "    \n",
        "    # Plot 1: Max Bid vs Min Ask\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(time_window.index, max_bid, label='Max Bid (across all venues)', \n",
        "            linewidth=2, alpha=0.9, color='green')\n",
        "    ax1.plot(time_window.index, min_ask, label='Min Ask (across all venues)', \n",
        "            linewidth=2, alpha=0.9, color='red', linestyle='--')\n",
        "    \n",
        "    # Highlight arbitrage opportunities (where max_bid > min_ask)\n",
        "    arb_mask = arbitrage_opportunity\n",
        "    if arb_mask.sum() > 0:\n",
        "        ax1.fill_between(time_window.index, max_bid, min_ask, \n",
        "                         where=arb_mask, alpha=0.3, color='yellow', \n",
        "                         label='Arbitrage Opportunity Region')\n",
        "    \n",
        "    ax1.set_title(f'Arbitrage Opportunity Detection: Max Bid vs Min Ask | ISIN: {isin}', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Price (â‚¬)', fontsize=12)\n",
        "    ax1.legend(loc='best', fontsize=10)\n",
        "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "    \n",
        "    # Plot 2: Theoretical Profit\n",
        "    ax2 = axes[1]\n",
        "    ax2.plot(time_window.index, theoretical_profit, \n",
        "            linewidth=1.5, alpha=0.8, color='purple', label='Theoretical Profit')\n",
        "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
        "    \n",
        "    # Highlight positive profit regions\n",
        "    positive_profit = theoretical_profit[theoretical_profit > 0]\n",
        "    if len(positive_profit) > 0:\n",
        "        ax2.fill_between(positive_profit.index, 0, positive_profit.values, \n",
        "                        alpha=0.4, color='green', label='Profitable Opportunities')\n",
        "    \n",
        "    ax2.set_title(f'Theoretical Profit per Unit (Max Bid - Min Ask) | ISIN: {isin}', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylabel('Profit (â‚¬)', fontsize=12)\n",
        "    ax2.legend(loc='best', fontsize=10)\n",
        "    ax2.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "    \n",
        "    # Plot 3: Arbitrage Opportunity Indicator (Binary)\n",
        "    ax3 = axes[2]\n",
        "    ax3.fill_between(time_window.index, 0, arbitrage_opportunity.astype(int), \n",
        "                    alpha=0.6, color='green', label='Arbitrage Opportunity Present')\n",
        "    ax3.set_title(f'Arbitrage Opportunity Indicator (1 = Opportunity Exists) | ISIN: {isin}', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax3.set_ylabel('Opportunity (0/1)', fontsize=12)\n",
        "    ax3.set_xlabel('Timestamp', fontsize=12)\n",
        "    ax3.set_ylim(-0.1, 1.1)\n",
        "    ax3.legend(loc='best', fontsize=10)\n",
        "    ax3.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print statistics\n",
        "    num_opportunities = arbitrage_opportunity.sum()\n",
        "    total_timestamps = len(arbitrage_opportunity)\n",
        "    opportunity_pct = (num_opportunities / total_timestamps) * 100 if total_timestamps > 0 else 0\n",
        "    \n",
        "    if num_opportunities > 0:\n",
        "        max_profit = theoretical_profit.max()\n",
        "        mean_profit = positive_profit.mean() if len(positive_profit) > 0 else 0\n",
        "        \n",
        "        print(f\"\\nðŸ“Š Arbitrage Statistics for {isin}:\")\n",
        "        print(f\"   â€¢ Total timestamps analyzed: {total_timestamps:,}\")\n",
        "        print(f\"   â€¢ Arbitrage opportunities found: {num_opportunities:,} ({opportunity_pct:.2f}%)\")\n",
        "        print(f\"   â€¢ Maximum theoretical profit: {max_profit:.4f} â‚¬\")\n",
        "        print(f\"   â€¢ Mean profit (when opportunity exists): {mean_profit:.4f} â‚¬\")\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸  No arbitrage opportunities found for {isin} in the selected time window\")\n",
        "    \n",
        "    print(f\"\\nâœ“ Created arbitrage opportunity plots for {isin}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION 5: Summary Statistics Visualization\n",
        "# ============================================================================\n",
        "# This visualization provides an overview of key statistics about the\n",
        "# consolidated tape data using bar charts and summary metrics.\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"VISUALIZATION 5: Summary Statistics Visualization\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Collect statistics across all ISINs\n",
        "venue_stats = {}\n",
        "isin_stats = []\n",
        "\n",
        "for isin, tape in consolidated_tapes.items():\n",
        "    bid_cols = [col for col in tape.columns if '_bid' in col]\n",
        "    ask_cols = [col for col in tape.columns if '_ask' in col]\n",
        "    venues = [col.replace('_bid', '') for col in bid_cols]\n",
        "    \n",
        "    # Count updates per venue\n",
        "    for venue in venues:\n",
        "        bid_col = f'{venue}_bid'\n",
        "        ask_col = f'{venue}_ask'\n",
        "        \n",
        "        if venue not in venue_stats:\n",
        "            venue_stats[venue] = {'updates': 0, 'isins': set()}\n",
        "        \n",
        "        if bid_col in tape.columns:\n",
        "            venue_stats[venue]['updates'] += tape[bid_col].notna().sum()\n",
        "        if ask_col in tape.columns:\n",
        "            venue_stats[venue]['updates'] += tape[ask_col].notna().sum()\n",
        "        \n",
        "        venue_stats[venue]['isins'].add(isin)\n",
        "    \n",
        "    # Calculate statistics for this ISIN\n",
        "    if len(bid_cols) > 0 and len(ask_cols) > 0:\n",
        "        # Calculate price ranges\n",
        "        all_bids = []\n",
        "        all_asks = []\n",
        "        for bid_col in bid_cols:\n",
        "            all_bids.extend(tape[bid_col].dropna().tolist())\n",
        "        for ask_col in ask_cols:\n",
        "            all_asks.extend(tape[ask_col].dropna().tolist())\n",
        "        \n",
        "        if len(all_bids) > 0 and len(all_asks) > 0:\n",
        "            min_bid = min(all_bids)\n",
        "            max_bid = max(all_bids)\n",
        "            min_ask = min(all_asks)\n",
        "            max_ask = max(all_asks)\n",
        "            \n",
        "            # Calculate average bid-ask spread\n",
        "            spreads = []\n",
        "            for venue in venues:\n",
        "                bid_col = f'{venue}_bid'\n",
        "                ask_col = f'{venue}_ask'\n",
        "                if bid_col in tape.columns and ask_col in tape.columns:\n",
        "                    spread = tape[ask_col] - tape[bid_col]\n",
        "                    spreads.extend(spread.dropna().tolist())\n",
        "            \n",
        "            avg_spread = np.mean(spreads) if len(spreads) > 0 else 0\n",
        "            \n",
        "            isin_stats.append({\n",
        "                'ISIN': isin,\n",
        "                'Num_Venues': len(venues),\n",
        "                'Total_Updates': len(tape),\n",
        "                'Min_Bid': min_bid,\n",
        "                'Max_Bid': max_bid,\n",
        "                'Min_Ask': min_ask,\n",
        "                'Max_Ask': max_ask,\n",
        "                'Avg_Spread': avg_spread\n",
        "            })\n",
        "\n",
        "# Create visualizations\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Plot 1: Updates per Venue (Bar Chart)\n",
        "ax1 = plt.subplot(2, 3, 1)\n",
        "venues_list = list(venue_stats.keys())\n",
        "updates_list = [venue_stats[v]['updates'] for v in venues_list]\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(venues_list)))\n",
        "\n",
        "bars = ax1.bar(venues_list, updates_list, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "ax1.set_title('Total Updates per Venue (All ISINs)', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Number of Updates', fontsize=11)\n",
        "ax1.set_xlabel('Venue', fontsize=11)\n",
        "ax1.grid(True, axis='y', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(height):,}',\n",
        "            ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Plot 2: Number of ISINs per Venue\n",
        "ax2 = plt.subplot(2, 3, 2)\n",
        "isins_per_venue = [len(venue_stats[v]['isins']) for v in venues_list]\n",
        "bars2 = ax2.bar(venues_list, isins_per_venue, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "ax2.set_title('Number of ISINs per Venue', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Number of ISINs', fontsize=11)\n",
        "ax2.set_xlabel('Venue', fontsize=11)\n",
        "ax2.grid(True, axis='y', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(height)}',\n",
        "            ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Plot 3: Distribution of Number of Venues per ISIN\n",
        "ax3 = plt.subplot(2, 3, 3)\n",
        "if isin_stats:\n",
        "    df_isin_stats = pd.DataFrame(isin_stats)\n",
        "    venue_counts = df_isin_stats['Num_Venues'].value_counts().sort_index()\n",
        "    bars3 = ax3.bar(venue_counts.index.astype(str), venue_counts.values, \n",
        "                   color='steelblue', alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "    ax3.set_title('Distribution: Number of Venues per ISIN', fontsize=12, fontweight='bold')\n",
        "    ax3.set_ylabel('Number of ISINs', fontsize=11)\n",
        "    ax3.set_xlabel('Number of Venues', fontsize=11)\n",
        "    ax3.grid(True, axis='y', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "    \n",
        "    for bar in bars3:\n",
        "        height = bar.get_height()\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{int(height)}',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Plot 4: Average Bid-Ask Spread by ISIN (Top 10)\n",
        "ax4 = plt.subplot(2, 3, 4)\n",
        "if isin_stats:\n",
        "    df_isin_stats = pd.DataFrame(isin_stats)\n",
        "    top_spreads = df_isin_stats.nlargest(10, 'Avg_Spread')\n",
        "    bars4 = ax4.barh(range(len(top_spreads)), top_spreads['Avg_Spread'], \n",
        "                    color='coral', alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "    ax4.set_yticks(range(len(top_spreads)))\n",
        "    ax4.set_yticklabels([isin[:15] + '...' if len(isin) > 15 else isin \n",
        "                        for isin in top_spreads['ISIN']], fontsize=8)\n",
        "    ax4.set_title('Top 10 ISINs by Average Bid-Ask Spread', fontsize=12, fontweight='bold')\n",
        "    ax4.set_xlabel('Average Spread (â‚¬)', fontsize=11)\n",
        "    ax4.grid(True, axis='x', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# Plot 5: Total Updates Distribution\n",
        "ax5 = plt.subplot(2, 3, 5)\n",
        "if isin_stats:\n",
        "    df_isin_stats = pd.DataFrame(isin_stats)\n",
        "    ax5.hist(df_isin_stats['Total_Updates'], bins=30, color='mediumseagreen', \n",
        "            alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "    ax5.set_title('Distribution of Total Updates per ISIN', fontsize=12, fontweight='bold')\n",
        "    ax5.set_ylabel('Number of ISINs', fontsize=11)\n",
        "    ax5.set_xlabel('Total Updates', fontsize=11)\n",
        "    ax5.grid(True, axis='y', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# Plot 6: Price Range Statistics\n",
        "ax6 = plt.subplot(2, 3, 6)\n",
        "if isin_stats:\n",
        "    df_isin_stats = pd.DataFrame(isin_stats)\n",
        "    # Calculate price range (max - min) for bids and asks\n",
        "    df_isin_stats['Bid_Range'] = df_isin_stats['Max_Bid'] - df_isin_stats['Min_Bid']\n",
        "    df_isin_stats['Ask_Range'] = df_isin_stats['Max_Ask'] - df_isin_stats['Min_Ask']\n",
        "    \n",
        "    x_pos = np.arange(len(df_isin_stats))\n",
        "    width = 0.35\n",
        "    \n",
        "    # Show top 10 ISINs by bid range\n",
        "    top_ranges = df_isin_stats.nlargest(10, 'Bid_Range')\n",
        "    x_pos = np.arange(len(top_ranges))\n",
        "    \n",
        "    bars6a = ax6.barh(x_pos - width/2, top_ranges['Bid_Range'], width, \n",
        "                     label='Bid Range', color='lightblue', alpha=0.8, edgecolor='black')\n",
        "    bars6b = ax6.barh(x_pos + width/2, top_ranges['Ask_Range'], width, \n",
        "                     label='Ask Range', color='lightcoral', alpha=0.8, edgecolor='black')\n",
        "    \n",
        "    ax6.set_yticks(x_pos)\n",
        "    ax6.set_yticklabels([isin[:12] + '...' if len(isin) > 12 else isin \n",
        "                         for isin in top_ranges['ISIN']], fontsize=8)\n",
        "    ax6.set_title('Top 10 ISINs by Price Range', fontsize=12, fontweight='bold')\n",
        "    ax6.set_xlabel('Price Range (â‚¬)', fontsize=11)\n",
        "    ax6.legend(loc='best', fontsize=9)\n",
        "    ax6.grid(True, axis='x', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nðŸ“Š Overall Summary Statistics:\")\n",
        "print(f\"   â€¢ Total ISINs analyzed: {len(consolidated_tapes)}\")\n",
        "print(f\"   â€¢ Total venues: {len(venue_stats)}\")\n",
        "print(f\"   â€¢ Venues: {', '.join(venues_list)}\")\n",
        "\n",
        "print(\"\\nðŸ“ˆ Updates per Venue:\")\n",
        "for venue in venues_list:\n",
        "    print(f\"   â€¢ {venue}: {venue_stats[venue]['updates']:,} updates across {len(venue_stats[venue]['isins'])} ISINs\")\n",
        "\n",
        "if isin_stats:\n",
        "    df_isin_stats = pd.DataFrame(isin_stats)\n",
        "    print(f\"\\nðŸ“Š ISIN Statistics:\")\n",
        "    print(f\"   â€¢ Average number of venues per ISIN: {df_isin_stats['Num_Venues'].mean():.2f}\")\n",
        "    print(f\"   â€¢ Average total updates per ISIN: {df_isin_stats['Total_Updates'].mean():.0f}\")\n",
        "    print(f\"   â€¢ Average bid-ask spread: {df_isin_stats['Avg_Spread'].mean():.4f} â‚¬\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"âœ“ Created summary statistics visualizations\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Signal Generation\n",
        "\n",
        "- **Arbitrage Condition:** An opportunity exists when Global Max Bid > Global Min Ask.\n",
        "- **Profit Calc:** (Max Bid - Min Ask) * Min(BidQty, AskQty).\n",
        "- **Rising Edge:** In a simulation, if an opportunity persists for 1 second (1000 snapshots), you can only trade it *once* (the first time it appears). Ensure you aren't \"double counting\" the same opportunity. If the opportunity vanishes and quickly reappears you can count it as a new opportunity for simplification.\n",
        "- **Simplification:** Only look at opportunities between Global Max Bid and Global Min Ask. There might be others at the second or third price levels of the orderbook, but let's make it simple and use only the best Bid Ask of each trading venue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Edge Cases in Signal Generation: Why They Appear and How We Handle Them\n",
        "\n",
        "As we observed in the **visualizations from Step 2** (particularly in the price evolution and spread analysis plots), the consolidated tape contains several edge cases that must be carefully handled to ensure accurate arbitrage signal generation:\n",
        "\n",
        "#### 1. **Missing Data (NaN Values) in Consolidated Tape**\n",
        "\n",
        "**Why They Appear:**\n",
        "- **Market Opening/Closing:** At the start of the trading day, venues open at different times. For example, BME might open at 08:00:00, while CBOE opens at 08:00:05. During this 5-microsecond gap, CBOE has no data, resulting in NaN values.\n",
        "- **Asynchronous Updates:** Even during continuous trading, venues update their orderbooks independently. If BME updates at t=100Âµs and CBOE hasn't updated since t=95Âµs, forward fill propagates CBOE's last known price. However, if a venue has never updated yet (e.g., at market open), we get NaN.\n",
        "- **Venue-Specific Trading:** Some ISINs may not trade on all venues. For example, a Spanish stock might trade on BME and CBOE but not on TURQUOISE, leading to permanent NaN values for that venue.\n",
        "\n",
        "**Why We Must Deal With Them:**\n",
        "- **`.idxmax()` and `.idxmin()` Failures:** When calculating which venue has the maximum bid or minimum ask, if all values are NaN at a timestamp, `idxmax()`/`idxmin()` return NaN. Attempting to extract the venue name from NaN using string operations would cause errors.\n",
        "- **False Arbitrage Signals:** If we don't filter NaN values properly, we might calculate `NaN > 10.50` which evaluates to False, but then try to extract quantities from a NaN venue name, causing crashes.\n",
        "- **Incorrect Profit Calculations:** Missing quantities would lead to NaN profits, which would skew our total profit calculations.\n",
        "\n",
        "**How We Handle It:**\n",
        "- We use `.fillna('')` before string operations to safely handle NaN venue names\n",
        "- We check for empty consolidated tapes and missing quantity columns before processing\n",
        "- We filter out opportunities where critical data (prices or quantities) is missing\n",
        "- We use `.fillna(False)` in rising edge detection to handle the first timestamp edge case\n",
        "\n",
        "#### 2. **Empty or Incomplete Consolidated Tapes**\n",
        "\n",
        "**Why They Appear:**\n",
        "- Some ISINs may have very limited trading activity\n",
        "- Data quality issues during cleaning (all prices filtered out as invalid)\n",
        "- ISINs that only trade on one venue (no arbitrage possible anyway)\n",
        "\n",
        "**Why We Must Deal With Them:**\n",
        "- Calling `.max(axis=1)` on an empty DataFrame would fail\n",
        "- Processing empty data wastes computational resources\n",
        "\n",
        "**How We Handle It:**\n",
        "- Early return with empty DataFrame if consolidated tape is empty\n",
        "- Check for presence of required columns (bid_cols, ask_cols) before operations\n",
        "\n",
        "#### 3. **Missing Quantities in Consolidated Tape**\n",
        "\n",
        "**Why They Appear:**\n",
        "- The original consolidated tape from Step 2 only includes prices, not quantities\n",
        "- Quantities are needed for profit calculation but weren't included initially\n",
        "\n",
        "**Why We Must Deal With Them:**\n",
        "- Without quantities, we cannot calculate `Min(BidQty, AskQty)` for profit calculation\n",
        "- We need to know how much volume is available at the arbitrage prices\n",
        "\n",
        "**How We Handle It:**\n",
        "- We check if quantities exist in the tape, and if not, we extend it using `extend_consolidated_tape_with_quantities()`\n",
        "- This function follows the same forward-fill pattern as prices to maintain consistency\n",
        "\n",
        "#### 4. **Rising Edge Detection Edge Cases**\n",
        "\n",
        "**Why They Appear:**\n",
        "- At the very first timestamp, there is no \"previous\" state to compare against\n",
        "- Opportunities that persist for extended periods (as seen in the visualizations)\n",
        "\n",
        "**Why We Must Deal With Them:**\n",
        "- `.shift(1)` on the first row returns NaN, which would break boolean operations\n",
        "- We need to correctly identify the first appearance of an opportunity\n",
        "\n",
        "**How We Handle It:**\n",
        "- We use `.fillna(False)` to treat the \"before first timestamp\" state as \"no opportunity\"\n",
        "- This ensures the first opportunity (if it exists) is correctly identified as a rising edge\n",
        "\n",
        "#### 5. **Venue Mismatches Between Prices and Quantities**\n",
        "\n",
        "**Why They Appear:**\n",
        "- Different venues may have different update frequencies\n",
        "- A venue might have a price update but the corresponding quantity column might be missing or NaN\n",
        "\n",
        "**Why We Must Deal With Them:**\n",
        "- We need to extract quantities from the specific venue that has the max bid or min ask\n",
        "- If the quantity column doesn't exist for that venue, we get NaN, which would break profit calculations\n",
        "\n",
        "**How We Handle It:**\n",
        "- We check for the existence of quantity columns before extraction\n",
        "- We initialize quantity Series with NaN and only fill them when data is available\n",
        "- The `Min(BidQty, AskQty)` operation naturally handles NaN by returning NaN, which we then filter out\n",
        "\n",
        "---\n",
        "\n",
        "**Key Insight from Step 2 Visualizations:**\n",
        "The price evolution plots showed that venues often have gaps in their data, especially at market open/close. The spread analysis revealed that arbitrage opportunities appear and disappear frequently. These visual patterns directly inform our edge case handling strategy, ensuring our signal generation is robust to real-world market data imperfections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 3: SIGNAL GENERATION\n",
        "# ============================================================================\n",
        "# This step generates arbitrage signals by:\n",
        "# 1. Extending consolidated tape with quantities (if not already present)\n",
        "# 2. Calculating Global Max Bid and Global Min Ask\n",
        "# 3. Identifying arbitrage opportunities (Max Bid > Min Ask)\n",
        "# 4. Calculating profit per opportunity\n",
        "# 5. Implementing rising edge detection to avoid double-counting\n",
        "# ============================================================================\n",
        "\n",
        "def extend_consolidated_tape_with_quantities(isin, consolidated_tape, cleaned_data_dict):\n",
        "    \"\"\"\n",
        "    Extends the consolidated tape to include bid and ask quantities.\n",
        "    \n",
        "    The consolidated tape from Step 2 only includes prices. This function\n",
        "    adds quantities following the same forward-fill pattern as prices.\n",
        "    \n",
        "    Market Microstructure Context:\n",
        "    - Quantities are needed to calculate tradable volume for arbitrage\n",
        "    - We forward-fill quantities just like prices to maintain consistency\n",
        "    - This ensures we know available liquidity at each timestamp\n",
        "    \n",
        "    Args:\n",
        "        isin: The ISIN identifier\n",
        "        consolidated_tape: DataFrame with prices (from Step 2)\n",
        "        cleaned_data_dict: Dictionary with original cleaned QTE data\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with prices and quantities (bid_qty, ask_qty for each venue)\n",
        "    \"\"\"\n",
        "    # ========================================================================\n",
        "    # EDGE CASE HANDLING 1: Empty Consolidated Tape\n",
        "    # ========================================================================\n",
        "    # Market Microstructure: Some ISINs may have no data after cleaning,\n",
        "    # or may only trade on one venue (no arbitrage possible). We must check\n",
        "    # for empty tapes before processing to avoid errors.\n",
        "    if consolidated_tape.empty:\n",
        "        return consolidated_tape\n",
        "    \n",
        "    # EDGE CASE HANDLING 2: Missing ISIN in Data Dictionary\n",
        "    # If the ISIN is not in the cleaned data dictionary, we cannot extend\n",
        "    # the tape with quantities. Return the original tape.\n",
        "    if isin not in cleaned_data_dict:\n",
        "        return consolidated_tape\n",
        "    \n",
        "    qte_dfs = cleaned_data_dict[isin].get('qte', {})\n",
        "    \n",
        "    if not qte_dfs:\n",
        "        return consolidated_tape\n",
        "    \n",
        "    # ========================================================================\n",
        "    # Extract quantities from each venue (same pattern as prices)\n",
        "    # ========================================================================\n",
        "    venue_data_list = []\n",
        "    \n",
        "    for venue, qte_df in qte_dfs.items():\n",
        "        if qte_df.empty:\n",
        "            continue\n",
        "        \n",
        "        # Extract quantities\n",
        "        venue_subset = qte_df[['qty_bid_0', 'qty_ask_0']].copy()\n",
        "        venue_subset['venue'] = venue\n",
        "        \n",
        "        # Rename columns\n",
        "        venue_subset = venue_subset.rename(columns={\n",
        "            'qty_bid_0': 'bid_qty',\n",
        "            'qty_ask_0': 'ask_qty'\n",
        "        })\n",
        "        \n",
        "        venue_data_list.append(venue_subset)\n",
        "    \n",
        "    if not venue_data_list:\n",
        "        return consolidated_tape\n",
        "    \n",
        "    # Combine all venues\n",
        "    tape_full = pd.concat(venue_data_list)\n",
        "    tape_full.sort_index(inplace=True)\n",
        "    \n",
        "    # Pivot quantities separately\n",
        "    tape_bid_qty_pivot = tape_full.pivot(columns='venue', values='bid_qty')\n",
        "    tape_ask_qty_pivot = tape_full.pivot(columns='venue', values='ask_qty')\n",
        "    \n",
        "    # Forward fill quantities (same as prices)\n",
        "    tape_bid_qty_ffill = tape_bid_qty_pivot.ffill()\n",
        "    tape_ask_qty_ffill = tape_ask_qty_pivot.ffill()\n",
        "    \n",
        "    # Rename columns to include suffix\n",
        "    tape_bid_qty_ffill.columns = [f'{venue}_bid_qty' for venue in tape_bid_qty_ffill.columns]\n",
        "    tape_ask_qty_ffill.columns = [f'{venue}_ask_qty' for venue in tape_ask_qty_ffill.columns]\n",
        "    \n",
        "    # Combine with existing consolidated tape\n",
        "    extended_tape = pd.concat([consolidated_tape, tape_bid_qty_ffill, tape_ask_qty_ffill], axis=1)\n",
        "    extended_tape = extended_tape.sort_index(axis=1)\n",
        "    \n",
        "    return extended_tape\n",
        "\n",
        "\n",
        "def generate_arbitrage_signals_for_isin(isin, consolidated_tape, cleaned_data_dict):\n",
        "    \"\"\"\n",
        "    Generates arbitrage signals for a single ISIN.\n",
        "    \n",
        "    This function identifies arbitrage opportunities where Global Max Bid > Global Min Ask,\n",
        "    calculates profits, and implements rising edge detection to avoid double-counting.\n",
        "    \n",
        "    Market Microstructure Context:\n",
        "    - Arbitrage exists when we can buy at the lowest ask and sell at the highest bid\n",
        "    - We can only trade the minimum of available bid and ask quantities\n",
        "    - Rising edge ensures we don't count the same opportunity multiple times\n",
        "    - Once an opportunity vanishes, we can trade it again when it reappears\n",
        "    \n",
        "    Args:\n",
        "        isin: The ISIN identifier\n",
        "        consolidated_tape: DataFrame with prices (and optionally quantities)\n",
        "        cleaned_data_dict: Dictionary with original cleaned data (for quantities if needed)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with arbitrage signals (only rising edges) containing:\n",
        "        - timestamp, prices, quantities, venues, profits\n",
        "    \"\"\"\n",
        "    # ========================================================================\n",
        "    # EDGE CASE HANDLING 1: Empty Consolidated Tape\n",
        "    # ========================================================================\n",
        "    # Market Microstructure: Some ISINs may have no valid data after cleaning\n",
        "    # (e.g., all prices filtered as invalid magic numbers). We must check\n",
        "    # before processing to avoid errors in vectorized operations.\n",
        "    if consolidated_tape.empty:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 1: Extend consolidated tape with quantities if not present\n",
        "    # ========================================================================\n",
        "    # EDGE CASE HANDLING 2: Missing Quantities in Consolidated Tape\n",
        "    # The consolidated tape from Step 2 only includes prices. We need quantities\n",
        "    # to calculate tradable volume. This check ensures we extend the tape if needed.\n",
        "    # Market Microstructure: Quantities are essential for profit calculation as\n",
        "    # we can only trade Min(BidQty, AskQty) - the maximum executable volume.\n",
        "    bid_qty_cols = [col for col in consolidated_tape.columns if '_bid_qty' in col]\n",
        "    ask_qty_cols = [col for col in consolidated_tape.columns if '_ask_qty' in col]\n",
        "    \n",
        "    if not bid_qty_cols or not ask_qty_cols:\n",
        "        # Extend tape with quantities\n",
        "        consolidated_tape = extend_consolidated_tape_with_quantities(\n",
        "            isin, consolidated_tape, cleaned_data_dict\n",
        "        )\n",
        "        # Re-extract column lists after extension\n",
        "        bid_qty_cols = [col for col in consolidated_tape.columns if '_bid_qty' in col]\n",
        "        ask_qty_cols = [col for col in consolidated_tape.columns if '_ask_qty' in col]\n",
        "    \n",
        "    # Get bid and ask price columns (exclude quantity columns)\n",
        "    bid_cols = [col for col in consolidated_tape.columns if '_bid' in col and '_qty' not in col]\n",
        "    ask_cols = [col for col in consolidated_tape.columns if '_ask' in col and '_qty' not in col]\n",
        "    \n",
        "    # EDGE CASE HANDLING 3: Missing Price Columns\n",
        "    # If no bid or ask price columns exist, we cannot calculate arbitrage.\n",
        "    # This can happen if data structure is corrupted or ISIN has no valid prices.\n",
        "    if not bid_cols or not ask_cols:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 2: Calculate Global Max Bid and Global Min Ask\n",
        "    # ========================================================================\n",
        "    # Market Microstructure: Global Max Bid is the highest price any venue\n",
        "    # is willing to pay. Global Min Ask is the lowest price any venue is\n",
        "    # willing to sell. When Max Bid > Min Ask, arbitrage is possible.\n",
        "    \n",
        "    global_max_bid = consolidated_tape[bid_cols].max(axis=1)\n",
        "    global_min_ask = consolidated_tape[ask_cols].min(axis=1)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 3: Identify Arbitrage Opportunities\n",
        "    # ========================================================================\n",
        "    # An opportunity exists when we can buy at Min Ask and sell at Max Bid\n",
        "    # simultaneously, making a risk-free profit.\n",
        "    \n",
        "    arbitrage_opportunity = global_max_bid > global_min_ask\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 4: Identify Venues with Max Bid and Min Ask\n",
        "    # ========================================================================\n",
        "    # We need to know which venues to trade on and what quantities are available\n",
        "    \n",
        "    # ========================================================================\n",
        "    # EDGE CASE HANDLING 4: NaN Values from idxmax/idxmin\n",
        "    # ========================================================================\n",
        "    # Market Microstructure: As seen in Step 2 visualizations, venues open\n",
        "    # at different times and update asynchronously. At market open, some venues\n",
        "    # may have no data yet, resulting in all-NaN rows. When all bids (or asks)\n",
        "    # are NaN, idxmax()/idxmin() returns NaN, which would cause errors in\n",
        "    # string operations.\n",
        "    #\n",
        "    # Why this happens:\n",
        "    # - Market opening: BME opens at 08:00:00, CBOE at 08:00:05 â†’ NaN for CBOE\n",
        "    # - Venue-specific trading: Some ISINs don't trade on all venues â†’ permanent NaN\n",
        "    # - Data gaps: Forward fill can't fill if venue never had data\n",
        "    #\n",
        "    # How we handle it:\n",
        "    # - Use fillna('') before string operations to safely handle NaN\n",
        "    # - Convert empty strings back to NaN for consistency\n",
        "    # - This ensures we can extract venue names even when some timestamps have NaN\n",
        "    \n",
        "    # Find which venue has the max bid at each timestamp\n",
        "    max_bid_venue_idx = consolidated_tape[bid_cols].idxmax(axis=1)\n",
        "    # Extract venue name from column name (e.g., 'BME_bid' -> 'BME')\n",
        "    # CRITICAL: Handle NaN values (when all bids are NaN at a timestamp)\n",
        "    max_bid_venue = max_bid_venue_idx.fillna('').str.replace('_bid', '', regex=False)\n",
        "    max_bid_venue = max_bid_venue.replace('', np.nan)\n",
        "    \n",
        "    # Find which venue has the min ask at each timestamp\n",
        "    min_ask_venue_idx = consolidated_tape[ask_cols].idxmin(axis=1)\n",
        "    # Extract venue name from column name (e.g., 'BME_ask' -> 'BME')\n",
        "    # CRITICAL: Handle NaN values (when all asks are NaN at a timestamp)\n",
        "    min_ask_venue = min_ask_venue_idx.fillna('').str.replace('_ask', '', regex=False)\n",
        "    min_ask_venue = min_ask_venue.replace('', np.nan)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 5: Extract Quantities for Max Bid and Min Ask Venues\n",
        "    # ========================================================================\n",
        "    # Market Microstructure: We can only trade the minimum of available\n",
        "    # bid and ask quantities. This represents the maximum executable volume.\n",
        "    #\n",
        "    # ========================================================================\n",
        "    # EDGE CASE HANDLING 5: Missing Quantity Columns for Specific Venues\n",
        "    # ========================================================================\n",
        "    # Market Microstructure: As seen in Step 2 visualizations, different venues\n",
        "    # update at different frequencies. A venue might have price data but missing\n",
        "    # quantity data, or the quantity column might not exist for that venue.\n",
        "    #\n",
        "    # Why this happens:\n",
        "    # - Different update frequencies: Price updates more frequently than quantity\n",
        "    # - Data quality issues: Quantity data might be filtered out during cleaning\n",
        "    # - Venue-specific data structure: Some venues might not report quantities\n",
        "    # - Market microstructure: At market open, quantities may not be available yet\n",
        "    #\n",
        "    # Why we must deal with it:\n",
        "    # - KeyError: If we try to access a non-existent column, pandas raises KeyError\n",
        "    # - Incorrect profits: Missing quantities would lead to NaN profits\n",
        "    # - False signals: We need quantities to calculate tradable volume\n",
        "    #\n",
        "    # How we handle it:\n",
        "    # - Initialize Series with NaN (safe default for missing data)\n",
        "    # - Check for column existence before extraction (if bid_qty_col in columns)\n",
        "    # - Skip NaN venues (pd.isna(venue) check)\n",
        "    # - NaN quantities will result in NaN profits, which are naturally filtered\n",
        "    #   out when we only keep opportunities with valid data\n",
        "    \n",
        "    # Initialize quantity Series with NaN (safe default for missing data)\n",
        "    bid_qty = pd.Series(index=consolidated_tape.index, dtype=float)\n",
        "    ask_qty = pd.Series(index=consolidated_tape.index, dtype=float)\n",
        "    \n",
        "    # For each venue, extract quantities when it's the max bid or min ask venue\n",
        "    # CRITICAL: Check for NaN venues and missing columns to avoid errors\n",
        "    for venue in max_bid_venue.unique():\n",
        "        if pd.isna(venue):  # Skip NaN venues (from edge case handling 4)\n",
        "            continue\n",
        "        bid_qty_col = f'{venue}_bid_qty'\n",
        "        if bid_qty_col in consolidated_tape.columns:  # Check column exists\n",
        "            mask = max_bid_venue == venue\n",
        "            bid_qty.loc[mask] = consolidated_tape.loc[mask, bid_qty_col]\n",
        "    \n",
        "    for venue in min_ask_venue.unique():\n",
        "        if pd.isna(venue):  # Skip NaN venues (from edge case handling 4)\n",
        "            continue\n",
        "        ask_qty_col = f'{venue}_ask_qty'\n",
        "        if ask_qty_col in consolidated_tape.columns:  # Check column exists\n",
        "            mask = min_ask_venue == venue\n",
        "            ask_qty.loc[mask] = consolidated_tape.loc[mask, ask_qty_col]\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 6: Calculate Profit per Opportunity\n",
        "    # ========================================================================\n",
        "    # Profit per unit = Max Bid - Min Ask\n",
        "    # Tradable quantity = Min(BidQty, AskQty)\n",
        "    # Total profit = Profit per unit * Tradable quantity\n",
        "    #\n",
        "    # Market Microstructure: We can only trade the minimum of available bid\n",
        "    # and ask quantities. This represents the maximum executable volume at\n",
        "    # the arbitrage prices.\n",
        "    #\n",
        "    # Note on NaN handling: Pandas operations naturally propagate NaN:\n",
        "    # - If bid_qty or ask_qty is NaN, Min() returns NaN\n",
        "    # - If profit_per_unit or tradable_qty is NaN, multiplication returns NaN\n",
        "    # - These NaN profits are filtered out in Step 8 (edge case handling 7)\n",
        "    \n",
        "    profit_per_unit = global_max_bid - global_min_ask\n",
        "    tradable_qty = pd.concat([bid_qty, ask_qty], axis=1).min(axis=1)\n",
        "    total_profit = profit_per_unit * tradable_qty\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 7: Implement Rising Edge Detection\n",
        "    # ========================================================================\n",
        "    # Market Microstructure: Rising edge detection ensures we only count\n",
        "    # an opportunity when it first appears (transitions from False to True).\n",
        "    # If an opportunity persists for 1 second (1000 snapshots), we only\n",
        "    # trade it once. Once it vanishes, we can trade it again when it reappears.\n",
        "    #\n",
        "    # ========================================================================\n",
        "    # EDGE CASE HANDLING 6: First Timestamp in Rising Edge Detection\n",
        "    # ========================================================================\n",
        "    # Market Microstructure: At the very first timestamp, there is no \"previous\"\n",
        "    # state to compare against. The .shift(1) operation returns NaN for the\n",
        "    # first row, which would break boolean operations.\n",
        "    #\n",
        "    # Why this happens:\n",
        "    # - .shift(1) shifts data down by one row, leaving first row as NaN\n",
        "    # - Boolean operations with NaN return False, but we want explicit control\n",
        "    #\n",
        "    # How we handle it:\n",
        "    # - Use .fillna(False) to treat \"before first timestamp\" as \"no opportunity\"\n",
        "    # - This ensures the first opportunity (if it exists) is correctly\n",
        "    #   identified as a rising edge (transition from \"no opportunity\" to \"opportunity\")\n",
        "    #\n",
        "    # Example: If first timestamp has an opportunity, it's a rising edge because\n",
        "    # we treat the \"before first\" state as False (no opportunity).\n",
        "    \n",
        "    # Detect rising edges: opportunity exists now but didn't exist before\n",
        "    # CRITICAL: fillna(False) handles the first timestamp edge case\n",
        "    opportunity_prev = arbitrage_opportunity.shift(1).fillna(False)\n",
        "    rising_edge = arbitrage_opportunity & ~opportunity_prev\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 8: Create Signal DataFrame (only rising edges)\n",
        "    # ========================================================================\n",
        "    # Filter to only rising edges and create structured output\n",
        "    \n",
        "    signals_df = pd.DataFrame({\n",
        "        'timestamp': consolidated_tape.index,\n",
        "        'global_max_bid': global_max_bid,\n",
        "        'global_min_ask': global_min_ask,\n",
        "        'max_bid_venue': max_bid_venue,\n",
        "        'min_ask_venue': min_ask_venue,\n",
        "        'bid_qty': bid_qty,\n",
        "        'ask_qty': ask_qty,\n",
        "        'tradable_qty': tradable_qty,\n",
        "        'profit_per_unit': profit_per_unit,\n",
        "        'total_profit': total_profit,\n",
        "        'arbitrage_opportunity': arbitrage_opportunity,\n",
        "        'rising_edge': rising_edge\n",
        "    })\n",
        "    \n",
        "    # Filter to only rising edges\n",
        "    signals_df = signals_df[signals_df['rising_edge'] == True].copy()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # EDGE CASE HANDLING 7: Filter Out NaN Profits\n",
        "    # ========================================================================\n",
        "    # Market Microstructure: If quantities are missing (NaN), the profit\n",
        "    # calculation results in NaN. We cannot trade an opportunity if we don't\n",
        "    # know how much volume is available. These NaN profits appear when:\n",
        "    # - Quantity columns are missing for the max bid or min ask venue\n",
        "    # - Quantities themselves are NaN in the data\n",
        "    # - Venue names are NaN (from edge case handling 4)\n",
        "    #\n",
        "    # Why we must filter them:\n",
        "    # - NaN profits are not actionable: We need to know tradable volume\n",
        "    # - They would skew statistics: Sum of profits would be NaN\n",
        "    # - They represent incomplete data: We can't execute without quantities\n",
        "    #\n",
        "    # How we handle it:\n",
        "    # - Filter out rows where total_profit is NaN\n",
        "    # - This ensures all signals have valid, executable opportunities\n",
        "    \n",
        "    # CRITICAL: Filter out NaN profits (incomplete data - can't execute without quantities)\n",
        "    signals_df = signals_df[signals_df['total_profit'].notna()].copy()\n",
        "    \n",
        "    # Set timestamp as index\n",
        "    signals_df.set_index('timestamp', inplace=True)\n",
        "    \n",
        "    # Drop the boolean columns (no longer needed)\n",
        "    signals_df = signals_df.drop(columns=['arbitrage_opportunity', 'rising_edge'])\n",
        "    \n",
        "    return signals_df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTE: Generate Arbitrage Signals for All ISINs\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 3: GENERATING ARBITRAGE SIGNALS FOR ALL ISINs\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Store signals for all ISINs\n",
        "arbitrage_signals = {}\n",
        "\n",
        "# Process each ISIN\n",
        "for idx, isin in enumerate(consolidated_tapes.keys(), 1):\n",
        "    print(f\"\\n[{idx}/{len(consolidated_tapes)}] Generating signals for ISIN: {isin}\")\n",
        "    \n",
        "    consolidated_tape = consolidated_tapes[isin]\n",
        "    \n",
        "    # Generate signals\n",
        "    signals = generate_arbitrage_signals_for_isin(isin, consolidated_tape, all_data)\n",
        "    \n",
        "    if not signals.empty:\n",
        "        arbitrage_signals[isin] = signals\n",
        "        \n",
        "        num_opportunities = len(signals)\n",
        "        total_profit = signals['total_profit'].sum()\n",
        "        max_profit = signals['total_profit'].max()\n",
        "        \n",
        "        print(f\"  âœ“ Found {num_opportunities:,} arbitrage opportunities (rising edges)\")\n",
        "        print(f\"  âœ“ Total theoretical profit: {total_profit:,.2f} â‚¬\")\n",
        "        print(f\"  âœ“ Maximum profit per opportunity: {max_profit:.4f} â‚¬\")\n",
        "        \n",
        "        # Show first few opportunities as example\n",
        "        if idx <= 3:  # Only show for first 3 ISINs\n",
        "            print(f\"\\n  Sample opportunities (first 5):\")\n",
        "            print(signals.head().to_string())\n",
        "    else:\n",
        "        print(f\"  âš ï¸  No arbitrage opportunities found\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"STEP 3 COMPLETE: Generated signals for {len(arbitrage_signals)} ISINs\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Summary statistics\n",
        "if arbitrage_signals:\n",
        "    total_opportunities = sum(len(signals) for signals in arbitrage_signals.values())\n",
        "    total_theoretical_profit = sum(signals['total_profit'].sum() for signals in arbitrage_signals.values())\n",
        "    \n",
        "    print(f\"\\nðŸ“Š OVERALL SUMMARY:\")\n",
        "    print(f\"   â€¢ Total ISINs with opportunities: {len(arbitrage_signals)}\")\n",
        "    print(f\"   â€¢ Total arbitrage opportunities: {total_opportunities:,}\")\n",
        "    print(f\"   â€¢ Total theoretical profit (0 latency): {total_theoretical_profit:,.2f} â‚¬\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  No arbitrage opportunities found across all ISINs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: The \"Time Machine\" (Latency Simulation)\n",
        "\n",
        "- In reality, if you see a price at time $T$, you cannot trade until $T + \\Delta$.\n",
        "- **Task:** Simulate execution latencies of [0, 100, 500, 1000, 2000, 3000, 4000, 5000, 10000, 15000, 20000, 30000, 50000, 100000] microseconds\n",
        "- *Method:* If a signal is detected at T, look up what the profit *actually is* at T + Latency in your dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 4: The \"Time Machine\" (Latency Simulation)\n",
        "# ============================================================================\n",
        "# Market Microstructure Context:\n",
        "# In real trading, there's a delay between detecting an opportunity and executing\n",
        "# the trade. This latency can be due to network delays, order processing time,\n",
        "# or exchange matching delays. By the time we execute, the opportunity may have\n",
        "# disappeared or changed. This simulation helps us understand how sensitive\n",
        "# our arbitrage strategy is to latency.\n",
        "#\n",
        "# CORRECTED IMPLEMENTATION:\n",
        "# - Uses integer epoch values instead of pandas timestamps for precision\n",
        "# - Uses searchsorted to find first snapshot >= execution time (not backward lookup)\n",
        "# - Calculates MICs at execution time, not signal time\n",
        "# - Executes trades even when spread <= 0 (realistic execution)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 4: LATENCY SIMULATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Define latency levels in microseconds\n",
        "# These represent realistic execution delays in HFT systems\n",
        "latency_levels = [0, 100, 500, 1000, 2000, 3000, 4000, 5000, 10000, 15000, 20000, 30000, 50000, 100000]\n",
        "\n",
        "# Dictionary to store realized profits for each latency level\n",
        "# Structure: {latency_us: total_realized_profit}\n",
        "latency_results = {}\n",
        "\n",
        "# ============================================================================\n",
        "# OPTIMIZATION: Pre-process all latency-independent data\n",
        "# ============================================================================\n",
        "# All expensive operations that don't depend on latency are done ONCE here,\n",
        "# before the latency loop. This includes:\n",
        "# - Extending tapes with quantities\n",
        "# - Calculating global_max_bid, global_min_ask\n",
        "# - Calculating mic_max_bid, mic_min_ask for all rows\n",
        "# - Converting timestamps to epoch\n",
        "# - Extracting signal timestamps\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Pre-processing ISINs (this is done once, before latency simulation)...\")\n",
        "print(f\"Processing {len(arbitrage_signals)} ISINs...\")\n",
        "\n",
        "preprocessed_isins = {}\n",
        "processed_count = 0\n",
        "\n",
        "for isin, signals_df in arbitrage_signals.items():\n",
        "    processed_count += 1\n",
        "    if processed_count % 10 == 0:\n",
        "        print(f\"  Processed {processed_count}/{len(arbitrage_signals)} ISINs...\")\n",
        "    if signals_df.empty:\n",
        "        continue\n",
        "    \n",
        "    # Get the consolidated tape for this ISIN\n",
        "    if isin not in consolidated_tapes:\n",
        "        continue\n",
        "    \n",
        "    consolidated_tape = consolidated_tapes[isin].copy()\n",
        "    if consolidated_tape.empty:\n",
        "        continue\n",
        "    \n",
        "    # Extend consolidated tape with quantities if needed (ONCE per ISIN)\n",
        "    bid_qty_cols = [col for col in consolidated_tape.columns if '_bid_qty' in col]\n",
        "    ask_qty_cols = [col for col in consolidated_tape.columns if '_ask_qty' in col]\n",
        "    \n",
        "    if not bid_qty_cols or not ask_qty_cols:\n",
        "        consolidated_tape = extend_consolidated_tape_with_quantities(\n",
        "            isin, consolidated_tape, all_data\n",
        "        )\n",
        "        bid_qty_cols = [col for col in consolidated_tape.columns if '_bid_qty' in col]\n",
        "        ask_qty_cols = [col for col in consolidated_tape.columns if '_ask_qty' in col]\n",
        "    \n",
        "    # Get bid and ask price columns (ONCE per ISIN)\n",
        "    bid_cols = [col for col in consolidated_tape.columns if '_bid' in col and '_qty' not in col]\n",
        "    ask_cols = [col for col in consolidated_tape.columns if '_ask' in col and '_qty' not in col]\n",
        "    \n",
        "    if not bid_cols or not ask_cols:\n",
        "        continue\n",
        "    \n",
        "    # Calculate global_max_bid and global_min_ask (ONCE per ISIN)\n",
        "    if 'global_max_bid' not in consolidated_tape.columns:\n",
        "        consolidated_tape['global_max_bid'] = consolidated_tape[bid_cols].max(axis=1)\n",
        "    if 'global_min_ask' not in consolidated_tape.columns:\n",
        "        consolidated_tape['global_min_ask'] = consolidated_tape[ask_cols].min(axis=1)\n",
        "    \n",
        "    # Get new_opportunity signals (ONCE per ISIN)\n",
        "    if 'new_opportunity' in signals_df.columns:\n",
        "        signal_indices = signals_df[signals_df['new_opportunity']].index\n",
        "    else:\n",
        "        # Calculate new_opportunity if not present\n",
        "        arbitrage_opportunity = consolidated_tape['global_max_bid'] > consolidated_tape['global_min_ask']\n",
        "        arbitrage_prev = arbitrage_opportunity.shift(1, fill_value=False)\n",
        "        new_opportunity = arbitrage_opportunity & ~arbitrage_prev\n",
        "        signal_indices = consolidated_tape[new_opportunity].index\n",
        "    \n",
        "    if len(signal_indices) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Convert timestamp index to epoch microseconds (ONCE per ISIN)\n",
        "    if consolidated_tape.index.dtype == 'datetime64[ns]' or (len(consolidated_tape) > 0 and isinstance(consolidated_tape.index[0], pd.Timestamp)):\n",
        "        tape_epoch = consolidated_tape.index.astype('int64') // 1000  # nanoseconds to microseconds\n",
        "    else:\n",
        "        tape_epoch = pd.Series(consolidated_tape.index).astype('int64')\n",
        "    \n",
        "    tape_epoch_values = tape_epoch.values  # numpy array for fast searchsorted\n",
        "    \n",
        "    # Convert signal timestamps to epoch microseconds (ONCE per ISIN)\n",
        "    signal_epochs = []\n",
        "    for signal_timestamp in signal_indices:\n",
        "        if isinstance(signal_timestamp, pd.Timestamp):\n",
        "            t_signal = int(signal_timestamp.value // 1000)\n",
        "        else:\n",
        "            t_signal = int(signal_timestamp)\n",
        "        signal_epochs.append(t_signal)\n",
        "    signal_epochs = np.array(signal_epochs)  # numpy array for vectorized operations\n",
        "    \n",
        "    # Calculate MICs for best bid/ask at each timestamp (ONCE per ISIN, for ALL rows)\n",
        "    mic_max_bid = pd.Series(index=consolidated_tape.index, dtype=\"object\")\n",
        "    mic_min_ask = pd.Series(index=consolidated_tape.index, dtype=\"object\")\n",
        "    \n",
        "    valid_bid_rows = consolidated_tape[bid_cols].notna().any(axis=1)\n",
        "    mic_max_bid.loc[valid_bid_rows] = (\n",
        "        consolidated_tape.loc[valid_bid_rows, bid_cols]\n",
        "        .idxmax(axis=1)\n",
        "        .str.replace('_bid', '', regex=False)\n",
        "    )\n",
        "    \n",
        "    valid_ask_rows = consolidated_tape[ask_cols].notna().any(axis=1)\n",
        "    mic_min_ask.loc[valid_ask_rows] = (\n",
        "        consolidated_tape.loc[valid_ask_rows, ask_cols]\n",
        "        .idxmin(axis=1)\n",
        "        .str.replace('_ask', '', regex=False)\n",
        "    )\n",
        "    \n",
        "    # Store preprocessed data for this ISIN\n",
        "    preprocessed_isins[isin] = {\n",
        "        'tape': consolidated_tape,\n",
        "        'epoch_values': tape_epoch_values,\n",
        "        'signal_epochs': signal_epochs,\n",
        "        'signal_indices': signal_indices,\n",
        "        'mic_max_bid': mic_max_bid,\n",
        "        'mic_min_ask': mic_min_ask,\n",
        "        'bid_qty_cols': bid_qty_cols,\n",
        "        'ask_qty_cols': ask_qty_cols,\n",
        "        'n': len(consolidated_tape)\n",
        "    }\n",
        "\n",
        "print(f\"Pre-processing complete: {len(preprocessed_isins)} ISINs ready for latency simulation\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# OPTIMIZED LATENCY LOOP\n",
        "# ============================================================================\n",
        "# Now we only loop through latencies and signals, using preprocessed data\n",
        "# ============================================================================\n",
        "\n",
        "# Process each latency level\n",
        "for latency_us in latency_levels:\n",
        "    print(f\"\\n[Processing Latency: {latency_us} microseconds]\")\n",
        "    \n",
        "    total_realized_profit = 0.0\n",
        "    total_opportunities = 0\n",
        "    \n",
        "    # Process each ISIN using preprocessed data\n",
        "    for isin, preprocessed in preprocessed_isins.items():\n",
        "        consolidated_tape = preprocessed['tape']\n",
        "        tape_epoch_values = preprocessed['epoch_values']\n",
        "        signal_epochs = preprocessed['signal_epochs']\n",
        "        signal_indices = preprocessed['signal_indices']\n",
        "        mic_max_bid = preprocessed['mic_max_bid']\n",
        "        mic_min_ask = preprocessed['mic_min_ask']\n",
        "        n = preprocessed['n']\n",
        "        \n",
        "        # Process each signal (new opportunity)\n",
        "        isin_profit = 0.0\n",
        "        isin_opportunities = 0\n",
        "        \n",
        "        for i, signal_epoch in enumerate(signal_epochs):\n",
        "            # Calculate target execution time (simple addition)\n",
        "            t_exec_target = signal_epoch + latency_us\n",
        "            \n",
        "            # Find first snapshot with epoch >= t_exec_target using searchsorted\n",
        "            pos = np.searchsorted(tape_epoch_values, t_exec_target, side='left')\n",
        "            \n",
        "            if pos >= n:\n",
        "                continue  # No data at or after t_exec_target\n",
        "            \n",
        "            # Get execution row (using iloc for faster access)\n",
        "            row_exec = consolidated_tape.iloc[pos]\n",
        "            \n",
        "            # Prices at execution time\n",
        "            max_bid = row_exec['global_max_bid']\n",
        "            min_ask = row_exec['global_min_ask']\n",
        "            spread = max_bid - min_ask\n",
        "            \n",
        "            # Get MICs at execution time (not signal time!)\n",
        "            buy_mic = mic_min_ask.iloc[pos]\n",
        "            sell_mic = mic_max_bid.iloc[pos]\n",
        "            \n",
        "            # Get quantities at execution time\n",
        "            if pd.notna(buy_mic) and pd.notna(sell_mic):\n",
        "                bid_qty_col = f'{sell_mic}_bid_qty'\n",
        "                ask_qty_col = f'{buy_mic}_ask_qty'\n",
        "                \n",
        "                bid_qty = row_exec.get(bid_qty_col, np.nan) if bid_qty_col in consolidated_tape.columns else np.nan\n",
        "                ask_qty = row_exec.get(ask_qty_col, np.nan) if ask_qty_col in consolidated_tape.columns else np.nan\n",
        "                \n",
        "                # How much we can trade\n",
        "                trade_qty = min(bid_qty, ask_qty) if pd.notna(bid_qty) and pd.notna(ask_qty) else 0.0\n",
        "                \n",
        "                if np.isnan(trade_qty) or trade_qty <= 0:\n",
        "                    profit = 0.0\n",
        "                else:\n",
        "                    # ALWAYS execute, even if spread <= 0 (realistic execution)\n",
        "                    profit = spread * trade_qty\n",
        "            else:\n",
        "                profit = 0.0\n",
        "            \n",
        "            isin_profit += profit\n",
        "            isin_opportunities += 1\n",
        "        \n",
        "        total_realized_profit += isin_profit\n",
        "        total_opportunities += isin_opportunities\n",
        "    \n",
        "    # Store results for this latency level\n",
        "    latency_results[latency_us] = {\n",
        "        'total_realized_profit': total_realized_profit,\n",
        "        'total_opportunities': total_opportunities\n",
        "    }\n",
        "    \n",
        "    print(f\"  Total Realized Profit: {total_realized_profit:,.2f} â‚¬\")\n",
        "    print(f\"  Total Opportunities Processed: {total_opportunities:,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 4 COMPLETE: Latency simulation finished\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Deliverables & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Submit a Jupyter Notebook containing your code and the following analysis:\n",
        "\n",
        "1. **The \"Money Table\":** A summary table showing the Total Realized Profit for all processed ISINs at each latency level.\n",
        "2. **The Decay Chart:** A line chart visualizing how Total Profit (Y-axis) decays as Latency (X-axis) increases.\n",
        "3. **Top Opportunities:** A list of the Top 5 most profitable ISINs (at 0 latency). **Sanity check these results**â€”do they look real?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. The \"Money Table\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DELIVERABLE 1: The \"Money Table\"\n",
        "# ============================================================================\n",
        "# Market Microstructure Context:\n",
        "# This table summarizes the total realized profit at each latency level.\n",
        "# It shows how quickly arbitrage opportunities disappear as execution latency\n",
        "# increases. This is critical for HFT firms to understand the minimum\n",
        "# latency requirements for profitability.\n",
        "\n",
        "# Create the Money Table DataFrame\n",
        "money_table_data = []\n",
        "for latency_us in latency_levels:\n",
        "    result = latency_results[latency_us]\n",
        "    money_table_data.append({\n",
        "        'Latency (Âµs)': latency_us,\n",
        "        'Total Realized Profit (â‚¬)': result['total_realized_profit'],\n",
        "        'Number of Opportunities': result['total_opportunities']\n",
        "    })\n",
        "\n",
        "money_table = pd.DataFrame(money_table_data)\n",
        "\n",
        "# Format for better readability\n",
        "money_table['Total Realized Profit (â‚¬)'] = money_table['Total Realized Profit (â‚¬)'].apply(lambda x: f\"{x:,.2f}\")\n",
        "\n",
        "# Display the table\n",
        "print(\"=\" * 70)\n",
        "print(\"THE MONEY TABLE: Total Realized Profit by Latency Level\")\n",
        "print(\"=\" * 70)\n",
        "print(money_table.to_string(index=False))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Also store the numeric version for the chart\n",
        "money_table_numeric = pd.DataFrame(money_table_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. The Decay Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DELIVERABLE 2: The Decay Chart\n",
        "# ============================================================================\n",
        "# Market Microstructure Context:\n",
        "# This visualization shows the \"latency decay curve\" - how quickly arbitrage\n",
        "# profits disappear as execution latency increases. This is a critical metric\n",
        "# for HFT firms: it shows the maximum acceptable latency before the strategy\n",
        "# becomes unprofitable. The steeper the decay, the more latency-sensitive\n",
        "# the arbitrage opportunity.\n",
        "\n",
        "# Create the decay chart\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Extract data for plotting\n",
        "latencies = money_table_numeric['Latency (Âµs)']\n",
        "profits = money_table_numeric['Total Realized Profit (â‚¬)']\n",
        "\n",
        "# Plot the decay curve\n",
        "ax.plot(latencies, profits, marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlabel('Latency (microseconds)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Total Realized Profit (â‚¬)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Latency Decay Curve: Profit vs Execution Latency', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "# Add grid for better readability\n",
        "ax.grid(True, linestyle='--', alpha=0.7, linewidth=0.5)\n",
        "\n",
        "# Format x-axis to show latency in a readable format\n",
        "ax.set_xscale('linear')\n",
        "# Add minor ticks for better granularity\n",
        "ax.xaxis.set_minor_locator(plt.MultipleLocator(5000))\n",
        "\n",
        "# Format y-axis to show currency with commas\n",
        "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:,.0f}'))\n",
        "\n",
        "# Add annotations for key points (0 latency and where profit drops significantly)\n",
        "if len(profits) > 0:\n",
        "    # Annotate 0 latency point\n",
        "    ax.annotate(f'â‚¬{profits.iloc[0]:,.2f}', \n",
        "                xy=(latencies.iloc[0], profits.iloc[0]),\n",
        "                xytext=(10, 10), textcoords='offset points',\n",
        "                bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
        "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'),\n",
        "                fontsize=10, fontweight='bold')\n",
        "    \n",
        "    # Find where profit drops to 50% of initial (if applicable)\n",
        "    if profits.iloc[0] > 0:\n",
        "        half_profit = profits.iloc[0] / 2\n",
        "        for i, profit in enumerate(profits):\n",
        "            if profit <= half_profit:\n",
        "                ax.annotate(f'50% Decay\\nâ‚¬{profit:,.2f}',\n",
        "                           xy=(latencies.iloc[i], profit),\n",
        "                           xytext=(10, -30), textcoords='offset points',\n",
        "                           bbox=dict(boxstyle='round,pad=0.5', facecolor='orange', alpha=0.7),\n",
        "                           arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'),\n",
        "                           fontsize=9)\n",
        "                break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DECAY CHART GENERATED: Visualizing profit decay with latency\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Top Opportunities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DELIVERABLE 3: Top 5 Opportunities (at 0 Latency)\n",
        "# ============================================================================\n",
        "# Market Microstructure Context:\n",
        "# This identifies the most profitable ISINs for arbitrage at 0 latency\n",
        "# (theoretical maximum). These are the instruments where price discrepancies\n",
        "# between venues are largest and most frequent. Sanity checking these results\n",
        "# is critical: we need to verify that the profits are realistic and not due\n",
        "# to data errors or off-book trades.\n",
        "\n",
        "# Calculate total profit per ISIN at 0 latency\n",
        "isin_profits = []\n",
        "for isin, signals_df in arbitrage_signals.items():\n",
        "    if not signals_df.empty:\n",
        "        total_profit = signals_df['total_profit'].sum()\n",
        "        num_opportunities = len(signals_df)\n",
        "        avg_profit_per_opportunity = total_profit / num_opportunities if num_opportunities > 0 else 0\n",
        "        max_profit = signals_df['total_profit'].max()\n",
        "        \n",
        "        isin_profits.append({\n",
        "            'ISIN': isin,\n",
        "            'Total Profit (â‚¬)': total_profit,\n",
        "            'Number of Opportunities': num_opportunities,\n",
        "            'Avg Profit per Opportunity (â‚¬)': avg_profit_per_opportunity,\n",
        "            'Max Profit per Opportunity (â‚¬)': max_profit\n",
        "        })\n",
        "\n",
        "# Create DataFrame and sort by total profit\n",
        "top_opportunities_df = pd.DataFrame(isin_profits)\n",
        "top_opportunities_df = top_opportunities_df.sort_values('Total Profit (â‚¬)', ascending=False)\n",
        "\n",
        "# Get top 5\n",
        "top_5_opportunities = top_opportunities_df.head(5).copy()\n",
        "\n",
        "# Format for display\n",
        "top_5_display = top_5_opportunities.copy()\n",
        "top_5_display['Total Profit (â‚¬)'] = top_5_display['Total Profit (â‚¬)'].apply(lambda x: f\"{x:,.2f}\")\n",
        "top_5_display['Avg Profit per Opportunity (â‚¬)'] = top_5_display['Avg Profit per Opportunity (â‚¬)'].apply(lambda x: f\"{x:.4f}\")\n",
        "top_5_display['Max Profit per Opportunity (â‚¬)'] = top_5_display['Max Profit per Opportunity (â‚¬)'].apply(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "# Display results\n",
        "print(\"=\" * 70)\n",
        "print(\"TOP 5 MOST PROFITABLE ISINs (at 0 Latency)\")\n",
        "print(\"=\" * 70)\n",
        "print(top_5_display.to_string(index=False))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Sanity Check Analysis\n",
        "print(\"=\" * 70)\n",
        "print(\"SANITY CHECK ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n1. Profit Magnitude Check:\")\n",
        "print(\"   - Are the profit values realistic for arbitrage?\")\n",
        "print(\"   - Typical arbitrage profits are small (cents per share)\")\n",
        "print(\"   - Large profits might indicate data quality issues\\n\")\n",
        "\n",
        "print(\"2. Opportunity Frequency Check:\")\n",
        "for idx, row in top_5_opportunities.iterrows():\n",
        "    isin = row['ISIN']\n",
        "    num_opps = int(row['Number of Opportunities'])\n",
        "    avg_profit = row['Avg Profit per Opportunity (â‚¬)']\n",
        "    print(f\"   - {isin}: {num_opps:,} opportunities, avg profit: â‚¬{avg_profit:.4f}\")\n",
        "\n",
        "print(\"\\n3. Data Quality Check:\")\n",
        "print(\"   - Verify that these ISINs have sufficient liquidity\")\n",
        "print(\"   - Check that prices are from lit order books (not off-book)\")\n",
        "print(\"   - Ensure timestamps are correctly synchronized across venues\\n\")\n",
        "\n",
        "print(\"4. Market Microstructure Validation:\")\n",
        "print(\"   - High-frequency arbitrage opportunities are typically:\")\n",
        "print(\"     * Very small profit per trade (â‚¬0.01 - â‚¬0.10)\")\n",
        "print(\"     * Very frequent (hundreds or thousands per day)\")\n",
        "print(\"     * Highly sensitive to latency (disappear in microseconds)\\n\")\n",
        "\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grading Rubric (Max 10 Points)\n",
        "\n",
        "- **5-6 Points (Baseline):** The code runs, correctly calculates the consolidated tape, identifies Bid > Ask opportunities, and estimates theoretical (0 latency) profit.\n",
        "\n",
        "- **7-8 Points (Robust):** The simulation accurately models latency (using strict time-lookups) and strictly adheres to the vendor's data quality specs.\n",
        "\n",
        "- **9-10 Points (Expert):** You demonstrate deep understanding of market microstructure. You handle **Market Status** correctly to avoid fake signals, identify anomalies in the instrument list, and handle edge cases around Market Open/Close."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
