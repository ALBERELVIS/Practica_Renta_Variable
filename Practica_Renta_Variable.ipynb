{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab Exercise: High-Frequency Arbitrage in Fragmented Markets\n",
        "\n",
        "**Deadline:** 9th of December 23:59 CET\n",
        "\n",
        "**Submission:** Email to francisco.merlos@six-group.com with title: \"Arbitrage study in BME | Your name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Context: The Fragmented Market\n",
        "\n",
        "In modern European equity markets, liquidity is **fragmented**. The same stock (ISIN) trades simultaneously on the primary exchange (BME) and various Multilateral Trading Facilities (MTFs) like CBOE, Turquoise, and Aquis.\n",
        "\n",
        "Due to this fragmentation, temporary price discrepancies occur. A stock might be offered for sale at €10.00 on Turquoise while a buyer is bidding €10.01 on BME. A High-Frequency Trader (HFT) can profit from this by buying low and selling high instantaneously.\n",
        "\n",
        "However, these opportunities are fleeting. The \"theoretical\" profit you see in a snapshot might disappear by the time your order reaches the exchange due to **latency**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Mission\n",
        "\n",
        "You have been hired as a Quantitative Researcher at a proprietary trading firm. Your boss has given you a dataset of high-resolution market data and asked you to answer three critical questions:\n",
        "\n",
        "1. **Do arbitrage opportunities still exist in Spanish equities?**\n",
        "2. **What is the maximum theoretical profit** (assuming 0 latency)?\n",
        "3. **The \"Latency Decay\" Curve:** How quickly does this profit vanish as our trading system gets slower (from 0µs to 100ms)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Specifications\n",
        "\n",
        "You are provided with a `DATA_BIG/` folder containing subfolders for specific trading dates. Inside, you will find three types of compressed CSV files for various instruments.\n",
        "\n",
        "**Note:** You can also find a `DATA_SMALL` folder that you can use to test quickly without needing to run the simulation over all the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### File Naming Convention\n",
        "\n",
        "The naming pattern for all three file types (QTE, STS, TRD) is:\n",
        "\n",
        "```\n",
        "<type>_<session>_<isin>_<ticker>_<mic>_<part>.csv.gz\n",
        "```\n",
        "\n",
        "| Field | Description |\n",
        "|-------|-------------|\n",
        "| **type** | QTE, TRD, or STS |\n",
        "| **session** | Trading date (YYYY-MM-DD) |\n",
        "| **isin** | Cross-venue **ISIN** (International Securities Identification Number) |\n",
        "| **ticker** | Venue-specific trading symbol (distinguishes multiple books for the same ISIN on the same MIC) |\n",
        "| **mic** | Market Identifier Code (MIC, e.g., XMAD) |\n",
        "| **part** | Integer part number. Assume it is always 1 for simplicity. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Order Book Identity and Join Key\n",
        "\n",
        "A single **order book identity** is defined by the tuple:\n",
        "\n",
        "```\n",
        "(session, isin, mic, ticker)\n",
        "```\n",
        "\n",
        "This identity is the **key used to join** corresponding QTE, TRD, and STS data belonging to the same book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### File Types\n",
        "\n",
        "1. **QTE (Quotes/Snapshots):** Represents the state of the order book (up to 10 levels deep).\n",
        "   - `epoch`: Timestamp in microseconds (UTC).\n",
        "   - `px_bid_0`, `px_ask_0`: Best Bid and Best Ask prices.\n",
        "   - `qty_bid_0`, `qty_ask_0`: Available volume at the best price.\n",
        "   - *Note: Columns exist for levels 0-9.*\n",
        "\n",
        "2. **STS (Trading Status):** Updates on the market phase (e.g., Open, Auction, Closed).\n",
        "   - `epoch`: Timestamp.\n",
        "   - `market_trading_status`: An integer code representing the state.\n",
        "\n",
        "3. **TRD (Trades):** Represents the transactions. Not needed for this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CRITICAL: Vendor Data Definitions\n",
        "\n",
        "Real-world financial data is rarely clean. The data vendor has provided the following specifications. **Ignoring these will result in massive errors in your P&L calculation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### A. \"Magic Numbers\" (Special Price Codes)\n",
        "\n",
        "The vendor uses specific high-value constants to indicate non-tradable states (e.g., Market Orders during auctions). **These are NOT real prices.** If you treat 999,999 as a valid bid, your algorithm will assume you can sell for a million euros.\n",
        "\n",
        "| Value | Meaning | Action Required |\n",
        "|-------|---------|----------------|\n",
        "| 666666.666 | Unquoted/Unknown | **Discard** |\n",
        "| 999999.999 | Market Order (At Best) | **Discard** |\n",
        "| 999999.989 | At Open Order | **Discard** |\n",
        "| 999999.988 | At Close Order | **Discard** |\n",
        "| 999999.979 | Pegged Order | **Discard** |\n",
        "| 999999.123 | Unquoted/Unknown | **Discard** |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### B. Market Status Codes\n",
        "\n",
        "You can only trade when the market is in **Continuous Trading**. If you trade during an Auction, a Halt, or Pre-Open, your order will not execute immediately. A snapshot is only valid/addressable if the STS for that venue is one of these codes:\n",
        "\n",
        "| Venue | Continuous Trading Code |\n",
        "|-------|------------------------|\n",
        "| AQUIS | 5308427 |\n",
        "| BME | 5832713, 5832756 |\n",
        "| CBOE | 12255233 |\n",
        "| TURQUOISE | 7608181 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Implementation Guide\n",
        "\n",
        "You are encouraged to use AI tools (ChatGPT, Claude, etc.) to generate the Python/Pandas code. However, **you** are responsible for the logic and the financial validity of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Data Ingestion & Cleaning\n",
        "\n",
        "- Write a function to load the QTE and STS files for a given ISIN.\n",
        "- **Task:** Ensure you are using only valid prices\n",
        "- **Task:** Ensure you are only looking at addressable orderbooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: DATA INGESTION & CLEANING\n",
        "# ============================================================================\n",
        "\n",
        "# Define magic numbers that represent invalid/non-tradable prices\n",
        "# These are special codes used by the vendor to indicate non-tradable states\n",
        "# (e.g., Market Orders during auctions). Treating these as real prices would\n",
        "# cause massive errors in P&L calculations.\n",
        "MAGIC_NUMBERS = [\n",
        "    666666.666,  # Unquoted/Unknown\n",
        "    999999.999,  # Market Order (At Best)\n",
        "    999999.989,  # At Open Order\n",
        "    999999.988,  # At Close Order\n",
        "    999999.979,  # Pegged Order\n",
        "    999999.123   # Unquoted/Unknown\n",
        "]\n",
        "\n",
        "# Define continuous trading status codes per venue\n",
        "# These codes indicate when the market is in continuous trading mode,\n",
        "# which is the only time orders can execute immediately. Trading during\n",
        "# auctions, halts, or pre-open would not execute immediately.\n",
        "CONTINUOUS_TRADING_STATUS = {\n",
        "    'AQEU': [5308427],           # AQUIS\n",
        "    'XMAD': [5832713, 5832756],  # BME (two codes for different segments)\n",
        "    'CEUX': [12255233],          # CBOE\n",
        "    'TQEX': [7608181]            # TURQUOISE\n",
        "}\n",
        "\n",
        "\n",
        "def discover_files_for_isin(isin, data_path='DATA_BIG'):\n",
        "    \"\"\"\n",
        "    Discover all QTE and STS files for a given ISIN across all venues and dates.\n",
        "    \n",
        "    This function scans the data directory structure to find all relevant files\n",
        "    for the specified ISIN. It handles the file naming convention:\n",
        "    <type>_<session>_<isin>_<ticker>_<mic>_<part>.csv.gz\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    isin : str\n",
        "        The ISIN (International Securities Identification Number) to search for\n",
        "    data_path : str\n",
        "        Path to the data directory (default: 'DATA_BIG')\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary with keys 'qte' and 'sts', each containing a list of\n",
        "        file paths grouped by orderbook identity (session, isin, mic, ticker)\n",
        "    \"\"\"\n",
        "    qte_files = {}\n",
        "    sts_files = {}\n",
        "    \n",
        "    # Get all venue-date folders (e.g., BME_2025-11-07, CBOE_2025-11-07)\n",
        "    data_dir = Path(data_path)\n",
        "    if not data_dir.exists():\n",
        "        raise ValueError(f\"Data path does not exist: {data_path}\")\n",
        "    \n",
        "    # Find all venue-date folders\n",
        "    venue_date_folders = [d for d in data_dir.iterdir() if d.is_dir()]\n",
        "    \n",
        "    # Search for QTE and STS files matching the ISIN pattern\n",
        "    for folder in venue_date_folders:\n",
        "        # Pattern: QTE_*_<isin>_*.csv.gz or STS_*_<isin>_*.csv.gz\n",
        "        qte_pattern = str(folder / f'QTE_*_{isin}_*.csv.gz')\n",
        "        sts_pattern = str(folder / f'STS_*_{isin}_*.csv.gz')\n",
        "        \n",
        "        # Find matching files\n",
        "        qte_matches = glob.glob(qte_pattern)\n",
        "        sts_matches = glob.glob(sts_pattern)\n",
        "        \n",
        "        # Process QTE files\n",
        "        for file_path in qte_matches:\n",
        "            # Parse filename to extract orderbook identity\n",
        "            # Format: QTE_<session>_<isin>_<ticker>_<mic>_<part>.csv.gz\n",
        "            filename = Path(file_path).name\n",
        "            parts = filename.replace('.csv.gz', '').split('_')\n",
        "            \n",
        "            if len(parts) >= 6:\n",
        "                session = parts[1]\n",
        "                file_isin = parts[2]\n",
        "                ticker = parts[3]\n",
        "                mic = parts[4]\n",
        "                part = int(parts[5])\n",
        "                \n",
        "                # Create orderbook identity key\n",
        "                identity = (session, file_isin, mic, ticker)\n",
        "                \n",
        "                # Handle multiple part numbers - keep only the highest part\n",
        "                if identity in qte_files:\n",
        "                    existing_part = int(Path(qte_files[identity]).name.split('_')[5].replace('.csv.gz', ''))\n",
        "                    if part > existing_part:\n",
        "                        warnings.warn(\n",
        "                            f\"Multiple part numbers found for {identity}. \"\n",
        "                            f\"Using part {part} instead of {existing_part}.\"\n",
        "                        )\n",
        "                        qte_files[identity] = file_path\n",
        "                    elif part < existing_part:\n",
        "                        warnings.warn(\n",
        "                            f\"Multiple part numbers found for {identity}. \"\n",
        "                            f\"Using part {existing_part} instead of {part}.\"\n",
        "                        )\n",
        "                else:\n",
        "                    qte_files[identity] = file_path\n",
        "        \n",
        "        # Process STS files (same logic)\n",
        "        for file_path in sts_matches:\n",
        "            filename = Path(file_path).name\n",
        "            parts = filename.replace('.csv.gz', '').split('_')\n",
        "            \n",
        "            if len(parts) >= 6:\n",
        "                session = parts[1]\n",
        "                file_isin = parts[2]\n",
        "                ticker = parts[3]\n",
        "                mic = parts[4]\n",
        "                part = int(parts[5])\n",
        "                \n",
        "                identity = (session, file_isin, mic, ticker)\n",
        "                \n",
        "                if identity in sts_files:\n",
        "                    existing_part = int(Path(sts_files[identity]).name.split('_')[5].replace('.csv.gz', ''))\n",
        "                    if part > existing_part:\n",
        "                        warnings.warn(\n",
        "                            f\"Multiple part numbers found for {identity}. \"\n",
        "                            f\"Using part {part} instead of {existing_part}.\"\n",
        "                        )\n",
        "                        sts_files[identity] = file_path\n",
        "                    elif part < existing_part:\n",
        "                        warnings.warn(\n",
        "                            f\"Multiple part numbers found for {identity}. \"\n",
        "                            f\"Using part {existing_part} instead of {part}.\"\n",
        "                        )\n",
        "                else:\n",
        "                    sts_files[identity] = file_path\n",
        "    \n",
        "    return {'qte': qte_files, 'sts': sts_files}\n",
        "\n",
        "\n",
        "def clean_qte_timestamps(df):\n",
        "    \"\"\"\n",
        "    Clean and create unique timestamp index for QTE (orderbook snapshot) data.\n",
        "    \n",
        "    For orderbook snapshots, if multiple snapshots occur at the same microsecond,\n",
        "    we only care about the final state (last snapshot). This is because the orderbook\n",
        "    state at the end of that microsecond is what matters for trading decisions.\n",
        "    \n",
        "    After keeping the last snapshot per epoch, we apply the nanosecond trick to\n",
        "    ensure unique timestamps for indexing purposes.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        QTE DataFrame with 'epoch' column\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with unique timestamp index\n",
        "    \"\"\"\n",
        "    # Sort by epoch (and sequence if it exists)\n",
        "    sort_cols = ['epoch']\n",
        "    if 'sequence' in df.columns:\n",
        "        sort_cols.append('sequence')\n",
        "    df = df.sort_values(by=sort_cols, ascending=[True] * len(sort_cols))\n",
        "    \n",
        "    # For orderbook snapshots, keep only the last snapshot per epoch\n",
        "    # Multiple snapshots at the same microsecond represent intermediate states,\n",
        "    # but we only care about the final state for trading decisions\n",
        "    df = df.drop_duplicates(subset='epoch', keep='last').copy()\n",
        "    \n",
        "    # Convert epoch to datetime (microseconds since Unix epoch)\n",
        "    temp_ts = pd.to_datetime(df['epoch'], unit='us')\n",
        "    \n",
        "    # Apply nanosecond trick to handle any remaining duplicates\n",
        "    # This creates a unique timestamp index by adding nanoseconds\n",
        "    offset_ns = df.groupby('epoch').cumcount()\n",
        "    \n",
        "    if len(offset_ns) > 0 and offset_ns.max() > 1000:\n",
        "        raise Exception(\n",
        "            f\"There are more than 1000 snapshots happening at the same microsecond. \"\n",
        "            f\"Max number: {offset_ns.max()}\"\n",
        "        )\n",
        "    \n",
        "    # Create final high-resolution timestamp\n",
        "    df['ts'] = temp_ts + pd.to_timedelta(offset_ns, unit='ns')\n",
        "    \n",
        "    # Set timestamp as index\n",
        "    df.set_index('ts', inplace=True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_sts_timestamps(df):\n",
        "    \"\"\"\n",
        "    Clean and create unique timestamp index for STS (trading status) data.\n",
        "    \n",
        "    Similar to QTE, but STS typically has fewer duplicates. We apply the\n",
        "    nanosecond trick to ensure unique timestamps.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        STS DataFrame with 'epoch' column\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with unique timestamp index\n",
        "    \"\"\"\n",
        "    # Sort by epoch\n",
        "    df = df.sort_values(by='epoch', ascending=True)\n",
        "    \n",
        "    # Convert epoch to datetime\n",
        "    temp_ts = pd.to_datetime(df['epoch'], unit='us')\n",
        "    \n",
        "    # Apply nanosecond trick for unique timestamps\n",
        "    offset_ns = df.groupby('epoch').cumcount()\n",
        "    \n",
        "    if len(offset_ns) > 0 and offset_ns.max() > 1000:\n",
        "        raise Exception(\n",
        "            f\"There are more than 1000 status updates at the same microsecond. \"\n",
        "            f\"Max number: {offset_ns.max()}\"\n",
        "        )\n",
        "    \n",
        "    # Create final timestamp\n",
        "    df['ts'] = temp_ts + pd.to_timedelta(offset_ns, unit='ns')\n",
        "    \n",
        "    # Set timestamp as index\n",
        "    df.set_index('ts', inplace=True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def load_qte_sts_for_isin(isin, data_path='DATA_BIG'):\n",
        "    \"\"\"\n",
        "    Load and clean QTE (quotes) and STS (trading status) files for a given ISIN.\n",
        "    \n",
        "    This function:\n",
        "    1. Discovers all QTE and STS files for the ISIN across all venues and dates\n",
        "    2. Loads the data using pandas\n",
        "    3. Filters out invalid prices (magic numbers)\n",
        "    4. Filters for addressable orderbooks (continuous trading status only)\n",
        "    5. Creates unique timestamp indexes using the nanosecond trick\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    isin : str\n",
        "        The ISIN to load data for\n",
        "    data_path : str\n",
        "        Path to the data directory (default: 'DATA_BIG')\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (df_qte_clean, df_sts_clean) - Cleaned QTE and STS DataFrames\n",
        "    \"\"\"\n",
        "    # Discover all files for this ISIN\n",
        "    files = discover_files_for_isin(isin, data_path)\n",
        "    \n",
        "    qte_files = files['qte']\n",
        "    sts_files = files['sts']\n",
        "    \n",
        "    if not qte_files and not sts_files:\n",
        "        warnings.warn(f\"No QTE or STS files found for ISIN: {isin}\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # LOAD AND CLEAN QTE FILES\n",
        "    # ========================================================================\n",
        "    qte_dataframes = []\n",
        "    \n",
        "    for identity, file_path in qte_files.items():\n",
        "        session, file_isin, mic, ticker = identity\n",
        "        \n",
        "        # Load QTE file\n",
        "        # Using pandas read_csv with compression='gzip' for efficient loading\n",
        "        df_qte = pd.read_csv(file_path, sep=';', compression='gzip')\n",
        "        \n",
        "        if df_qte.empty:\n",
        "            continue\n",
        "        \n",
        "        # Add orderbook identity columns for tracking\n",
        "        df_qte['session'] = session\n",
        "        df_qte['isin'] = file_isin\n",
        "        df_qte['mic'] = mic\n",
        "        df_qte['ticker'] = ticker\n",
        "        \n",
        "        # Filter invalid prices (magic numbers)\n",
        "        # These represent non-tradable states and must be discarded\n",
        "        # We check both bid and ask prices at level 0 (best prices)\n",
        "        valid_mask = (\n",
        "            ~df_qte['px_bid_0'].isin(MAGIC_NUMBERS) &\n",
        "            ~df_qte['px_ask_0'].isin(MAGIC_NUMBERS)\n",
        "        )\n",
        "        df_qte = df_qte[valid_mask].copy()\n",
        "        \n",
        "        if df_qte.empty:\n",
        "            warnings.warn(\n",
        "                f\"All prices filtered out (magic numbers) for {identity}. \"\n",
        "                f\"Skipping this orderbook.\"\n",
        "            )\n",
        "            continue\n",
        "        \n",
        "        # Clean timestamps and create unique index\n",
        "        df_qte = clean_qte_timestamps(df_qte)\n",
        "        \n",
        "        # Keep only essential columns for Step 1\n",
        "        # We focus on best bid/ask (level 0) for arbitrage detection\n",
        "        essential_cols = ['session', 'isin', 'mic', 'ticker', \n",
        "                         'px_bid_0', 'px_ask_0', 'qty_bid_0', 'qty_ask_0']\n",
        "        df_qte = df_qte[essential_cols].copy()\n",
        "        \n",
        "        qte_dataframes.append(df_qte)\n",
        "    \n",
        "    # Concatenate all QTE DataFrames\n",
        "    if qte_dataframes:\n",
        "        df_qte_clean = pd.concat(qte_dataframes, axis=0)\n",
        "        df_qte_clean.sort_index(inplace=True)\n",
        "    else:\n",
        "        df_qte_clean = pd.DataFrame()\n",
        "        warnings.warn(f\"No valid QTE data after filtering for ISIN: {isin}\")\n",
        "    \n",
        "    # ========================================================================\n",
        "    # LOAD AND CLEAN STS FILES\n",
        "    # ========================================================================\n",
        "    sts_dataframes = []\n",
        "    \n",
        "    for identity, file_path in sts_files.items():\n",
        "        session, file_isin, mic, ticker = identity\n",
        "        \n",
        "        # Load STS file\n",
        "        df_sts = pd.read_csv(file_path, sep=';', compression='gzip')\n",
        "        \n",
        "        if df_sts.empty:\n",
        "            continue\n",
        "        \n",
        "        # Add orderbook identity columns\n",
        "        df_sts['session'] = session\n",
        "        df_sts['isin'] = file_isin\n",
        "        df_sts['mic'] = mic\n",
        "        df_sts['ticker'] = ticker\n",
        "        \n",
        "        # Filter for addressable orderbooks (continuous trading status only)\n",
        "        # Only orders placed during continuous trading can execute immediately\n",
        "        # Trading during auctions, halts, or pre-open would not execute\n",
        "        if mic in CONTINUOUS_TRADING_STATUS:\n",
        "            valid_statuses = CONTINUOUS_TRADING_STATUS[mic]\n",
        "            addressable_mask = df_sts['market_trading_status'].isin(valid_statuses)\n",
        "            df_sts = df_sts[addressable_mask].copy()\n",
        "        else:\n",
        "            # If MIC not in our mapping, warn and skip filtering\n",
        "            warnings.warn(\n",
        "                f\"Unknown MIC '{mic}' for {identity}. \"\n",
        "                f\"Cannot filter by trading status. Including all statuses.\"\n",
        "            )\n",
        "        \n",
        "        if df_sts.empty:\n",
        "            warnings.warn(\n",
        "                f\"No addressable orderbook periods (continuous trading) for {identity}. \"\n",
        "                f\"Skipping this orderbook.\"\n",
        "            )\n",
        "            continue\n",
        "        \n",
        "        # Clean timestamps and create unique index\n",
        "        df_sts = clean_sts_timestamps(df_sts)\n",
        "        \n",
        "        # Keep essential columns\n",
        "        essential_cols = ['session', 'isin', 'mic', 'ticker', 'market_trading_status']\n",
        "        df_sts = df_sts[essential_cols].copy()\n",
        "        \n",
        "        sts_dataframes.append(df_sts)\n",
        "    \n",
        "    # Concatenate all STS DataFrames\n",
        "    if sts_dataframes:\n",
        "        df_sts_clean = pd.concat(sts_dataframes, axis=0)\n",
        "        df_sts_clean.sort_index(inplace=True)\n",
        "    else:\n",
        "        df_sts_clean = pd.DataFrame()\n",
        "        warnings.warn(f\"No valid STS data after filtering for ISIN: {isin}\")\n",
        "    \n",
        "    return df_qte_clean, df_sts_clean\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TEST THE FUNCTION\n",
        "# ============================================================================\n",
        "# Test with a sample ISIN from DATA_SMALL first to verify the function works\n",
        "# Then we can use it with DATA_BIG\n",
        "\n",
        "print(\"Testing load_qte_sts_for_isin function...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test with sample ISIN from DATA_SMALL\n",
        "test_isin = 'ES0113900J37'\n",
        "df_qte, df_sts = load_qte_sts_for_isin(test_isin, data_path='DATA_SMALL')\n",
        "\n",
        "print(f\"\\nQTE DataFrame shape: {df_qte.shape}\")\n",
        "print(f\"QTE columns: {df_qte.columns.tolist() if not df_qte.empty else 'Empty'}\")\n",
        "if not df_qte.empty:\n",
        "    print(f\"QTE date range: {df_qte.index.min()} to {df_qte.index.max()}\")\n",
        "    print(f\"\\nQTE sample (first 5 rows):\")\n",
        "    print(df_qte.head())\n",
        "\n",
        "print(f\"\\nSTS DataFrame shape: {df_sts.shape}\")\n",
        "print(f\"STS columns: {df_sts.columns.tolist() if not df_sts.empty else 'Empty'}\")\n",
        "if not df_sts.empty:\n",
        "    print(f\"STS date range: {df_sts.index.min()} to {df_sts.index.max()}\")\n",
        "    print(f\"\\nSTS sample (first 5 rows):\")\n",
        "    print(df_sts.head())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Function implementation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create the \"Consolidated Tape\"\n",
        "\n",
        "- To detect arbitrage, you need to compare prices across venues *at the exact same time*.\n",
        "- **Task:** Create a single DataFrame where the index is the timestamp, and the columns represent the Best Bid and Best Ask for **every** venue (BME, XMAD, CBOE, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 2: CREATE THE CONSOLIDATED TAPE\n",
        "# ============================================================================\n",
        "\n",
        "def create_consolidated_tape(df_qte, df_sts):\n",
        "    \"\"\"\n",
        "    Create a consolidated tape showing best bid and ask prices for all venues over time.\n",
        "    \n",
        "    This function:\n",
        "    1. Merges QTE snapshots with STS status to ensure only addressable orderbooks\n",
        "    2. Creates separate pivots for bid and ask prices per venue\n",
        "    3. Forward-fills prices to handle asynchronous updates across venues\n",
        "    4. Returns a unified view where each timestamp shows all venues' best bid/ask\n",
        "    \n",
        "    The consolidated tape is essential for arbitrage detection because it allows\n",
        "    us to compare prices across venues at the exact same moment in time.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df_qte : pd.DataFrame\n",
        "        Cleaned QTE DataFrame from load_qte_sts_for_isin()\n",
        "        Must have columns: timestamp index, 'session', 'isin', 'mic', 'ticker',\n",
        "        'px_bid_0', 'px_ask_0', 'qty_bid_0', 'qty_ask_0'\n",
        "    df_sts : pd.DataFrame\n",
        "        Cleaned STS DataFrame from load_qte_sts_for_isin()\n",
        "        Must have columns: timestamp index, 'session', 'isin', 'mic', 'ticker',\n",
        "        'market_trading_status'\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Consolidated tape with timestamp index and columns:\n",
        "        - {MIC}_bid, {MIC}_ask for each venue (e.g., XMAD_bid, XMAD_ask, CEUX_bid, CEUX_ask)\n",
        "        - Forward-filled prices (last known price propagates forward)\n",
        "    \"\"\"\n",
        "    if df_qte.empty or df_sts.empty:\n",
        "        warnings.warn(\"Empty QTE or STS DataFrame provided. Returning empty consolidated tape.\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 1: MERGE QTE WITH STS PER ORDERBOOK\n",
        "    # ========================================================================\n",
        "    # We need to ensure each QTE snapshot is only included when the market\n",
        "    # is in continuous trading (addressable). We use merge_asof to find the\n",
        "    # most recent STS status before/at each QTE snapshot.\n",
        "    \n",
        "    qte_filtered_list = []\n",
        "    \n",
        "    # Get unique orderbook identities\n",
        "    qte_identities = df_qte[['session', 'isin', 'mic', 'ticker']].drop_duplicates()\n",
        "    sts_identities = df_sts[['session', 'isin', 'mic', 'ticker']].drop_duplicates()\n",
        "    \n",
        "    # Process each orderbook identity\n",
        "    for _, identity_row in qte_identities.iterrows():\n",
        "        session = identity_row['session']\n",
        "        isin = identity_row['isin']\n",
        "        mic = identity_row['mic']\n",
        "        ticker = identity_row['ticker']\n",
        "        \n",
        "        identity = (session, isin, mic, ticker)\n",
        "        \n",
        "        # Get QTE data for this orderbook\n",
        "        qte_mask = (\n",
        "            (df_qte['session'] == session) &\n",
        "            (df_qte['isin'] == isin) &\n",
        "            (df_qte['mic'] == mic) &\n",
        "            (df_qte['ticker'] == ticker)\n",
        "        )\n",
        "        qte_orderbook = df_qte[qte_mask].copy()\n",
        "        \n",
        "        if qte_orderbook.empty:\n",
        "            continue\n",
        "        \n",
        "        # Get STS data for this orderbook\n",
        "        sts_mask = (\n",
        "            (df_sts['session'] == session) &\n",
        "            (df_sts['isin'] == isin) &\n",
        "            (df_sts['mic'] == mic) &\n",
        "            (df_sts['ticker'] == ticker)\n",
        "        )\n",
        "        sts_orderbook = df_sts[sts_mask].copy()\n",
        "        \n",
        "        if sts_orderbook.empty:\n",
        "            # No STS data means we can't verify addressability - skip this orderbook\n",
        "            warnings.warn(\n",
        "                f\"No STS data for orderbook {identity}. \"\n",
        "                f\"Cannot verify addressability. Skipping.\"\n",
        "            )\n",
        "            continue\n",
        "        \n",
        "        # Prepare for merge_asof: reset index to make timestamp a column\n",
        "        qte_for_merge = qte_orderbook.reset_index()\n",
        "        sts_for_merge = sts_orderbook.reset_index()\n",
        "        \n",
        "        # Ensure both are sorted by timestamp (required for merge_asof)\n",
        "        qte_for_merge = qte_for_merge.sort_values('ts')\n",
        "        sts_for_merge = sts_for_merge.sort_values('ts')\n",
        "        \n",
        "        # Merge QTE with STS using merge_asof\n",
        "        # direction='backward' means: for each QTE snapshot, find the most recent\n",
        "        # STS status that occurred at or before that snapshot's timestamp.\n",
        "        # This ensures we only use status information that was known at the time.\n",
        "        qte_with_status = pd.merge_asof(\n",
        "            qte_for_merge,\n",
        "            sts_for_merge[['ts', 'market_trading_status']],\n",
        "            on='ts',\n",
        "            direction='backward'  # Look backwards in time (no look-ahead bias)\n",
        "        )\n",
        "        \n",
        "        # Filter to keep only snapshots where market is in continuous trading\n",
        "        # This ensures we only include addressable orderbooks\n",
        "        if mic in CONTINUOUS_TRADING_STATUS:\n",
        "            valid_statuses = CONTINUOUS_TRADING_STATUS[mic]\n",
        "            addressable_mask = qte_with_status['market_trading_status'].isin(valid_statuses)\n",
        "            qte_addressable = qte_with_status[addressable_mask].copy()\n",
        "        else:\n",
        "            # Unknown MIC - warn but include all data\n",
        "            warnings.warn(\n",
        "                f\"Unknown MIC '{mic}' for {identity}. \"\n",
        "                f\"Cannot filter by trading status. Including all snapshots.\"\n",
        "            )\n",
        "            qte_addressable = qte_with_status.copy()\n",
        "        \n",
        "        if qte_addressable.empty:\n",
        "            continue\n",
        "        \n",
        "        # Set timestamp back as index\n",
        "        qte_addressable.set_index('ts', inplace=True)\n",
        "        \n",
        "        # Keep only essential columns\n",
        "        essential_cols = ['session', 'isin', 'mic', 'ticker',\n",
        "                         'px_bid_0', 'px_ask_0', 'qty_bid_0', 'qty_ask_0']\n",
        "        qte_addressable = qte_addressable[essential_cols].copy()\n",
        "        \n",
        "        qte_filtered_list.append(qte_addressable)\n",
        "    \n",
        "    if not qte_filtered_list:\n",
        "        warnings.warn(\"No addressable QTE data after merging with STS. Returning empty tape.\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 2: COMBINE ALL VENUES\n",
        "    # ========================================================================\n",
        "    # Concatenate all filtered QTE data from all orderbooks\n",
        "    # This creates a single DataFrame with all venues' snapshots\n",
        "    qte_combined = pd.concat(qte_filtered_list, axis=0)\n",
        "    qte_combined.sort_index(inplace=True)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 3: CREATE BID PRICE PIVOT\n",
        "    # ========================================================================\n",
        "    # Pivot the data to create columns for each venue's bid price\n",
        "    # This transforms from long format (one row per snapshot) to wide format\n",
        "    # (one row per timestamp, one column per venue)\n",
        "    \n",
        "    # Reset index temporarily to use timestamp as a column for pivot\n",
        "    qte_reset = qte_combined.reset_index()\n",
        "    \n",
        "    # Create pivot table: columns = venue (MIC), values = bid price\n",
        "    # This creates a DataFrame where each row is a timestamp and each column\n",
        "    # is a venue's bid price\n",
        "    bid_pivot = qte_reset.pivot(index='ts', columns='mic', values='px_bid_0')\n",
        "    \n",
        "    # Forward fill: propagate the last known bid price forward\n",
        "    # This handles the asynchronous nature of market data - if a venue doesn't\n",
        "    # update at a given timestamp, we use its last known price\n",
        "    # This is the \"Last Traded Price\" (LTP) logic: if no update happened,\n",
        "    # the valid price is still the last price that did happen\n",
        "    bid_pivot_ffilled = bid_pivot.ffill()\n",
        "    \n",
        "    # Rename columns to indicate these are bid prices\n",
        "    bid_pivot_ffilled.columns = [f'{mic}_bid' for mic in bid_pivot_ffilled.columns]\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 4: CREATE ASK PRICE PIVOT\n",
        "    # ========================================================================\n",
        "    # Same process for ask prices\n",
        "    ask_pivot = qte_reset.pivot(index='ts', columns='mic', values='px_ask_0')\n",
        "    ask_pivot_ffilled = ask_pivot.ffill()\n",
        "    \n",
        "    # Rename columns to indicate these are ask prices\n",
        "    ask_pivot_ffilled.columns = [f'{mic}_ask' for mic in ask_pivot_ffilled.columns]\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 5: COMBINE BID AND ASK INTO FINAL TAPE\n",
        "    # ========================================================================\n",
        "    # Merge the bid and ask pivots on timestamp index\n",
        "    # This creates the final consolidated tape with both bid and ask prices\n",
        "    # for each venue at each timestamp\n",
        "    \n",
        "    # Use outer join to include all timestamps from both bid and ask\n",
        "    consolidated_tape = pd.merge(\n",
        "        bid_pivot_ffilled,\n",
        "        ask_pivot_ffilled,\n",
        "        left_index=True,\n",
        "        right_index=True,\n",
        "        how='outer'\n",
        "    )\n",
        "    \n",
        "    # Sort by timestamp to ensure chronological order\n",
        "    consolidated_tape.sort_index(inplace=True)\n",
        "    \n",
        "    # Forward fill one more time after merging to handle any remaining gaps\n",
        "    # This ensures that if a venue has a bid but no ask (or vice versa) at\n",
        "    # a given timestamp, we still have the last known value\n",
        "    consolidated_tape = consolidated_tape.ffill()\n",
        "    \n",
        "    return consolidated_tape\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TEST THE FUNCTION\n",
        "# ============================================================================\n",
        "# Test with the sample data from Step 1\n",
        "\n",
        "print(\"Testing create_consolidated_tape function...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use the test data from Step 1 (if it exists)\n",
        "if 'df_qte' in locals() and 'df_sts' in locals() and not df_qte.empty and not df_sts.empty:\n",
        "    consolidated_tape = create_consolidated_tape(df_qte, df_sts)\n",
        "    \n",
        "    print(f\"\\nConsolidated Tape shape: {consolidated_tape.shape}\")\n",
        "    print(f\"Consolidated Tape columns: {consolidated_tape.columns.tolist()}\")\n",
        "    if not consolidated_tape.empty:\n",
        "        print(f\"Date range: {consolidated_tape.index.min()} to {consolidated_tape.index.max()}\")\n",
        "        print(f\"\\nConsolidated Tape sample (first 10 rows):\")\n",
        "        print(consolidated_tape.head(10))\n",
        "        \n",
        "        # Show summary statistics\n",
        "        print(f\"\\nSummary Statistics:\")\n",
        "        print(consolidated_tape.describe())\n",
        "        \n",
        "        # Show which venues are present\n",
        "        venues_bid = [col.replace('_bid', '') for col in consolidated_tape.columns if col.endswith('_bid')]\n",
        "        venues_ask = [col.replace('_ask', '') for col in consolidated_tape.columns if col.endswith('_ask')]\n",
        "        print(f\"\\nVenues with bid data: {venues_bid}\")\n",
        "        print(f\"Venues with ask data: {venues_ask}\")\n",
        "else:\n",
        "    print(\"\\nNote: Run Step 1 first to load QTE and STS data, then run this cell again.\")\n",
        "    print(\"Or test with:\")\n",
        "    print(\"  test_isin = 'ES0113900J37'\")\n",
        "    print(\"  df_qte, df_sts = load_qte_sts_for_isin(test_isin, data_path='DATA_SMALL')\")\n",
        "    print(\"  consolidated_tape = create_consolidated_tape(df_qte, df_sts)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Consolidated tape function implementation complete!\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATIONS: SHOW DATA ACROSS ALL VENUES\n",
        "# ============================================================================\n",
        "# Similar to the reference notebook, we create visualizations to compare\n",
        "# prices across all venues. This helps identify arbitrage opportunities\n",
        "# and understand market microstructure dynamics.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# Set style for better-looking charts\n",
        "plt.style.use('seaborn-v0_8-darkgrid' if 'seaborn-v0_8-darkgrid' in plt.style.available else 'default')\n",
        "\n",
        "# Only create visualizations if we have consolidated tape data\n",
        "if 'consolidated_tape' in locals() and not consolidated_tape.empty:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Creating visualizations of consolidated tape data...\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Get list of venues dynamically from the consolidated tape\n",
        "    bid_cols = [col for col in consolidated_tape.columns if col.endswith('_bid')]\n",
        "    ask_cols = [col for col in consolidated_tape.columns if col.endswith('_ask')]\n",
        "    venues = sorted(set([col.replace('_bid', '').replace('_ask', '') for col in bid_cols + ask_cols]))\n",
        "    \n",
        "    if venues:\n",
        "        # ========================================================================\n",
        "        # CHART 1: BID AND ASK PRICES ACROSS ALL VENUES\n",
        "        # ========================================================================\n",
        "        # This chart shows the best bid and ask prices for each venue over time.\n",
        "        # It allows us to visually identify when prices diverge across venues,\n",
        "        # which is essential for spotting arbitrage opportunities.\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
        "        \n",
        "        # Define colors and line styles for different venues\n",
        "        # Using a consistent color scheme that works well for financial data\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
        "        line_styles = ['-', '--', '-.', ':', '-', '--']\n",
        "        \n",
        "        # Plot Bid Prices (top subplot)\n",
        "        ax1 = axes[0]\n",
        "        for i, venue in enumerate(venues):\n",
        "            bid_col = f'{venue}_bid'\n",
        "            if bid_col in consolidated_tape.columns:\n",
        "                ax1.plot(\n",
        "                    consolidated_tape.index,\n",
        "                    consolidated_tape[bid_col],\n",
        "                    label=f'{venue} Bid',\n",
        "                    color=colors[i % len(colors)],\n",
        "                    linestyle=line_styles[i % len(line_styles)],\n",
        "                    linewidth=1.5,\n",
        "                    alpha=0.8\n",
        "                )\n",
        "        \n",
        "        ax1.set_ylabel('Bid Price (€)', fontsize=12, fontweight='bold')\n",
        "        ax1.set_title('Best Bid Prices Across All Venues Over Time', fontsize=14, fontweight='bold')\n",
        "        ax1.legend(loc='best', fontsize=10)\n",
        "        ax1.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "        \n",
        "        # Plot Ask Prices (bottom subplot)\n",
        "        ax2 = axes[1]\n",
        "        for i, venue in enumerate(venues):\n",
        "            ask_col = f'{venue}_ask'\n",
        "            if ask_col in consolidated_tape.columns:\n",
        "                ax2.plot(\n",
        "                    consolidated_tape.index,\n",
        "                    consolidated_tape[ask_col],\n",
        "                    label=f'{venue} Ask',\n",
        "                    color=colors[i % len(colors)],\n",
        "                    linestyle=line_styles[i % len(line_styles)],\n",
        "                    linewidth=1.5,\n",
        "                    alpha=0.8\n",
        "                )\n",
        "        \n",
        "        ax2.set_xlabel('Timestamp', fontsize=12, fontweight='bold')\n",
        "        ax2.set_ylabel('Ask Price (€)', fontsize=12, fontweight='bold')\n",
        "        ax2.set_title('Best Ask Prices Across All Venues Over Time', fontsize=14, fontweight='bold')\n",
        "        ax2.legend(loc='best', fontsize=10)\n",
        "        ax2.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "        \n",
        "        # Format x-axis to show time nicely\n",
        "        for ax in axes:\n",
        "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
        "            ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=max(1, len(consolidated_tape) // 100)))\n",
        "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # ========================================================================\n",
        "        # CHART 2: PRICE SPREADS BETWEEN VENUES\n",
        "        # ========================================================================\n",
        "        # Calculate and visualize price differences (spreads) between venues.\n",
        "        # Positive spreads indicate potential arbitrage opportunities where\n",
        "        # one venue's bid is higher than another venue's ask.\n",
        "        \n",
        "        if len(venues) >= 2:\n",
        "            # Calculate spreads between the first two venues (most common case)\n",
        "            # In a real scenario, you might want to calculate all pairwise spreads\n",
        "            venue1 = venues[0]\n",
        "            venue2 = venues[1] if len(venues) > 1 else None\n",
        "            \n",
        "            if venue2:\n",
        "                bid_col1 = f'{venue1}_bid'\n",
        "                ask_col2 = f'{venue2}_ask'\n",
        "                \n",
        "                if bid_col1 in consolidated_tape.columns and ask_col2 in consolidated_tape.columns:\n",
        "                    # Calculate spread: Bid from venue1 - Ask from venue2\n",
        "                    # Positive spread means we can buy on venue2 and sell on venue1\n",
        "                    spread = consolidated_tape[bid_col1] - consolidated_tape[ask_col2]\n",
        "                    \n",
        "                    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "                    \n",
        "                    ax.plot(\n",
        "                        consolidated_tape.index,\n",
        "                        spread,\n",
        "                        label=f'Spread ({venue1} Bid - {venue2} Ask)',\n",
        "                        color='#2ca02c',\n",
        "                        linewidth=1.5,\n",
        "                        alpha=0.8\n",
        "                    )\n",
        "                    \n",
        "                    # Add horizontal line at zero for reference\n",
        "                    ax.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Zero Spread')\n",
        "                    \n",
        "                    # Highlight positive spreads (arbitrage opportunities)\n",
        "                    positive_spread = spread[spread > 0]\n",
        "                    if not positive_spread.empty:\n",
        "                        ax.fill_between(\n",
        "                            consolidated_tape.index,\n",
        "                            0,\n",
        "                            spread,\n",
        "                            where=(spread > 0),\n",
        "                            color='green',\n",
        "                            alpha=0.2,\n",
        "                            label='Arbitrage Opportunity'\n",
        "                        )\n",
        "                    \n",
        "                    ax.set_xlabel('Timestamp', fontsize=12, fontweight='bold')\n",
        "                    ax.set_ylabel('Price Spread (€)', fontsize=12, fontweight='bold')\n",
        "                    ax.set_title(\n",
        "                        f'Price Spread Between Venues: {venue1} Bid vs {venue2} Ask\\n'\n",
        "                        f'(Positive values indicate arbitrage opportunities)',\n",
        "                        fontsize=14,\n",
        "                        fontweight='bold'\n",
        "                    )\n",
        "                    ax.legend(loc='best', fontsize=10)\n",
        "                    ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "                    \n",
        "                    # Format x-axis\n",
        "                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
        "                    ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=max(1, len(consolidated_tape) // 100)))\n",
        "                    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "                    \n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "                    \n",
        "                    # Print summary statistics\n",
        "                    print(f\"\\nSpread Statistics ({venue1} Bid - {venue2} Ask):\")\n",
        "                    print(f\"  Mean spread: {spread.mean():.4f} €\")\n",
        "                    print(f\"  Max spread: {spread.max():.4f} €\")\n",
        "                    print(f\"  Min spread: {spread.min():.4f} €\")\n",
        "                    print(f\"  Std deviation: {spread.std():.4f} €\")\n",
        "                    print(f\"  Positive spread periods: {len(positive_spread)} ({len(positive_spread)/len(spread)*100:.2f}%)\")\n",
        "        \n",
        "        # ========================================================================\n",
        "        # CHART 3: COMBINED VIEW - ALL BID AND ASK PRICES TOGETHER\n",
        "        # ========================================================================\n",
        "        # This chart shows both bid and ask prices for all venues on the same plot.\n",
        "        # It provides a comprehensive view of the market microstructure and helps\n",
        "        # identify when the bid-ask spread widens or narrows across venues.\n",
        "        \n",
        "        fig, ax = plt.subplots(figsize=(14, 8))\n",
        "        \n",
        "        # Plot all bid prices\n",
        "        for i, venue in enumerate(venues):\n",
        "            bid_col = f'{venue}_bid'\n",
        "            ask_col = f'{venue}_ask'\n",
        "            \n",
        "            if bid_col in consolidated_tape.columns:\n",
        "                ax.plot(\n",
        "                    consolidated_tape.index,\n",
        "                    consolidated_tape[bid_col],\n",
        "                    label=f'{venue} Bid',\n",
        "                    color=colors[i % len(colors)],\n",
        "                    linestyle='-',\n",
        "                    linewidth=2,\n",
        "                    alpha=0.7\n",
        "                )\n",
        "            \n",
        "            if ask_col in consolidated_tape.columns:\n",
        "                ax.plot(\n",
        "                    consolidated_tape.index,\n",
        "                    consolidated_tape[ask_col],\n",
        "                    label=f'{venue} Ask',\n",
        "                    color=colors[i % len(colors)],\n",
        "                    linestyle='--',\n",
        "                    linewidth=2,\n",
        "                    alpha=0.7\n",
        "                )\n",
        "        \n",
        "        ax.set_xlabel('Timestamp', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('Price (€)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            'Consolidated Tape: All Venues Bid and Ask Prices Over Time\\n'\n",
        "            '(Solid lines = Bid, Dashed lines = Ask)',\n",
        "            fontsize=14,\n",
        "            fontweight='bold'\n",
        "        )\n",
        "        ax.legend(loc='best', fontsize=9, ncol=2)\n",
        "        ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "        \n",
        "        # Format x-axis\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
        "        ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=max(1, len(consolidated_tape) // 100)))\n",
        "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"\\nVisualizations complete!\")\n",
        "        print(\"=\" * 80)\n",
        "    else:\n",
        "        print(\"\\nNo venue data available for visualization.\")\n",
        "else:\n",
        "    print(\"\\nNote: Run the consolidated tape creation code above first to generate visualizations.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Signal Generation\n",
        "\n",
        "- **Arbitrage Condition:** An opportunity exists when Global Max Bid > Global Min Ask.\n",
        "- **Profit Calc:** (Max Bid - Min Ask) * Min(BidQty, AskQty).\n",
        "- **Rising Edge:** In a simulation, if an opportunity persists for 1 second (1000 snapshots), you can only trade it *once* (the first time it appears). Ensure you aren't \"double counting\" the same opportunity. If the opportunity vanishes and quickly reappears you can count it as a new opportunity for simplification.\n",
        "- **Simplification:** Only look at opportunities between Global Max Bid and Global Min Ask. There might be others at the second or third price levels of the orderbook, but let's make it simple and use only the best Bid Ask of each trading venue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: The \"Time Machine\" (Latency Simulation)\n",
        "\n",
        "- In reality, if you see a price at time $T$, you cannot trade until $T + \\Delta$.\n",
        "- **Task:** Simulate execution latencies of [0, 100, 500, 1000, 2000, 3000, 4000, 5000, 10000, 15000, 20000, 30000, 50000, 100000] microseconds\n",
        "- *Method:* If a signal is detected at T, look up what the profit *actually is* at T + Latency in your dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Deliverables & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Submit a Jupyter Notebook containing your code and the following analysis:\n",
        "\n",
        "1. **The \"Money Table\":** A summary table showing the Total Realized Profit for all processed ISINs at each latency level.\n",
        "2. **The Decay Chart:** A line chart visualizing how Total Profit (Y-axis) decays as Latency (X-axis) increases.\n",
        "3. **Top Opportunities:** A list of the Top 5 most profitable ISINs (at 0 latency). **Sanity check these results**—do they look real?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. The \"Money Table\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. The Decay Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Top Opportunities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grading Rubric (Max 10 Points)\n",
        "\n",
        "- **5-6 Points (Baseline):** The code runs, correctly calculates the consolidated tape, identifies Bid > Ask opportunities, and estimates theoretical (0 latency) profit.\n",
        "\n",
        "- **7-8 Points (Robust):** The simulation accurately models latency (using strict time-lookups) and strictly adheres to the vendor's data quality specs.\n",
        "\n",
        "- **9-10 Points (Expert):** You demonstrate deep understanding of market microstructure. You handle **Market Status** correctly to avoid fake signals, identify anomalies in the instrument list, and handle edge cases around Market Open/Close."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
